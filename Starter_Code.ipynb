{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN                          NAME APPLICATION_TYPE  AFFILIATION  \\\n",
       "0  10520599  BLUE KNIGHTS MOTORCYCLE CLUB              T10  Independent   \n",
       "\n",
       "  CLASSIFICATION    USE_CASE ORGANIZATION  STATUS INCOME_AMT  \\\n",
       "0          C1000  ProductDev  Association       1          0   \n",
       "\n",
       "  SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0                      N     5000              1  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "\n",
    "if not os.path.isfile('./charity_data.csv'):\n",
    "    application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
    "    application_df.to_csv('./charity_data.csv', encoding='utf-8')\n",
    "else:\n",
    "    application_df = pd.read_csv('./charity_data.csv')\n",
    "    application_df = application_df.loc[:, ~application_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "application_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  APPLICATION_TYPE  AFFILIATION CLASSIFICATION    USE_CASE ORGANIZATION  \\\n",
       "0              T10  Independent          C1000  ProductDev  Association   \n",
       "\n",
       "   STATUS INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0       1          0                      N     5000              1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df.drop(['EIN', 'NAME'],  axis=1, inplace=True)\n",
    "# application_df = application_df.loc[:, ~application_df.columns.str.contains('^Unnamed')]\n",
    "application_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_TYPE            17\n",
       "AFFILIATION                  6\n",
       "CLASSIFICATION              71\n",
       "USE_CASE                     5\n",
       "ORGANIZATION                 4\n",
       "STATUS                       2\n",
       "INCOME_AMT                   9\n",
       "SPECIAL_CONSIDERATIONS       2\n",
       "ASK_AMT                   8747\n",
       "IS_SUCCESSFUL                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "application_df.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3     27037\n",
       "T4      1542\n",
       "T6      1216\n",
       "T5      1173\n",
       "T19     1065\n",
       "T8       737\n",
       "T7       725\n",
       "T10      528\n",
       "T9       156\n",
       "T13       66\n",
       "T12       27\n",
       "T2        16\n",
       "T25        3\n",
       "T14        3\n",
       "T29        2\n",
       "T15        2\n",
       "T17        1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "application_type_counts = application_df['APPLICATION_TYPE'].value_counts()\n",
    "application_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3       27037\n",
       "Other     2266\n",
       "T4        1542\n",
       "T6        1216\n",
       "T5        1173\n",
       "T19       1065\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "application_types_to_replace = list(application_type_counts[application_type_counts < 750].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Check to make sure binning was successful. Binning is a technique for reducing data cardinality by grouping related values together in bins \n",
    "# Cardinality is number that is obtained after a count, in a nutshell... The set {1, 2, 3, 4, 5} has cardinality five which is more than the cardinality of {1, 2, 3} which is three.\n",
    "application_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "         ...  \n",
       "C4120        1\n",
       "C8210        1\n",
       "C2561        1\n",
       "C4500        1\n",
       "C2150        1\n",
       "Name: CLASSIFICATION, Length: 71, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "classification_val_counts = application_df['CLASSIFICATION'].value_counts()\n",
    "classification_val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "C7000      777\n",
       "C1700      287\n",
       "C4000      194\n",
       "C5000      116\n",
       "C1270      114\n",
       "C2700      104\n",
       "C2800       95\n",
       "C7100       75\n",
       "C1300       58\n",
       "C1280       50\n",
       "C1230       36\n",
       "C1400       34\n",
       "C7200       32\n",
       "C2300       32\n",
       "C1240       30\n",
       "C8000       20\n",
       "C7120       18\n",
       "C1500       16\n",
       "C1800       15\n",
       "C6000       15\n",
       "C1250       14\n",
       "C8200       11\n",
       "C1238       10\n",
       "C1278       10\n",
       "C1235        9\n",
       "C1237        9\n",
       "C7210        7\n",
       "C2400        6\n",
       "C1720        6\n",
       "C4100        6\n",
       "C1257        5\n",
       "C1600        5\n",
       "C1260        3\n",
       "C2710        3\n",
       "C0           3\n",
       "C3200        2\n",
       "C1234        2\n",
       "C1246        2\n",
       "C1267        2\n",
       "C1256        2\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
    "greater_than_one = classification_val_counts[classification_val_counts>1]\n",
    "greater_than_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "Other     1484\n",
       "C7000      777\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "classifications_to_replace = list (classification_val_counts[classification_val_counts<750].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "      <th>APPLICATION_TYPE_Other</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T3</th>\n",
       "      <th>APPLICATION_TYPE_T4</th>\n",
       "      <th>APPLICATION_TYPE_T5</th>\n",
       "      <th>APPLICATION_TYPE_T6</th>\n",
       "      <th>AFFILIATION_CompanySponsored</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATUS  ASK_AMT  IS_SUCCESSFUL  APPLICATION_TYPE_Other  \\\n",
       "0       1     5000              1                     1.0   \n",
       "1       1   108590              1                     0.0   \n",
       "2       1     5000              0                     0.0   \n",
       "3       1     6692              1                     0.0   \n",
       "4       1   142590              1                     0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T3  APPLICATION_TYPE_T4  \\\n",
       "0                   0.0                  0.0                  0.0   \n",
       "1                   0.0                  1.0                  0.0   \n",
       "2                   0.0                  0.0                  0.0   \n",
       "3                   0.0                  1.0                  0.0   \n",
       "4                   0.0                  1.0                  0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T5  APPLICATION_TYPE_T6  AFFILIATION_CompanySponsored  \\\n",
       "0                  0.0                  0.0                           0.0   \n",
       "1                  0.0                  0.0                           0.0   \n",
       "2                  1.0                  0.0                           1.0   \n",
       "3                  0.0                  0.0                           1.0   \n",
       "4                  0.0                  0.0                           0.0   \n",
       "\n",
       "   ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n",
       "0  ...                0.0                     0.0                       0.0   \n",
       "1  ...                1.0                     0.0                       0.0   \n",
       "2  ...                0.0                     0.0                       0.0   \n",
       "3  ...                0.0                     1.0                       0.0   \n",
       "4  ...                0.0                     0.0                       1.0   \n",
       "\n",
       "   INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n",
       "0                 0.0               0.0                     0.0   \n",
       "1                 0.0               0.0                     0.0   \n",
       "2                 0.0               0.0                     0.0   \n",
       "3                 0.0               0.0                     0.0   \n",
       "4                 0.0               0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n",
       "0              0.0                0.0                       1.0   \n",
       "1              0.0                0.0                       1.0   \n",
       "2              0.0                0.0                       1.0   \n",
       "3              0.0                0.0                       1.0   \n",
       "4              0.0                0.0                       1.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "application_df = pd.get_dummies(application_df,dtype=float)\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gus Bustillos\\AppData\\Local\\Temp\\ipykernel_24960\\1048604523.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = application_df.drop(['IS_SUCCESSFUL'],1).values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.000000e+00, 2.631396e+06, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       ...,\n",
       "       [1.000000e+00, 1.443006e+06, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = application_df['IS_SUCCESSFUL'].values\n",
    "X = application_df.drop(['IS_SUCCESSFUL'],1).values\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_23 (Dense)            (None, 2)                 84        \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 4)                 12        \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133 (532.00 Byte)\n",
      "Trainable params: 133 (532.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "import tensorflow as tf\n",
    "keras = tf.keras  \n",
    "features = len( X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 2\n",
    "hidden_nodes_layer2 = 4\n",
    "hidden_nodes_layer3 = 6 \n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=features, activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, input_dim=features, activation='relu'))\n",
    "\n",
    "# third hidden \n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, input_dim=features, activation= keras.layers.LeakyReLU(alpha=0.01)))\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, input_dim=features, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "# nn.add(tf.keras.layers.Dense(units=1, activation=tf.keras.activations.softmax))\n",
    "\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.6476 - accuracy: 0.6509 - val_loss: 0.5930 - val_accuracy: 0.7054\n",
      "Epoch 2/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5892 - accuracy: 0.7109 - val_loss: 0.5654 - val_accuracy: 0.7336\n",
      "Epoch 3/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5788 - accuracy: 0.7162 - val_loss: 0.5601 - val_accuracy: 0.7334\n",
      "Epoch 4/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5756 - accuracy: 0.7164 - val_loss: 0.5585 - val_accuracy: 0.7349\n",
      "Epoch 5/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5728 - accuracy: 0.7178 - val_loss: 0.5579 - val_accuracy: 0.7365\n",
      "Epoch 6/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5721 - accuracy: 0.7187 - val_loss: 0.5576 - val_accuracy: 0.7344\n",
      "Epoch 7/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5711 - accuracy: 0.7191 - val_loss: 0.5552 - val_accuracy: 0.7372\n",
      "Epoch 8/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5706 - accuracy: 0.7187 - val_loss: 0.5536 - val_accuracy: 0.7365\n",
      "Epoch 9/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5703 - accuracy: 0.7199 - val_loss: 0.5548 - val_accuracy: 0.7365\n",
      "Epoch 10/780\n",
      "684/684 [==============================] - 1s 944us/step - loss: 0.5704 - accuracy: 0.7204 - val_loss: 0.5533 - val_accuracy: 0.7367\n",
      "Epoch 11/780\n",
      "684/684 [==============================] - 1s 940us/step - loss: 0.5701 - accuracy: 0.7200 - val_loss: 0.5533 - val_accuracy: 0.7370\n",
      "Epoch 12/780\n",
      "684/684 [==============================] - 1s 937us/step - loss: 0.5698 - accuracy: 0.7202 - val_loss: 0.5526 - val_accuracy: 0.7372\n",
      "Epoch 13/780\n",
      "684/684 [==============================] - 1s 967us/step - loss: 0.5698 - accuracy: 0.7206 - val_loss: 0.5547 - val_accuracy: 0.7372\n",
      "Epoch 14/780\n",
      "684/684 [==============================] - 1s 951us/step - loss: 0.5695 - accuracy: 0.7203 - val_loss: 0.5546 - val_accuracy: 0.7367\n",
      "Epoch 15/780\n",
      "684/684 [==============================] - 1s 981us/step - loss: 0.5695 - accuracy: 0.7199 - val_loss: 0.5539 - val_accuracy: 0.7378\n",
      "Epoch 16/780\n",
      "684/684 [==============================] - 1s 935us/step - loss: 0.5693 - accuracy: 0.7198 - val_loss: 0.5550 - val_accuracy: 0.7378\n",
      "Epoch 17/780\n",
      "684/684 [==============================] - 1s 938us/step - loss: 0.5693 - accuracy: 0.7201 - val_loss: 0.5524 - val_accuracy: 0.7378\n",
      "Epoch 18/780\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.5691 - accuracy: 0.7201 - val_loss: 0.5535 - val_accuracy: 0.7372\n",
      "Epoch 19/780\n",
      "684/684 [==============================] - 1s 913us/step - loss: 0.5690 - accuracy: 0.7203 - val_loss: 0.5523 - val_accuracy: 0.7375\n",
      "Epoch 20/780\n",
      "684/684 [==============================] - 1s 952us/step - loss: 0.5685 - accuracy: 0.7208 - val_loss: 0.5532 - val_accuracy: 0.7380\n",
      "Epoch 21/780\n",
      "684/684 [==============================] - 1s 931us/step - loss: 0.5684 - accuracy: 0.7208 - val_loss: 0.5527 - val_accuracy: 0.7380\n",
      "Epoch 22/780\n",
      "684/684 [==============================] - 1s 913us/step - loss: 0.5682 - accuracy: 0.7212 - val_loss: 0.5528 - val_accuracy: 0.7385\n",
      "Epoch 23/780\n",
      "684/684 [==============================] - 1s 919us/step - loss: 0.5679 - accuracy: 0.7217 - val_loss: 0.5520 - val_accuracy: 0.7385\n",
      "Epoch 24/780\n",
      "684/684 [==============================] - 1s 947us/step - loss: 0.5678 - accuracy: 0.7217 - val_loss: 0.5530 - val_accuracy: 0.7388\n",
      "Epoch 25/780\n",
      "684/684 [==============================] - 1s 935us/step - loss: 0.5679 - accuracy: 0.7215 - val_loss: 0.5529 - val_accuracy: 0.7385\n",
      "Epoch 26/780\n",
      "684/684 [==============================] - 1s 935us/step - loss: 0.5676 - accuracy: 0.7220 - val_loss: 0.5519 - val_accuracy: 0.7385\n",
      "Epoch 27/780\n",
      "684/684 [==============================] - 1s 954us/step - loss: 0.5677 - accuracy: 0.7210 - val_loss: 0.5517 - val_accuracy: 0.7383\n",
      "Epoch 28/780\n",
      "684/684 [==============================] - 1s 927us/step - loss: 0.5675 - accuracy: 0.7219 - val_loss: 0.5522 - val_accuracy: 0.7380\n",
      "Epoch 29/780\n",
      "684/684 [==============================] - 1s 974us/step - loss: 0.5676 - accuracy: 0.7208 - val_loss: 0.5510 - val_accuracy: 0.7383\n",
      "Epoch 30/780\n",
      "684/684 [==============================] - 1s 937us/step - loss: 0.5673 - accuracy: 0.7219 - val_loss: 0.5506 - val_accuracy: 0.7385\n",
      "Epoch 31/780\n",
      "684/684 [==============================] - 1s 898us/step - loss: 0.5668 - accuracy: 0.7223 - val_loss: 0.5558 - val_accuracy: 0.7383\n",
      "Epoch 32/780\n",
      "684/684 [==============================] - 1s 976us/step - loss: 0.5672 - accuracy: 0.7216 - val_loss: 0.5520 - val_accuracy: 0.7372\n",
      "Epoch 33/780\n",
      "684/684 [==============================] - 1s 955us/step - loss: 0.5667 - accuracy: 0.7223 - val_loss: 0.5506 - val_accuracy: 0.7370\n",
      "Epoch 34/780\n",
      "684/684 [==============================] - 1s 938us/step - loss: 0.5668 - accuracy: 0.7223 - val_loss: 0.5514 - val_accuracy: 0.7385\n",
      "Epoch 35/780\n",
      "684/684 [==============================] - 1s 927us/step - loss: 0.5665 - accuracy: 0.7222 - val_loss: 0.5510 - val_accuracy: 0.7375\n",
      "Epoch 36/780\n",
      "684/684 [==============================] - 1s 915us/step - loss: 0.5666 - accuracy: 0.7224 - val_loss: 0.5513 - val_accuracy: 0.7375\n",
      "Epoch 37/780\n",
      "684/684 [==============================] - 1s 983us/step - loss: 0.5666 - accuracy: 0.7223 - val_loss: 0.5536 - val_accuracy: 0.7370\n",
      "Epoch 38/780\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.5664 - accuracy: 0.7223 - val_loss: 0.5517 - val_accuracy: 0.7375\n",
      "Epoch 39/780\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.5661 - accuracy: 0.7222 - val_loss: 0.5511 - val_accuracy: 0.7372\n",
      "Epoch 40/780\n",
      "684/684 [==============================] - 1s 917us/step - loss: 0.5663 - accuracy: 0.7226 - val_loss: 0.5546 - val_accuracy: 0.7372\n",
      "Epoch 41/780\n",
      "684/684 [==============================] - 1s 917us/step - loss: 0.5658 - accuracy: 0.7223 - val_loss: 0.5509 - val_accuracy: 0.7372\n",
      "Epoch 42/780\n",
      "684/684 [==============================] - 1s 969us/step - loss: 0.5662 - accuracy: 0.7220 - val_loss: 0.5510 - val_accuracy: 0.7372\n",
      "Epoch 43/780\n",
      "684/684 [==============================] - 1s 917us/step - loss: 0.5660 - accuracy: 0.7226 - val_loss: 0.5524 - val_accuracy: 0.7375\n",
      "Epoch 44/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5661 - accuracy: 0.7222 - val_loss: 0.5506 - val_accuracy: 0.7375\n",
      "Epoch 45/780\n",
      "684/684 [==============================] - 1s 917us/step - loss: 0.5659 - accuracy: 0.7222 - val_loss: 0.5537 - val_accuracy: 0.7378\n",
      "Epoch 46/780\n",
      "684/684 [==============================] - 1s 990us/step - loss: 0.5658 - accuracy: 0.7224 - val_loss: 0.5533 - val_accuracy: 0.7383\n",
      "Epoch 47/780\n",
      "684/684 [==============================] - 1s 938us/step - loss: 0.5659 - accuracy: 0.7223 - val_loss: 0.5541 - val_accuracy: 0.7380\n",
      "Epoch 48/780\n",
      "684/684 [==============================] - 1s 942us/step - loss: 0.5658 - accuracy: 0.7225 - val_loss: 0.5528 - val_accuracy: 0.7367\n",
      "Epoch 49/780\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.5658 - accuracy: 0.7227 - val_loss: 0.5517 - val_accuracy: 0.7383\n",
      "Epoch 50/780\n",
      "684/684 [==============================] - 1s 930us/step - loss: 0.5657 - accuracy: 0.7223 - val_loss: 0.5517 - val_accuracy: 0.7372\n",
      "Epoch 51/780\n",
      "684/684 [==============================] - 1s 963us/step - loss: 0.5658 - accuracy: 0.7223 - val_loss: 0.5511 - val_accuracy: 0.7378\n",
      "Epoch 52/780\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.5658 - accuracy: 0.7225 - val_loss: 0.5520 - val_accuracy: 0.7380\n",
      "Epoch 53/780\n",
      "684/684 [==============================] - 1s 904us/step - loss: 0.5656 - accuracy: 0.7225 - val_loss: 0.5524 - val_accuracy: 0.7378\n",
      "Epoch 54/780\n",
      "684/684 [==============================] - 1s 909us/step - loss: 0.5654 - accuracy: 0.7227 - val_loss: 0.5539 - val_accuracy: 0.7375\n",
      "Epoch 55/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5656 - accuracy: 0.7226 - val_loss: 0.5522 - val_accuracy: 0.7380\n",
      "Epoch 56/780\n",
      "684/684 [==============================] - 1s 934us/step - loss: 0.5654 - accuracy: 0.7225 - val_loss: 0.5518 - val_accuracy: 0.7375\n",
      "Epoch 57/780\n",
      "684/684 [==============================] - 1s 916us/step - loss: 0.5654 - accuracy: 0.7222 - val_loss: 0.5505 - val_accuracy: 0.7378\n",
      "Epoch 58/780\n",
      "684/684 [==============================] - 1s 948us/step - loss: 0.5655 - accuracy: 0.7223 - val_loss: 0.5528 - val_accuracy: 0.7378\n",
      "Epoch 59/780\n",
      "684/684 [==============================] - 1s 975us/step - loss: 0.5654 - accuracy: 0.7221 - val_loss: 0.5515 - val_accuracy: 0.7388\n",
      "Epoch 60/780\n",
      "684/684 [==============================] - 1s 922us/step - loss: 0.5652 - accuracy: 0.7223 - val_loss: 0.5544 - val_accuracy: 0.7380\n",
      "Epoch 61/780\n",
      "684/684 [==============================] - 1s 930us/step - loss: 0.5653 - accuracy: 0.7226 - val_loss: 0.5519 - val_accuracy: 0.7383\n",
      "Epoch 62/780\n",
      "684/684 [==============================] - 1s 956us/step - loss: 0.5655 - accuracy: 0.7224 - val_loss: 0.5511 - val_accuracy: 0.7383\n",
      "Epoch 63/780\n",
      "684/684 [==============================] - 1s 961us/step - loss: 0.5651 - accuracy: 0.7226 - val_loss: 0.5531 - val_accuracy: 0.7378\n",
      "Epoch 64/780\n",
      "684/684 [==============================] - 1s 933us/step - loss: 0.5655 - accuracy: 0.7223 - val_loss: 0.5527 - val_accuracy: 0.7380\n",
      "Epoch 65/780\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.5652 - accuracy: 0.7226 - val_loss: 0.5522 - val_accuracy: 0.7378\n",
      "Epoch 66/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5654 - accuracy: 0.7225 - val_loss: 0.5522 - val_accuracy: 0.7378\n",
      "Epoch 67/780\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.5655 - accuracy: 0.7224 - val_loss: 0.5514 - val_accuracy: 0.7378\n",
      "Epoch 68/780\n",
      "684/684 [==============================] - 1s 980us/step - loss: 0.5652 - accuracy: 0.7221 - val_loss: 0.5533 - val_accuracy: 0.7375\n",
      "Epoch 69/780\n",
      "684/684 [==============================] - 1s 964us/step - loss: 0.5654 - accuracy: 0.7225 - val_loss: 0.5510 - val_accuracy: 0.7375\n",
      "Epoch 70/780\n",
      "684/684 [==============================] - 1s 944us/step - loss: 0.5650 - accuracy: 0.7224 - val_loss: 0.5518 - val_accuracy: 0.7380\n",
      "Epoch 71/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5652 - accuracy: 0.7223 - val_loss: 0.5518 - val_accuracy: 0.7380\n",
      "Epoch 72/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5649 - accuracy: 0.7223 - val_loss: 0.5530 - val_accuracy: 0.7370\n",
      "Epoch 73/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5649 - accuracy: 0.7226 - val_loss: 0.5566 - val_accuracy: 0.7372\n",
      "Epoch 74/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5650 - accuracy: 0.7224 - val_loss: 0.5516 - val_accuracy: 0.7378\n",
      "Epoch 75/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5651 - accuracy: 0.7225 - val_loss: 0.5545 - val_accuracy: 0.7367\n",
      "Epoch 76/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5652 - accuracy: 0.7223 - val_loss: 0.5525 - val_accuracy: 0.7372\n",
      "Epoch 77/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5650 - accuracy: 0.7226 - val_loss: 0.5526 - val_accuracy: 0.7372\n",
      "Epoch 78/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5650 - accuracy: 0.7224 - val_loss: 0.5517 - val_accuracy: 0.7372\n",
      "Epoch 79/780\n",
      "684/684 [==============================] - 1s 966us/step - loss: 0.5649 - accuracy: 0.7220 - val_loss: 0.5523 - val_accuracy: 0.7370\n",
      "Epoch 80/780\n",
      "684/684 [==============================] - 1s 955us/step - loss: 0.5648 - accuracy: 0.7225 - val_loss: 0.5520 - val_accuracy: 0.7370\n",
      "Epoch 81/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5650 - accuracy: 0.7224 - val_loss: 0.5521 - val_accuracy: 0.7375\n",
      "Epoch 82/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5647 - accuracy: 0.7223 - val_loss: 0.5547 - val_accuracy: 0.7367\n",
      "Epoch 83/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5647 - accuracy: 0.7225 - val_loss: 0.5530 - val_accuracy: 0.7372\n",
      "Epoch 84/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5648 - accuracy: 0.7220 - val_loss: 0.5518 - val_accuracy: 0.7375\n",
      "Epoch 85/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5649 - accuracy: 0.7226 - val_loss: 0.5520 - val_accuracy: 0.7385\n",
      "Epoch 86/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5649 - accuracy: 0.7218 - val_loss: 0.5525 - val_accuracy: 0.7380\n",
      "Epoch 87/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5647 - accuracy: 0.7217 - val_loss: 0.5529 - val_accuracy: 0.7375\n",
      "Epoch 88/780\n",
      "684/684 [==============================] - 1s 999us/step - loss: 0.5648 - accuracy: 0.7224 - val_loss: 0.5537 - val_accuracy: 0.7378\n",
      "Epoch 89/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5647 - accuracy: 0.7221 - val_loss: 0.5515 - val_accuracy: 0.7375\n",
      "Epoch 90/780\n",
      "684/684 [==============================] - 1s 991us/step - loss: 0.5649 - accuracy: 0.7220 - val_loss: 0.5535 - val_accuracy: 0.7370\n",
      "Epoch 91/780\n",
      "684/684 [==============================] - 1s 950us/step - loss: 0.5648 - accuracy: 0.7223 - val_loss: 0.5534 - val_accuracy: 0.7372\n",
      "Epoch 92/780\n",
      "684/684 [==============================] - 1s 927us/step - loss: 0.5650 - accuracy: 0.7222 - val_loss: 0.5532 - val_accuracy: 0.7378\n",
      "Epoch 93/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5648 - accuracy: 0.7223 - val_loss: 0.5522 - val_accuracy: 0.7370\n",
      "Epoch 94/780\n",
      "684/684 [==============================] - 1s 969us/step - loss: 0.5648 - accuracy: 0.7223 - val_loss: 0.5522 - val_accuracy: 0.7372\n",
      "Epoch 95/780\n",
      "684/684 [==============================] - 1s 932us/step - loss: 0.5647 - accuracy: 0.7216 - val_loss: 0.5511 - val_accuracy: 0.7370\n",
      "Epoch 96/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5649 - accuracy: 0.7221 - val_loss: 0.5530 - val_accuracy: 0.7372\n",
      "Epoch 97/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5644 - accuracy: 0.7223 - val_loss: 0.5515 - val_accuracy: 0.7372\n",
      "Epoch 98/780\n",
      "684/684 [==============================] - 1s 948us/step - loss: 0.5648 - accuracy: 0.7217 - val_loss: 0.5534 - val_accuracy: 0.7378\n",
      "Epoch 99/780\n",
      "684/684 [==============================] - 1s 997us/step - loss: 0.5649 - accuracy: 0.7216 - val_loss: 0.5522 - val_accuracy: 0.7380\n",
      "Epoch 100/780\n",
      "684/684 [==============================] - 1s 932us/step - loss: 0.5646 - accuracy: 0.7217 - val_loss: 0.5538 - val_accuracy: 0.7380\n",
      "Epoch 101/780\n",
      "684/684 [==============================] - 1s 946us/step - loss: 0.5643 - accuracy: 0.7218 - val_loss: 0.5519 - val_accuracy: 0.7383\n",
      "Epoch 102/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5643 - accuracy: 0.7220 - val_loss: 0.5553 - val_accuracy: 0.7378\n",
      "Epoch 103/780\n",
      "684/684 [==============================] - 1s 983us/step - loss: 0.5644 - accuracy: 0.7217 - val_loss: 0.5519 - val_accuracy: 0.7388\n",
      "Epoch 104/780\n",
      "684/684 [==============================] - 1s 984us/step - loss: 0.5644 - accuracy: 0.7214 - val_loss: 0.5529 - val_accuracy: 0.7378\n",
      "Epoch 105/780\n",
      "684/684 [==============================] - 1s 914us/step - loss: 0.5647 - accuracy: 0.7217 - val_loss: 0.5514 - val_accuracy: 0.7378\n",
      "Epoch 106/780\n",
      "684/684 [==============================] - 1s 915us/step - loss: 0.5646 - accuracy: 0.7218 - val_loss: 0.5523 - val_accuracy: 0.7380\n",
      "Epoch 107/780\n",
      "684/684 [==============================] - 1s 977us/step - loss: 0.5646 - accuracy: 0.7217 - val_loss: 0.5525 - val_accuracy: 0.7378\n",
      "Epoch 108/780\n",
      "684/684 [==============================] - 1s 938us/step - loss: 0.5645 - accuracy: 0.7214 - val_loss: 0.5526 - val_accuracy: 0.7385\n",
      "Epoch 109/780\n",
      "684/684 [==============================] - 1s 915us/step - loss: 0.5646 - accuracy: 0.7220 - val_loss: 0.5519 - val_accuracy: 0.7385\n",
      "Epoch 110/780\n",
      "684/684 [==============================] - 1s 935us/step - loss: 0.5644 - accuracy: 0.7221 - val_loss: 0.5515 - val_accuracy: 0.7383\n",
      "Epoch 111/780\n",
      "684/684 [==============================] - 1s 996us/step - loss: 0.5641 - accuracy: 0.7220 - val_loss: 0.5524 - val_accuracy: 0.7383\n",
      "Epoch 112/780\n",
      "684/684 [==============================] - 1s 922us/step - loss: 0.5644 - accuracy: 0.7219 - val_loss: 0.5525 - val_accuracy: 0.7372\n",
      "Epoch 113/780\n",
      "684/684 [==============================] - 1s 950us/step - loss: 0.5645 - accuracy: 0.7219 - val_loss: 0.5537 - val_accuracy: 0.7383\n",
      "Epoch 114/780\n",
      "684/684 [==============================] - 1s 964us/step - loss: 0.5643 - accuracy: 0.7214 - val_loss: 0.5540 - val_accuracy: 0.7383\n",
      "Epoch 115/780\n",
      "684/684 [==============================] - 1s 924us/step - loss: 0.5645 - accuracy: 0.7222 - val_loss: 0.5522 - val_accuracy: 0.7383\n",
      "Epoch 116/780\n",
      "684/684 [==============================] - 1s 965us/step - loss: 0.5642 - accuracy: 0.7220 - val_loss: 0.5525 - val_accuracy: 0.7378\n",
      "Epoch 117/780\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.5644 - accuracy: 0.7224 - val_loss: 0.5525 - val_accuracy: 0.7380\n",
      "Epoch 118/780\n",
      "684/684 [==============================] - 1s 998us/step - loss: 0.5644 - accuracy: 0.7215 - val_loss: 0.5520 - val_accuracy: 0.7383\n",
      "Epoch 119/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5645 - accuracy: 0.7217 - val_loss: 0.5540 - val_accuracy: 0.7383\n",
      "Epoch 120/780\n",
      "684/684 [==============================] - 1s 933us/step - loss: 0.5645 - accuracy: 0.7215 - val_loss: 0.5537 - val_accuracy: 0.7383\n",
      "Epoch 121/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5642 - accuracy: 0.7219 - val_loss: 0.5527 - val_accuracy: 0.7383\n",
      "Epoch 122/780\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.5643 - accuracy: 0.7220 - val_loss: 0.5521 - val_accuracy: 0.7388\n",
      "Epoch 123/780\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.5644 - accuracy: 0.7219 - val_loss: 0.5520 - val_accuracy: 0.7388\n",
      "Epoch 124/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5646 - accuracy: 0.7219 - val_loss: 0.5538 - val_accuracy: 0.7378\n",
      "Epoch 125/780\n",
      "684/684 [==============================] - 1s 909us/step - loss: 0.5644 - accuracy: 0.7218 - val_loss: 0.5511 - val_accuracy: 0.7380\n",
      "Epoch 126/780\n",
      "684/684 [==============================] - 1s 948us/step - loss: 0.5644 - accuracy: 0.7213 - val_loss: 0.5510 - val_accuracy: 0.7385\n",
      "Epoch 127/780\n",
      "684/684 [==============================] - 1s 982us/step - loss: 0.5645 - accuracy: 0.7219 - val_loss: 0.5516 - val_accuracy: 0.7383\n",
      "Epoch 128/780\n",
      "684/684 [==============================] - 1s 936us/step - loss: 0.5643 - accuracy: 0.7219 - val_loss: 0.5515 - val_accuracy: 0.7380\n",
      "Epoch 129/780\n",
      "684/684 [==============================] - 1s 941us/step - loss: 0.5642 - accuracy: 0.7222 - val_loss: 0.5522 - val_accuracy: 0.7385\n",
      "Epoch 130/780\n",
      "684/684 [==============================] - 1s 984us/step - loss: 0.5643 - accuracy: 0.7213 - val_loss: 0.5518 - val_accuracy: 0.7380\n",
      "Epoch 131/780\n",
      "684/684 [==============================] - 1s 968us/step - loss: 0.5645 - accuracy: 0.7215 - val_loss: 0.5518 - val_accuracy: 0.7385\n",
      "Epoch 132/780\n",
      "684/684 [==============================] - 1s 973us/step - loss: 0.5644 - accuracy: 0.7221 - val_loss: 0.5516 - val_accuracy: 0.7380\n",
      "Epoch 133/780\n",
      "684/684 [==============================] - 1s 952us/step - loss: 0.5642 - accuracy: 0.7218 - val_loss: 0.5509 - val_accuracy: 0.7378\n",
      "Epoch 134/780\n",
      "684/684 [==============================] - 1s 979us/step - loss: 0.5643 - accuracy: 0.7213 - val_loss: 0.5530 - val_accuracy: 0.7378\n",
      "Epoch 135/780\n",
      "684/684 [==============================] - 1s 944us/step - loss: 0.5644 - accuracy: 0.7215 - val_loss: 0.5531 - val_accuracy: 0.7372\n",
      "Epoch 136/780\n",
      "684/684 [==============================] - 1s 951us/step - loss: 0.5643 - accuracy: 0.7218 - val_loss: 0.5528 - val_accuracy: 0.7380\n",
      "Epoch 137/780\n",
      "684/684 [==============================] - 1s 985us/step - loss: 0.5646 - accuracy: 0.7215 - val_loss: 0.5519 - val_accuracy: 0.7370\n",
      "Epoch 138/780\n",
      "684/684 [==============================] - 1s 939us/step - loss: 0.5641 - accuracy: 0.7213 - val_loss: 0.5530 - val_accuracy: 0.7383\n",
      "Epoch 139/780\n",
      "684/684 [==============================] - 1s 953us/step - loss: 0.5643 - accuracy: 0.7214 - val_loss: 0.5512 - val_accuracy: 0.7383\n",
      "Epoch 140/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5642 - accuracy: 0.7216 - val_loss: 0.5527 - val_accuracy: 0.7378\n",
      "Epoch 141/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5643 - accuracy: 0.7218 - val_loss: 0.5557 - val_accuracy: 0.7380\n",
      "Epoch 142/780\n",
      "684/684 [==============================] - 1s 965us/step - loss: 0.5644 - accuracy: 0.7221 - val_loss: 0.5537 - val_accuracy: 0.7380\n",
      "Epoch 143/780\n",
      "684/684 [==============================] - 1s 933us/step - loss: 0.5645 - accuracy: 0.7215 - val_loss: 0.5512 - val_accuracy: 0.7375\n",
      "Epoch 144/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5643 - accuracy: 0.7217 - val_loss: 0.5531 - val_accuracy: 0.7375\n",
      "Epoch 145/780\n",
      "684/684 [==============================] - 1s 936us/step - loss: 0.5641 - accuracy: 0.7217 - val_loss: 0.5509 - val_accuracy: 0.7380\n",
      "Epoch 146/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5639 - accuracy: 0.7217 - val_loss: 0.5534 - val_accuracy: 0.7383\n",
      "Epoch 147/780\n",
      "684/684 [==============================] - 1s 941us/step - loss: 0.5644 - accuracy: 0.7213 - val_loss: 0.5533 - val_accuracy: 0.7383\n",
      "Epoch 148/780\n",
      "684/684 [==============================] - 1s 982us/step - loss: 0.5642 - accuracy: 0.7218 - val_loss: 0.5525 - val_accuracy: 0.7375\n",
      "Epoch 149/780\n",
      "684/684 [==============================] - 1s 923us/step - loss: 0.5642 - accuracy: 0.7217 - val_loss: 0.5518 - val_accuracy: 0.7378\n",
      "Epoch 150/780\n",
      "684/684 [==============================] - 1s 940us/step - loss: 0.5641 - accuracy: 0.7217 - val_loss: 0.5524 - val_accuracy: 0.7378\n",
      "Epoch 151/780\n",
      "684/684 [==============================] - 1s 987us/step - loss: 0.5640 - accuracy: 0.7217 - val_loss: 0.5512 - val_accuracy: 0.7385\n",
      "Epoch 152/780\n",
      "684/684 [==============================] - 1s 923us/step - loss: 0.5642 - accuracy: 0.7214 - val_loss: 0.5525 - val_accuracy: 0.7383\n",
      "Epoch 153/780\n",
      "684/684 [==============================] - 1s 943us/step - loss: 0.5643 - accuracy: 0.7219 - val_loss: 0.5517 - val_accuracy: 0.7380\n",
      "Epoch 154/780\n",
      "684/684 [==============================] - 1s 924us/step - loss: 0.5638 - accuracy: 0.7214 - val_loss: 0.5540 - val_accuracy: 0.7378\n",
      "Epoch 155/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5642 - accuracy: 0.7222 - val_loss: 0.5531 - val_accuracy: 0.7378\n",
      "Epoch 156/780\n",
      "684/684 [==============================] - 1s 942us/step - loss: 0.5643 - accuracy: 0.7216 - val_loss: 0.5522 - val_accuracy: 0.7378\n",
      "Epoch 157/780\n",
      "684/684 [==============================] - 1s 986us/step - loss: 0.5642 - accuracy: 0.7217 - val_loss: 0.5529 - val_accuracy: 0.7380\n",
      "Epoch 158/780\n",
      "684/684 [==============================] - 1s 995us/step - loss: 0.5643 - accuracy: 0.7216 - val_loss: 0.5528 - val_accuracy: 0.7385\n",
      "Epoch 159/780\n",
      "684/684 [==============================] - 1s 937us/step - loss: 0.5643 - accuracy: 0.7217 - val_loss: 0.5509 - val_accuracy: 0.7378\n",
      "Epoch 160/780\n",
      "684/684 [==============================] - 1s 984us/step - loss: 0.5642 - accuracy: 0.7216 - val_loss: 0.5535 - val_accuracy: 0.7378\n",
      "Epoch 161/780\n",
      "684/684 [==============================] - 1s 965us/step - loss: 0.5643 - accuracy: 0.7213 - val_loss: 0.5518 - val_accuracy: 0.7378\n",
      "Epoch 162/780\n",
      "684/684 [==============================] - 1s 951us/step - loss: 0.5641 - accuracy: 0.7215 - val_loss: 0.5528 - val_accuracy: 0.7380\n",
      "Epoch 163/780\n",
      "684/684 [==============================] - 1s 950us/step - loss: 0.5641 - accuracy: 0.7218 - val_loss: 0.5517 - val_accuracy: 0.7378\n",
      "Epoch 164/780\n",
      "684/684 [==============================] - 1s 968us/step - loss: 0.5644 - accuracy: 0.7216 - val_loss: 0.5518 - val_accuracy: 0.7378\n",
      "Epoch 165/780\n",
      "684/684 [==============================] - 1s 954us/step - loss: 0.5641 - accuracy: 0.7219 - val_loss: 0.5523 - val_accuracy: 0.7380\n",
      "Epoch 166/780\n",
      "684/684 [==============================] - 1s 939us/step - loss: 0.5642 - accuracy: 0.7217 - val_loss: 0.5527 - val_accuracy: 0.7378\n",
      "Epoch 167/780\n",
      "684/684 [==============================] - 1s 940us/step - loss: 0.5643 - accuracy: 0.7214 - val_loss: 0.5519 - val_accuracy: 0.7383\n",
      "Epoch 168/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5640 - accuracy: 0.7217 - val_loss: 0.5545 - val_accuracy: 0.7385\n",
      "Epoch 169/780\n",
      "684/684 [==============================] - 1s 965us/step - loss: 0.5644 - accuracy: 0.7218 - val_loss: 0.5541 - val_accuracy: 0.7383\n",
      "Epoch 170/780\n",
      "684/684 [==============================] - 1s 914us/step - loss: 0.5642 - accuracy: 0.7217 - val_loss: 0.5524 - val_accuracy: 0.7383\n",
      "Epoch 171/780\n",
      "684/684 [==============================] - 1s 993us/step - loss: 0.5642 - accuracy: 0.7217 - val_loss: 0.5518 - val_accuracy: 0.7372\n",
      "Epoch 172/780\n",
      "684/684 [==============================] - 1s 955us/step - loss: 0.5644 - accuracy: 0.7220 - val_loss: 0.5510 - val_accuracy: 0.7380\n",
      "Epoch 173/780\n",
      "684/684 [==============================] - 1s 941us/step - loss: 0.5641 - accuracy: 0.7221 - val_loss: 0.5527 - val_accuracy: 0.7380\n",
      "Epoch 174/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5643 - accuracy: 0.7218 - val_loss: 0.5532 - val_accuracy: 0.7375\n",
      "Epoch 175/780\n",
      "684/684 [==============================] - 1s 932us/step - loss: 0.5640 - accuracy: 0.7217 - val_loss: 0.5514 - val_accuracy: 0.7383\n",
      "Epoch 176/780\n",
      "684/684 [==============================] - 1s 981us/step - loss: 0.5641 - accuracy: 0.7217 - val_loss: 0.5524 - val_accuracy: 0.7372\n",
      "Epoch 177/780\n",
      "684/684 [==============================] - 1s 978us/step - loss: 0.5643 - accuracy: 0.7220 - val_loss: 0.5519 - val_accuracy: 0.7380\n",
      "Epoch 178/780\n",
      "684/684 [==============================] - 1s 961us/step - loss: 0.5641 - accuracy: 0.7218 - val_loss: 0.5516 - val_accuracy: 0.7378\n",
      "Epoch 179/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5640 - accuracy: 0.7217 - val_loss: 0.5519 - val_accuracy: 0.7367\n",
      "Epoch 180/780\n",
      "684/684 [==============================] - 1s 989us/step - loss: 0.5642 - accuracy: 0.7216 - val_loss: 0.5520 - val_accuracy: 0.7383\n",
      "Epoch 181/780\n",
      "684/684 [==============================] - 1s 937us/step - loss: 0.5641 - accuracy: 0.7212 - val_loss: 0.5511 - val_accuracy: 0.7383\n",
      "Epoch 182/780\n",
      "684/684 [==============================] - 1s 961us/step - loss: 0.5642 - accuracy: 0.7218 - val_loss: 0.5521 - val_accuracy: 0.7372\n",
      "Epoch 183/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5643 - accuracy: 0.7218 - val_loss: 0.5523 - val_accuracy: 0.7372\n",
      "Epoch 184/780\n",
      "684/684 [==============================] - 1s 961us/step - loss: 0.5641 - accuracy: 0.7216 - val_loss: 0.5521 - val_accuracy: 0.7378\n",
      "Epoch 185/780\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.5641 - accuracy: 0.7215 - val_loss: 0.5519 - val_accuracy: 0.7380\n",
      "Epoch 186/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5642 - accuracy: 0.7216 - val_loss: 0.5513 - val_accuracy: 0.7383\n",
      "Epoch 187/780\n",
      "684/684 [==============================] - 1s 937us/step - loss: 0.5639 - accuracy: 0.7219 - val_loss: 0.5530 - val_accuracy: 0.7372\n",
      "Epoch 188/780\n",
      "684/684 [==============================] - 1s 942us/step - loss: 0.5643 - accuracy: 0.7217 - val_loss: 0.5524 - val_accuracy: 0.7370\n",
      "Epoch 189/780\n",
      "684/684 [==============================] - 1s 999us/step - loss: 0.5641 - accuracy: 0.7217 - val_loss: 0.5526 - val_accuracy: 0.7375\n",
      "Epoch 190/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7212 - val_loss: 0.5520 - val_accuracy: 0.7378\n",
      "Epoch 191/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5642 - accuracy: 0.7212 - val_loss: 0.5520 - val_accuracy: 0.7372\n",
      "Epoch 192/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5643 - accuracy: 0.7215 - val_loss: 0.5522 - val_accuracy: 0.7378\n",
      "Epoch 193/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7217 - val_loss: 0.5517 - val_accuracy: 0.7378\n",
      "Epoch 194/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7216 - val_loss: 0.5528 - val_accuracy: 0.7372\n",
      "Epoch 195/780\n",
      "684/684 [==============================] - 1s 917us/step - loss: 0.5644 - accuracy: 0.7218 - val_loss: 0.5515 - val_accuracy: 0.7380\n",
      "Epoch 196/780\n",
      "684/684 [==============================] - 1s 955us/step - loss: 0.5642 - accuracy: 0.7213 - val_loss: 0.5518 - val_accuracy: 0.7383\n",
      "Epoch 197/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7219 - val_loss: 0.5525 - val_accuracy: 0.7383\n",
      "Epoch 198/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7212 - val_loss: 0.5523 - val_accuracy: 0.7380\n",
      "Epoch 199/780\n",
      "684/684 [==============================] - 1s 919us/step - loss: 0.5641 - accuracy: 0.7218 - val_loss: 0.5523 - val_accuracy: 0.7380\n",
      "Epoch 200/780\n",
      "684/684 [==============================] - 1s 970us/step - loss: 0.5641 - accuracy: 0.7218 - val_loss: 0.5544 - val_accuracy: 0.7378\n",
      "Epoch 201/780\n",
      "684/684 [==============================] - 1s 955us/step - loss: 0.5640 - accuracy: 0.7222 - val_loss: 0.5561 - val_accuracy: 0.7378\n",
      "Epoch 202/780\n",
      "684/684 [==============================] - 1s 943us/step - loss: 0.5640 - accuracy: 0.7219 - val_loss: 0.5517 - val_accuracy: 0.7380\n",
      "Epoch 203/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5640 - accuracy: 0.7218 - val_loss: 0.5525 - val_accuracy: 0.7367\n",
      "Epoch 204/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5639 - accuracy: 0.7212 - val_loss: 0.5521 - val_accuracy: 0.7372\n",
      "Epoch 205/780\n",
      "684/684 [==============================] - 1s 942us/step - loss: 0.5641 - accuracy: 0.7214 - val_loss: 0.5515 - val_accuracy: 0.7380\n",
      "Epoch 206/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7217 - val_loss: 0.5561 - val_accuracy: 0.7372\n",
      "Epoch 207/780\n",
      "684/684 [==============================] - 1s 923us/step - loss: 0.5641 - accuracy: 0.7215 - val_loss: 0.5530 - val_accuracy: 0.7378\n",
      "Epoch 208/780\n",
      "684/684 [==============================] - 1s 930us/step - loss: 0.5641 - accuracy: 0.7219 - val_loss: 0.5520 - val_accuracy: 0.7380\n",
      "Epoch 209/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5521 - val_accuracy: 0.7378\n",
      "Epoch 210/780\n",
      "684/684 [==============================] - 1s 956us/step - loss: 0.5640 - accuracy: 0.7214 - val_loss: 0.5526 - val_accuracy: 0.7380\n",
      "Epoch 211/780\n",
      "684/684 [==============================] - 1s 934us/step - loss: 0.5642 - accuracy: 0.7217 - val_loss: 0.5535 - val_accuracy: 0.7378\n",
      "Epoch 212/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5507 - val_accuracy: 0.7380\n",
      "Epoch 213/780\n",
      "684/684 [==============================] - 1s 919us/step - loss: 0.5640 - accuracy: 0.7215 - val_loss: 0.5504 - val_accuracy: 0.7383\n",
      "Epoch 214/780\n",
      "684/684 [==============================] - 1s 940us/step - loss: 0.5640 - accuracy: 0.7216 - val_loss: 0.5520 - val_accuracy: 0.7380\n",
      "Epoch 215/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5640 - accuracy: 0.7216 - val_loss: 0.5516 - val_accuracy: 0.7383\n",
      "Epoch 216/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7218 - val_loss: 0.5531 - val_accuracy: 0.7380\n",
      "Epoch 217/780\n",
      "684/684 [==============================] - 1s 978us/step - loss: 0.5640 - accuracy: 0.7220 - val_loss: 0.5540 - val_accuracy: 0.7372\n",
      "Epoch 218/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7216 - val_loss: 0.5515 - val_accuracy: 0.7380\n",
      "Epoch 219/780\n",
      "684/684 [==============================] - 1s 992us/step - loss: 0.5641 - accuracy: 0.7215 - val_loss: 0.5537 - val_accuracy: 0.7380\n",
      "Epoch 220/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7218 - val_loss: 0.5518 - val_accuracy: 0.7375\n",
      "Epoch 221/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7217 - val_loss: 0.5532 - val_accuracy: 0.7375\n",
      "Epoch 222/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7217 - val_loss: 0.5520 - val_accuracy: 0.7380\n",
      "Epoch 223/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7220 - val_loss: 0.5535 - val_accuracy: 0.7378\n",
      "Epoch 224/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7217 - val_loss: 0.5526 - val_accuracy: 0.7380\n",
      "Epoch 225/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5640 - accuracy: 0.7220 - val_loss: 0.5510 - val_accuracy: 0.7378\n",
      "Epoch 226/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7221 - val_loss: 0.5523 - val_accuracy: 0.7370\n",
      "Epoch 227/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7219 - val_loss: 0.5507 - val_accuracy: 0.7378\n",
      "Epoch 228/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7218 - val_loss: 0.5528 - val_accuracy: 0.7378\n",
      "Epoch 229/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5640 - accuracy: 0.7218 - val_loss: 0.5514 - val_accuracy: 0.7372\n",
      "Epoch 230/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7216 - val_loss: 0.5525 - val_accuracy: 0.7380\n",
      "Epoch 231/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7215 - val_loss: 0.5516 - val_accuracy: 0.7375\n",
      "Epoch 232/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7216 - val_loss: 0.5525 - val_accuracy: 0.7375\n",
      "Epoch 233/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7219 - val_loss: 0.5521 - val_accuracy: 0.7375\n",
      "Epoch 234/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7217 - val_loss: 0.5508 - val_accuracy: 0.7370\n",
      "Epoch 235/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7218 - val_loss: 0.5516 - val_accuracy: 0.7378\n",
      "Epoch 236/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7213 - val_loss: 0.5511 - val_accuracy: 0.7380\n",
      "Epoch 237/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7216 - val_loss: 0.5529 - val_accuracy: 0.7375\n",
      "Epoch 238/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7217 - val_loss: 0.5527 - val_accuracy: 0.7367\n",
      "Epoch 239/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7217 - val_loss: 0.5519 - val_accuracy: 0.7380\n",
      "Epoch 240/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5641 - accuracy: 0.7221 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 241/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5640 - accuracy: 0.7215 - val_loss: 0.5527 - val_accuracy: 0.7372\n",
      "Epoch 242/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7215 - val_loss: 0.5523 - val_accuracy: 0.7378\n",
      "Epoch 243/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5533 - val_accuracy: 0.7375\n",
      "Epoch 244/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7217 - val_loss: 0.5518 - val_accuracy: 0.7378\n",
      "Epoch 245/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7219 - val_loss: 0.5525 - val_accuracy: 0.7375\n",
      "Epoch 246/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7219 - val_loss: 0.5523 - val_accuracy: 0.7375\n",
      "Epoch 247/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7218 - val_loss: 0.5518 - val_accuracy: 0.7380\n",
      "Epoch 248/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7217 - val_loss: 0.5529 - val_accuracy: 0.7380\n",
      "Epoch 249/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5636 - accuracy: 0.7216 - val_loss: 0.5536 - val_accuracy: 0.7372\n",
      "Epoch 250/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5640 - accuracy: 0.7221 - val_loss: 0.5529 - val_accuracy: 0.7370\n",
      "Epoch 251/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7217 - val_loss: 0.5531 - val_accuracy: 0.7372\n",
      "Epoch 252/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7217 - val_loss: 0.5528 - val_accuracy: 0.7380\n",
      "Epoch 253/780\n",
      "684/684 [==============================] - 1s 998us/step - loss: 0.5637 - accuracy: 0.7218 - val_loss: 0.5515 - val_accuracy: 0.7375\n",
      "Epoch 254/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.7219 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 255/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7218 - val_loss: 0.5509 - val_accuracy: 0.7378\n",
      "Epoch 256/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7217 - val_loss: 0.5516 - val_accuracy: 0.7380\n",
      "Epoch 257/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5509 - val_accuracy: 0.7378\n",
      "Epoch 258/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5636 - accuracy: 0.7219 - val_loss: 0.5520 - val_accuracy: 0.7383\n",
      "Epoch 259/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7219 - val_loss: 0.5519 - val_accuracy: 0.7380\n",
      "Epoch 260/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7218 - val_loss: 0.5513 - val_accuracy: 0.7383\n",
      "Epoch 261/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7217 - val_loss: 0.5518 - val_accuracy: 0.7378\n",
      "Epoch 262/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7218 - val_loss: 0.5526 - val_accuracy: 0.7372\n",
      "Epoch 263/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7215 - val_loss: 0.5527 - val_accuracy: 0.7378\n",
      "Epoch 264/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5512 - val_accuracy: 0.7378\n",
      "Epoch 265/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7214 - val_loss: 0.5511 - val_accuracy: 0.7372\n",
      "Epoch 266/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7218 - val_loss: 0.5524 - val_accuracy: 0.7372\n",
      "Epoch 267/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7216 - val_loss: 0.5514 - val_accuracy: 0.7378\n",
      "Epoch 268/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5636 - accuracy: 0.7220 - val_loss: 0.5536 - val_accuracy: 0.7372\n",
      "Epoch 269/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5636 - accuracy: 0.7212 - val_loss: 0.5505 - val_accuracy: 0.7380\n",
      "Epoch 270/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7215 - val_loss: 0.5526 - val_accuracy: 0.7372\n",
      "Epoch 271/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.7217 - val_loss: 0.5519 - val_accuracy: 0.7380\n",
      "Epoch 272/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5524 - val_accuracy: 0.7372\n",
      "Epoch 273/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5636 - accuracy: 0.7220 - val_loss: 0.5527 - val_accuracy: 0.7372\n",
      "Epoch 274/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7219 - val_loss: 0.5522 - val_accuracy: 0.7372\n",
      "Epoch 275/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7214 - val_loss: 0.5536 - val_accuracy: 0.7378\n",
      "Epoch 276/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7216 - val_loss: 0.5523 - val_accuracy: 0.7372\n",
      "Epoch 277/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5526 - val_accuracy: 0.7375\n",
      "Epoch 278/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5637 - accuracy: 0.7214 - val_loss: 0.5521 - val_accuracy: 0.7375\n",
      "Epoch 279/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7214 - val_loss: 0.5537 - val_accuracy: 0.7378\n",
      "Epoch 280/780\n",
      "684/684 [==============================] - 1s 996us/step - loss: 0.5636 - accuracy: 0.7215 - val_loss: 0.5516 - val_accuracy: 0.7370\n",
      "Epoch 281/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5636 - accuracy: 0.7219 - val_loss: 0.5513 - val_accuracy: 0.7380\n",
      "Epoch 282/780\n",
      "684/684 [==============================] - 1s 953us/step - loss: 0.5636 - accuracy: 0.7217 - val_loss: 0.5511 - val_accuracy: 0.7380\n",
      "Epoch 283/780\n",
      "684/684 [==============================] - 1s 934us/step - loss: 0.5638 - accuracy: 0.7217 - val_loss: 0.5505 - val_accuracy: 0.7380\n",
      "Epoch 284/780\n",
      "684/684 [==============================] - 1s 963us/step - loss: 0.5637 - accuracy: 0.7219 - val_loss: 0.5523 - val_accuracy: 0.7380\n",
      "Epoch 285/780\n",
      "684/684 [==============================] - 1s 912us/step - loss: 0.5639 - accuracy: 0.7217 - val_loss: 0.5523 - val_accuracy: 0.7380\n",
      "Epoch 286/780\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.5635 - accuracy: 0.7217 - val_loss: 0.5520 - val_accuracy: 0.7372\n",
      "Epoch 287/780\n",
      "684/684 [==============================] - 1s 991us/step - loss: 0.5636 - accuracy: 0.7219 - val_loss: 0.5517 - val_accuracy: 0.7370\n",
      "Epoch 288/780\n",
      "684/684 [==============================] - 1s 914us/step - loss: 0.5638 - accuracy: 0.7217 - val_loss: 0.5516 - val_accuracy: 0.7372\n",
      "Epoch 289/780\n",
      "684/684 [==============================] - 1s 964us/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5517 - val_accuracy: 0.7375\n",
      "Epoch 290/780\n",
      "684/684 [==============================] - 1s 977us/step - loss: 0.5636 - accuracy: 0.7215 - val_loss: 0.5516 - val_accuracy: 0.7378\n",
      "Epoch 291/780\n",
      "684/684 [==============================] - 1s 956us/step - loss: 0.5635 - accuracy: 0.7216 - val_loss: 0.5531 - val_accuracy: 0.7375\n",
      "Epoch 292/780\n",
      "684/684 [==============================] - 1s 942us/step - loss: 0.5635 - accuracy: 0.7216 - val_loss: 0.5522 - val_accuracy: 0.7375\n",
      "Epoch 293/780\n",
      "684/684 [==============================] - 1s 994us/step - loss: 0.5636 - accuracy: 0.7219 - val_loss: 0.5531 - val_accuracy: 0.7375\n",
      "Epoch 294/780\n",
      "684/684 [==============================] - 1s 914us/step - loss: 0.5637 - accuracy: 0.7217 - val_loss: 0.5510 - val_accuracy: 0.7375\n",
      "Epoch 295/780\n",
      "684/684 [==============================] - 1s 950us/step - loss: 0.5636 - accuracy: 0.7217 - val_loss: 0.5537 - val_accuracy: 0.7383\n",
      "Epoch 296/780\n",
      "684/684 [==============================] - 1s 963us/step - loss: 0.5635 - accuracy: 0.7210 - val_loss: 0.5509 - val_accuracy: 0.7378\n",
      "Epoch 297/780\n",
      "684/684 [==============================] - 1s 967us/step - loss: 0.5636 - accuracy: 0.7218 - val_loss: 0.5523 - val_accuracy: 0.7372\n",
      "Epoch 298/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5517 - val_accuracy: 0.7383\n",
      "Epoch 299/780\n",
      "684/684 [==============================] - 1s 958us/step - loss: 0.5636 - accuracy: 0.7216 - val_loss: 0.5517 - val_accuracy: 0.7378\n",
      "Epoch 300/780\n",
      "684/684 [==============================] - 1s 930us/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5522 - val_accuracy: 0.7378\n",
      "Epoch 301/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.7217 - val_loss: 0.5514 - val_accuracy: 0.7380\n",
      "Epoch 302/780\n",
      "684/684 [==============================] - 1s 931us/step - loss: 0.5636 - accuracy: 0.7214 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 303/780\n",
      "684/684 [==============================] - 1s 995us/step - loss: 0.5634 - accuracy: 0.7221 - val_loss: 0.5518 - val_accuracy: 0.7375\n",
      "Epoch 304/780\n",
      "684/684 [==============================] - 1s 948us/step - loss: 0.5635 - accuracy: 0.7218 - val_loss: 0.5517 - val_accuracy: 0.7380\n",
      "Epoch 305/780\n",
      "684/684 [==============================] - 1s 942us/step - loss: 0.5634 - accuracy: 0.7220 - val_loss: 0.5520 - val_accuracy: 0.7383\n",
      "Epoch 306/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.7215 - val_loss: 0.5516 - val_accuracy: 0.7385\n",
      "Epoch 307/780\n",
      "684/684 [==============================] - 1s 976us/step - loss: 0.5636 - accuracy: 0.7217 - val_loss: 0.5526 - val_accuracy: 0.7375\n",
      "Epoch 308/780\n",
      "684/684 [==============================] - 1s 987us/step - loss: 0.5634 - accuracy: 0.7215 - val_loss: 0.5516 - val_accuracy: 0.7372\n",
      "Epoch 309/780\n",
      "684/684 [==============================] - 1s 969us/step - loss: 0.5635 - accuracy: 0.7217 - val_loss: 0.5522 - val_accuracy: 0.7380\n",
      "Epoch 310/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5636 - accuracy: 0.7212 - val_loss: 0.5516 - val_accuracy: 0.7372\n",
      "Epoch 311/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.7218 - val_loss: 0.5514 - val_accuracy: 0.7375\n",
      "Epoch 312/780\n",
      "684/684 [==============================] - 1s 963us/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5516 - val_accuracy: 0.7370\n",
      "Epoch 313/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5512 - val_accuracy: 0.7378\n",
      "Epoch 314/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.7219 - val_loss: 0.5508 - val_accuracy: 0.7380\n",
      "Epoch 315/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5523 - val_accuracy: 0.7378\n",
      "Epoch 316/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5519 - val_accuracy: 0.7375\n",
      "Epoch 317/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5631 - accuracy: 0.7219 - val_loss: 0.5521 - val_accuracy: 0.7383\n",
      "Epoch 318/780\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.7216 - val_loss: 0.5526 - val_accuracy: 0.7383\n",
      "Epoch 319/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7216 - val_loss: 0.5523 - val_accuracy: 0.7380\n",
      "Epoch 320/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.7217 - val_loss: 0.5516 - val_accuracy: 0.7383\n",
      "Epoch 321/780\n",
      "684/684 [==============================] - 1s 945us/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5523 - val_accuracy: 0.7378\n",
      "Epoch 322/780\n",
      "684/684 [==============================] - 1s 993us/step - loss: 0.5633 - accuracy: 0.7221 - val_loss: 0.5525 - val_accuracy: 0.7378\n",
      "Epoch 323/780\n",
      "684/684 [==============================] - 1s 955us/step - loss: 0.5632 - accuracy: 0.7223 - val_loss: 0.5520 - val_accuracy: 0.7383\n",
      "Epoch 324/780\n",
      "684/684 [==============================] - 1s 926us/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5514 - val_accuracy: 0.7380\n",
      "Epoch 325/780\n",
      "684/684 [==============================] - 1s 989us/step - loss: 0.5634 - accuracy: 0.7218 - val_loss: 0.5509 - val_accuracy: 0.7372\n",
      "Epoch 326/780\n",
      "684/684 [==============================] - 1s 935us/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5522 - val_accuracy: 0.7380\n",
      "Epoch 327/780\n",
      "684/684 [==============================] - 1s 934us/step - loss: 0.5633 - accuracy: 0.7219 - val_loss: 0.5520 - val_accuracy: 0.7383\n",
      "Epoch 328/780\n",
      "684/684 [==============================] - 1s 980us/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5521 - val_accuracy: 0.7372\n",
      "Epoch 329/780\n",
      "684/684 [==============================] - 1s 937us/step - loss: 0.5633 - accuracy: 0.7218 - val_loss: 0.5510 - val_accuracy: 0.7378\n",
      "Epoch 330/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5631 - accuracy: 0.7217 - val_loss: 0.5523 - val_accuracy: 0.7372\n",
      "Epoch 331/780\n",
      "684/684 [==============================] - 1s 999us/step - loss: 0.5633 - accuracy: 0.7219 - val_loss: 0.5518 - val_accuracy: 0.7380\n",
      "Epoch 332/780\n",
      "684/684 [==============================] - 1s 969us/step - loss: 0.5633 - accuracy: 0.7217 - val_loss: 0.5522 - val_accuracy: 0.7383\n",
      "Epoch 333/780\n",
      "684/684 [==============================] - 1s 916us/step - loss: 0.5634 - accuracy: 0.7218 - val_loss: 0.5517 - val_accuracy: 0.7380\n",
      "Epoch 334/780\n",
      "684/684 [==============================] - 1s 935us/step - loss: 0.5633 - accuracy: 0.7218 - val_loss: 0.5528 - val_accuracy: 0.7380\n",
      "Epoch 335/780\n",
      "684/684 [==============================] - 1s 899us/step - loss: 0.5631 - accuracy: 0.7218 - val_loss: 0.5523 - val_accuracy: 0.7375\n",
      "Epoch 336/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7218 - val_loss: 0.5514 - val_accuracy: 0.7372\n",
      "Epoch 337/780\n",
      "684/684 [==============================] - 1s 935us/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5516 - val_accuracy: 0.7380\n",
      "Epoch 338/780\n",
      "684/684 [==============================] - 1s 994us/step - loss: 0.5631 - accuracy: 0.7220 - val_loss: 0.5527 - val_accuracy: 0.7370\n",
      "Epoch 339/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.7215 - val_loss: 0.5513 - val_accuracy: 0.7380\n",
      "Epoch 340/780\n",
      "684/684 [==============================] - 1s 992us/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5518 - val_accuracy: 0.7372\n",
      "Epoch 341/780\n",
      "684/684 [==============================] - 1s 976us/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5524 - val_accuracy: 0.7378\n",
      "Epoch 342/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5636 - accuracy: 0.7217 - val_loss: 0.5520 - val_accuracy: 0.7380\n",
      "Epoch 343/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5515 - val_accuracy: 0.7378\n",
      "Epoch 344/780\n",
      "684/684 [==============================] - 1s 961us/step - loss: 0.5631 - accuracy: 0.7217 - val_loss: 0.5520 - val_accuracy: 0.7380\n",
      "Epoch 345/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7218 - val_loss: 0.5519 - val_accuracy: 0.7383\n",
      "Epoch 346/780\n",
      "684/684 [==============================] - 1s 934us/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5509 - val_accuracy: 0.7380\n",
      "Epoch 347/780\n",
      "684/684 [==============================] - 1s 991us/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5515 - val_accuracy: 0.7380\n",
      "Epoch 348/780\n",
      "684/684 [==============================] - 1s 981us/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5511 - val_accuracy: 0.7380\n",
      "Epoch 349/780\n",
      "684/684 [==============================] - 1s 945us/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5526 - val_accuracy: 0.7378\n",
      "Epoch 350/780\n",
      "684/684 [==============================] - 1s 989us/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5524 - val_accuracy: 0.7380\n",
      "Epoch 351/780\n",
      "684/684 [==============================] - 1s 949us/step - loss: 0.5631 - accuracy: 0.7219 - val_loss: 0.5517 - val_accuracy: 0.7375\n",
      "Epoch 352/780\n",
      "684/684 [==============================] - 1s 976us/step - loss: 0.5633 - accuracy: 0.7217 - val_loss: 0.5511 - val_accuracy: 0.7378\n",
      "Epoch 353/780\n",
      "684/684 [==============================] - 1s 926us/step - loss: 0.5631 - accuracy: 0.7218 - val_loss: 0.5519 - val_accuracy: 0.7378\n",
      "Epoch 354/780\n",
      "684/684 [==============================] - 1s 959us/step - loss: 0.5633 - accuracy: 0.7219 - val_loss: 0.5526 - val_accuracy: 0.7372\n",
      "Epoch 355/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7215 - val_loss: 0.5523 - val_accuracy: 0.7367\n",
      "Epoch 356/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7219 - val_loss: 0.5528 - val_accuracy: 0.7367\n",
      "Epoch 357/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5631 - accuracy: 0.7217 - val_loss: 0.5522 - val_accuracy: 0.7378\n",
      "Epoch 358/780\n",
      "684/684 [==============================] - 1s 929us/step - loss: 0.5631 - accuracy: 0.7218 - val_loss: 0.5513 - val_accuracy: 0.7372\n",
      "Epoch 359/780\n",
      "684/684 [==============================] - 1s 974us/step - loss: 0.5635 - accuracy: 0.7218 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 360/780\n",
      "684/684 [==============================] - 1s 912us/step - loss: 0.5632 - accuracy: 0.7216 - val_loss: 0.5519 - val_accuracy: 0.7375\n",
      "Epoch 361/780\n",
      "684/684 [==============================] - 1s 932us/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5511 - val_accuracy: 0.7380\n",
      "Epoch 362/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7216 - val_loss: 0.5524 - val_accuracy: 0.7372\n",
      "Epoch 363/780\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.5633 - accuracy: 0.7217 - val_loss: 0.5520 - val_accuracy: 0.7380\n",
      "Epoch 364/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5510 - val_accuracy: 0.7380\n",
      "Epoch 365/780\n",
      "684/684 [==============================] - 1s 973us/step - loss: 0.5632 - accuracy: 0.7219 - val_loss: 0.5520 - val_accuracy: 0.7378\n",
      "Epoch 366/780\n",
      "684/684 [==============================] - 1s 953us/step - loss: 0.5632 - accuracy: 0.7219 - val_loss: 0.5524 - val_accuracy: 0.7378\n",
      "Epoch 367/780\n",
      "684/684 [==============================] - 1s 977us/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5515 - val_accuracy: 0.7378\n",
      "Epoch 368/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7218 - val_loss: 0.5513 - val_accuracy: 0.7380\n",
      "Epoch 369/780\n",
      "684/684 [==============================] - 1s 924us/step - loss: 0.5633 - accuracy: 0.7220 - val_loss: 0.5524 - val_accuracy: 0.7380\n",
      "Epoch 370/780\n",
      "684/684 [==============================] - 1s 990us/step - loss: 0.5634 - accuracy: 0.7219 - val_loss: 0.5510 - val_accuracy: 0.7380\n",
      "Epoch 371/780\n",
      "684/684 [==============================] - 1s 989us/step - loss: 0.5633 - accuracy: 0.7218 - val_loss: 0.5530 - val_accuracy: 0.7380\n",
      "Epoch 372/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7217 - val_loss: 0.5526 - val_accuracy: 0.7378\n",
      "Epoch 373/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7220 - val_loss: 0.5518 - val_accuracy: 0.7378\n",
      "Epoch 374/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5521 - val_accuracy: 0.7378\n",
      "Epoch 375/780\n",
      "684/684 [==============================] - 1s 998us/step - loss: 0.5630 - accuracy: 0.7220 - val_loss: 0.5535 - val_accuracy: 0.7372\n",
      "Epoch 376/780\n",
      "684/684 [==============================] - 1s 968us/step - loss: 0.5632 - accuracy: 0.7219 - val_loss: 0.5515 - val_accuracy: 0.7375\n",
      "Epoch 377/780\n",
      "684/684 [==============================] - 1s 927us/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5512 - val_accuracy: 0.7380\n",
      "Epoch 378/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7219 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 379/780\n",
      "684/684 [==============================] - 1s 953us/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5512 - val_accuracy: 0.7378\n",
      "Epoch 380/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5630 - accuracy: 0.7219 - val_loss: 0.5511 - val_accuracy: 0.7378\n",
      "Epoch 381/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5535 - val_accuracy: 0.7378\n",
      "Epoch 382/780\n",
      "684/684 [==============================] - 1s 949us/step - loss: 0.5633 - accuracy: 0.7220 - val_loss: 0.5514 - val_accuracy: 0.7378\n",
      "Epoch 383/780\n",
      "684/684 [==============================] - 1s 974us/step - loss: 0.5632 - accuracy: 0.7219 - val_loss: 0.5526 - val_accuracy: 0.7375\n",
      "Epoch 384/780\n",
      "684/684 [==============================] - 1s 953us/step - loss: 0.5630 - accuracy: 0.7219 - val_loss: 0.5531 - val_accuracy: 0.7372\n",
      "Epoch 385/780\n",
      "684/684 [==============================] - 1s 964us/step - loss: 0.5632 - accuracy: 0.7217 - val_loss: 0.5510 - val_accuracy: 0.7375\n",
      "Epoch 386/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7221 - val_loss: 0.5504 - val_accuracy: 0.7378\n",
      "Epoch 387/780\n",
      "684/684 [==============================] - 1s 924us/step - loss: 0.5633 - accuracy: 0.7221 - val_loss: 0.5525 - val_accuracy: 0.7375\n",
      "Epoch 388/780\n",
      "684/684 [==============================] - 1s 914us/step - loss: 0.5629 - accuracy: 0.7222 - val_loss: 0.5519 - val_accuracy: 0.7378\n",
      "Epoch 389/780\n",
      "684/684 [==============================] - 1s 978us/step - loss: 0.5630 - accuracy: 0.7221 - val_loss: 0.5535 - val_accuracy: 0.7370\n",
      "Epoch 390/780\n",
      "684/684 [==============================] - 1s 945us/step - loss: 0.5631 - accuracy: 0.7219 - val_loss: 0.5523 - val_accuracy: 0.7372\n",
      "Epoch 391/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7217 - val_loss: 0.5521 - val_accuracy: 0.7378\n",
      "Epoch 392/780\n",
      "684/684 [==============================] - 1s 955us/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5518 - val_accuracy: 0.7372\n",
      "Epoch 393/780\n",
      "684/684 [==============================] - 1s 965us/step - loss: 0.5629 - accuracy: 0.7220 - val_loss: 0.5517 - val_accuracy: 0.7375\n",
      "Epoch 394/780\n",
      "684/684 [==============================] - 1s 937us/step - loss: 0.5630 - accuracy: 0.7216 - val_loss: 0.5517 - val_accuracy: 0.7375\n",
      "Epoch 395/780\n",
      "684/684 [==============================] - 1s 941us/step - loss: 0.5630 - accuracy: 0.7218 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 396/780\n",
      "684/684 [==============================] - 1s 996us/step - loss: 0.5630 - accuracy: 0.7219 - val_loss: 0.5511 - val_accuracy: 0.7375\n",
      "Epoch 397/780\n",
      "684/684 [==============================] - 1s 926us/step - loss: 0.5630 - accuracy: 0.7218 - val_loss: 0.5515 - val_accuracy: 0.7375\n",
      "Epoch 398/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7221 - val_loss: 0.5506 - val_accuracy: 0.7375\n",
      "Epoch 399/780\n",
      "684/684 [==============================] - 1s 980us/step - loss: 0.5629 - accuracy: 0.7220 - val_loss: 0.5521 - val_accuracy: 0.7375\n",
      "Epoch 400/780\n",
      "684/684 [==============================] - 1s 912us/step - loss: 0.5630 - accuracy: 0.7220 - val_loss: 0.5514 - val_accuracy: 0.7375\n",
      "Epoch 401/780\n",
      "684/684 [==============================] - 1s 985us/step - loss: 0.5628 - accuracy: 0.7220 - val_loss: 0.5522 - val_accuracy: 0.7375\n",
      "Epoch 402/780\n",
      "684/684 [==============================] - 1s 956us/step - loss: 0.5631 - accuracy: 0.7219 - val_loss: 0.5525 - val_accuracy: 0.7378\n",
      "Epoch 403/780\n",
      "684/684 [==============================] - 1s 955us/step - loss: 0.5629 - accuracy: 0.7219 - val_loss: 0.5523 - val_accuracy: 0.7375\n",
      "Epoch 404/780\n",
      "684/684 [==============================] - 1s 979us/step - loss: 0.5631 - accuracy: 0.7219 - val_loss: 0.5515 - val_accuracy: 0.7375\n",
      "Epoch 405/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5630 - accuracy: 0.7221 - val_loss: 0.5510 - val_accuracy: 0.7378\n",
      "Epoch 406/780\n",
      "684/684 [==============================] - 1s 928us/step - loss: 0.5630 - accuracy: 0.7219 - val_loss: 0.5509 - val_accuracy: 0.7372\n",
      "Epoch 407/780\n",
      "684/684 [==============================] - 1s 956us/step - loss: 0.5631 - accuracy: 0.7219 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 408/780\n",
      "684/684 [==============================] - 1s 926us/step - loss: 0.5628 - accuracy: 0.7219 - val_loss: 0.5536 - val_accuracy: 0.7378\n",
      "Epoch 409/780\n",
      "684/684 [==============================] - 1s 923us/step - loss: 0.5632 - accuracy: 0.7218 - val_loss: 0.5521 - val_accuracy: 0.7378\n",
      "Epoch 410/780\n",
      "684/684 [==============================] - 1s 887us/step - loss: 0.5630 - accuracy: 0.7221 - val_loss: 0.5526 - val_accuracy: 0.7375\n",
      "Epoch 411/780\n",
      "684/684 [==============================] - 1s 993us/step - loss: 0.5630 - accuracy: 0.7221 - val_loss: 0.5527 - val_accuracy: 0.7372\n",
      "Epoch 412/780\n",
      "684/684 [==============================] - 1s 936us/step - loss: 0.5631 - accuracy: 0.7218 - val_loss: 0.5527 - val_accuracy: 0.7378\n",
      "Epoch 413/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5631 - accuracy: 0.7219 - val_loss: 0.5540 - val_accuracy: 0.7378\n",
      "Epoch 414/780\n",
      "684/684 [==============================] - 1s 940us/step - loss: 0.5629 - accuracy: 0.7219 - val_loss: 0.5511 - val_accuracy: 0.7378\n",
      "Epoch 415/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7220 - val_loss: 0.5517 - val_accuracy: 0.7378\n",
      "Epoch 416/780\n",
      "684/684 [==============================] - 1s 951us/step - loss: 0.5628 - accuracy: 0.7221 - val_loss: 0.5544 - val_accuracy: 0.7378\n",
      "Epoch 417/780\n",
      "684/684 [==============================] - 1s 991us/step - loss: 0.5630 - accuracy: 0.7220 - val_loss: 0.5518 - val_accuracy: 0.7378\n",
      "Epoch 418/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7220 - val_loss: 0.5518 - val_accuracy: 0.7378\n",
      "Epoch 419/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7216 - val_loss: 0.5504 - val_accuracy: 0.7378\n",
      "Epoch 420/780\n",
      "684/684 [==============================] - 1s 982us/step - loss: 0.5630 - accuracy: 0.7220 - val_loss: 0.5511 - val_accuracy: 0.7365\n",
      "Epoch 421/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7217 - val_loss: 0.5509 - val_accuracy: 0.7372\n",
      "Epoch 422/780\n",
      "684/684 [==============================] - 1s 961us/step - loss: 0.5630 - accuracy: 0.7218 - val_loss: 0.5522 - val_accuracy: 0.7378\n",
      "Epoch 423/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7222 - val_loss: 0.5511 - val_accuracy: 0.7370\n",
      "Epoch 424/780\n",
      "684/684 [==============================] - 1s 972us/step - loss: 0.5631 - accuracy: 0.7221 - val_loss: 0.5510 - val_accuracy: 0.7375\n",
      "Epoch 425/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7221 - val_loss: 0.5525 - val_accuracy: 0.7378\n",
      "Epoch 426/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7216 - val_loss: 0.5512 - val_accuracy: 0.7375\n",
      "Epoch 427/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7222 - val_loss: 0.5513 - val_accuracy: 0.7375\n",
      "Epoch 428/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5631 - accuracy: 0.7221 - val_loss: 0.5505 - val_accuracy: 0.7372\n",
      "Epoch 429/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5630 - accuracy: 0.7219 - val_loss: 0.5519 - val_accuracy: 0.7372\n",
      "Epoch 430/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7221 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 431/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7222 - val_loss: 0.5515 - val_accuracy: 0.7375\n",
      "Epoch 432/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7222 - val_loss: 0.5511 - val_accuracy: 0.7378\n",
      "Epoch 433/780\n",
      "684/684 [==============================] - 1s 984us/step - loss: 0.5629 - accuracy: 0.7221 - val_loss: 0.5508 - val_accuracy: 0.7375\n",
      "Epoch 434/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7221 - val_loss: 0.5519 - val_accuracy: 0.7378\n",
      "Epoch 435/780\n",
      "684/684 [==============================] - 1s 973us/step - loss: 0.5631 - accuracy: 0.7221 - val_loss: 0.5510 - val_accuracy: 0.7378\n",
      "Epoch 436/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7222 - val_loss: 0.5523 - val_accuracy: 0.7378\n",
      "Epoch 437/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7219 - val_loss: 0.5513 - val_accuracy: 0.7375\n",
      "Epoch 438/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7221 - val_loss: 0.5517 - val_accuracy: 0.7372\n",
      "Epoch 439/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5630 - accuracy: 0.7218 - val_loss: 0.5511 - val_accuracy: 0.7375\n",
      "Epoch 440/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7220 - val_loss: 0.5521 - val_accuracy: 0.7375\n",
      "Epoch 441/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7218 - val_loss: 0.5519 - val_accuracy: 0.7362\n",
      "Epoch 442/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7219 - val_loss: 0.5510 - val_accuracy: 0.7372\n",
      "Epoch 443/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7221 - val_loss: 0.5510 - val_accuracy: 0.7372\n",
      "Epoch 444/780\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5630 - accuracy: 0.7219 - val_loss: 0.5519 - val_accuracy: 0.7372\n",
      "Epoch 445/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7218 - val_loss: 0.5534 - val_accuracy: 0.7375\n",
      "Epoch 446/780\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7220 - val_loss: 0.5510 - val_accuracy: 0.7372\n",
      "Epoch 447/780\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5630 - accuracy: 0.7222 - val_loss: 0.5519 - val_accuracy: 0.7372\n",
      "Epoch 448/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7222 - val_loss: 0.5535 - val_accuracy: 0.7378\n",
      "Epoch 449/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7220 - val_loss: 0.5525 - val_accuracy: 0.7375\n",
      "Epoch 450/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7221 - val_loss: 0.5511 - val_accuracy: 0.7372\n",
      "Epoch 451/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7222 - val_loss: 0.5512 - val_accuracy: 0.7367\n",
      "Epoch 452/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5632 - accuracy: 0.7220 - val_loss: 0.5507 - val_accuracy: 0.7370\n",
      "Epoch 453/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7222 - val_loss: 0.5520 - val_accuracy: 0.7375\n",
      "Epoch 454/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7223 - val_loss: 0.5506 - val_accuracy: 0.7378\n",
      "Epoch 455/780\n",
      "684/684 [==============================] - 1s 992us/step - loss: 0.5628 - accuracy: 0.7223 - val_loss: 0.5538 - val_accuracy: 0.7367\n",
      "Epoch 456/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5630 - accuracy: 0.7221 - val_loss: 0.5514 - val_accuracy: 0.7367\n",
      "Epoch 457/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5630 - accuracy: 0.7223 - val_loss: 0.5512 - val_accuracy: 0.7367\n",
      "Epoch 458/780\n",
      "684/684 [==============================] - 1s 959us/step - loss: 0.5631 - accuracy: 0.7221 - val_loss: 0.5514 - val_accuracy: 0.7370\n",
      "Epoch 459/780\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.5631 - accuracy: 0.7218 - val_loss: 0.5520 - val_accuracy: 0.7372\n",
      "Epoch 460/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7221 - val_loss: 0.5525 - val_accuracy: 0.7372\n",
      "Epoch 461/780\n",
      "684/684 [==============================] - 1s 937us/step - loss: 0.5624 - accuracy: 0.7223 - val_loss: 0.5505 - val_accuracy: 0.7375\n",
      "Epoch 462/780\n",
      "684/684 [==============================] - 1s 976us/step - loss: 0.5629 - accuracy: 0.7219 - val_loss: 0.5506 - val_accuracy: 0.7370\n",
      "Epoch 463/780\n",
      "684/684 [==============================] - 1s 968us/step - loss: 0.5629 - accuracy: 0.7222 - val_loss: 0.5511 - val_accuracy: 0.7372\n",
      "Epoch 464/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5630 - accuracy: 0.7223 - val_loss: 0.5519 - val_accuracy: 0.7378\n",
      "Epoch 465/780\n",
      "684/684 [==============================] - 1s 985us/step - loss: 0.5628 - accuracy: 0.7221 - val_loss: 0.5513 - val_accuracy: 0.7378\n",
      "Epoch 466/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7222 - val_loss: 0.5511 - val_accuracy: 0.7375\n",
      "Epoch 467/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7221 - val_loss: 0.5517 - val_accuracy: 0.7375\n",
      "Epoch 468/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5626 - accuracy: 0.7223 - val_loss: 0.5508 - val_accuracy: 0.7375\n",
      "Epoch 469/780\n",
      "684/684 [==============================] - 1s 979us/step - loss: 0.5626 - accuracy: 0.7221 - val_loss: 0.5511 - val_accuracy: 0.7378\n",
      "Epoch 470/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.7222 - val_loss: 0.5522 - val_accuracy: 0.7378\n",
      "Epoch 471/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7221 - val_loss: 0.5515 - val_accuracy: 0.7370\n",
      "Epoch 472/780\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5632 - accuracy: 0.7219 - val_loss: 0.5511 - val_accuracy: 0.7380\n",
      "Epoch 473/780\n",
      "663/684 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7221"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,validation_split=0.15, epochs=780)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 0s - loss: 0.5631 - accuracy: 0.7374 - 247ms/epoch - 922us/step\n",
      "Loss: 0.5630524158477783, Accuracy: 0.7373760938644409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Best result accuracy only'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "#\n",
    "\n",
    "# #using 2 hidden layers with Relu, PReLU as the Output layer, hidden layers values 12,24,48 \n",
    "# 268/268 - 1s - loss: 0.6315 - accuracy: 0.7368 - 692ms/epoch - 3ms/step\n",
    "# Loss: 0.6315085291862488, Accuracy: 0.7367929816246033\n",
    "\n",
    "'''Best results loos/accuracy '''\n",
    "#using 2 hidden layers with Relu, sigmoid as outter layer, hidden layers values 12,24,48   \n",
    "# 268/268 - 0s - loss: 0.5572 - accuracy: 0.7368 - 263ms/epoch - 981us/step\n",
    "# Loss: 0.5571669936180115, Accuracy: 0.7367929816246033\n",
    "\n",
    "#using 2 hidden layers with Relu a third hidden layer with LeakyReLU, sigmoid as Output layer, hidden layers values 12,24,48 \n",
    "# 268/268 - 0s - loss: 0.5608 - accuracy: 0.7388 - 249ms/epoch - 928us/step\n",
    "# Loss: 0.5607810020446777, Accuracy: 0.7387754917144775\n",
    "\n",
    "#using a third hidden layer with LeakyReLU, softmax as Output layer, hidden layers values 12,24,48 \n",
    "# 268/268 - 0s - loss: 0.5609 - accuracy: 0.5349 - 236ms/epoch - 882us/step\n",
    "# Loss: 0.5608648657798767, Accuracy: 0.5349271297454834\n",
    "\n",
    "'''Best result accuracy only'''\n",
    "#using 2 hidden layers with Relu a third hidden layer with LeakyReLU, sigmoid as outter layer, hidden layers values 24,48,96   \n",
    "# 268/268 - 0s - loss: 0.6040 - accuracy: 0.7392 - 262ms/epoch - 977us/step\n",
    "# Loss: 0.6040363311767578, Accuracy: 0.7392419576644897\n",
    "\n",
    "#using 3 hidden layers with Relu, sigmoid as outter layer, hidden layers values 24,48,96 \n",
    "# 268/268 - 0s - loss: 0.6273 - accuracy: 0.7366 - 417ms/epoch - 2ms/step\n",
    "# Loss: 0.6273306608200073, Accuracy: 0.7365597486495972\n",
    "\n",
    "#using 3 hidden layers with Relu, sigmoid as outter layer, hidden layers values 2, 4, 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gus Bustillos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export our model to HDF5 file\n",
    "# .h5 files are considered legacy and the library suggested to save it as .keras, saved both formats. \n",
    "nn.save(\"./modelHDF5.h5\")\n",
    "nn.save(\"./modelKERAS.keras\")\n",
    "plot_df = pd.DataFrame(fit_model.history, index = range(1, len(fit_model.history['loss'])+1))\n",
    "plot_df.plot(y = 'accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
