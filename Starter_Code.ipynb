{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN                          NAME APPLICATION_TYPE  AFFILIATION  \\\n",
       "0  10520599  BLUE KNIGHTS MOTORCYCLE CLUB              T10  Independent   \n",
       "\n",
       "  CLASSIFICATION    USE_CASE ORGANIZATION  STATUS INCOME_AMT  \\\n",
       "0          C1000  ProductDev  Association       1          0   \n",
       "\n",
       "  SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0                      N     5000              1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "\n",
    "if not os.path.isfile('./charity_data.csv'):\n",
    "    application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
    "    application_df.to_csv('./charity_data.csv', encoding='utf-8')\n",
    "else:\n",
    "    application_df = pd.read_csv('./charity_data.csv')\n",
    "    application_df = application_df.loc[:, ~application_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "application_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  APPLICATION_TYPE  AFFILIATION CLASSIFICATION    USE_CASE ORGANIZATION  \\\n",
       "0              T10  Independent          C1000  ProductDev  Association   \n",
       "\n",
       "   STATUS INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0       1          0                      N     5000              1  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df.drop(['EIN', 'NAME'],  axis=1, inplace=True)\n",
    "# application_df = application_df.loc[:, ~application_df.columns.str.contains('^Unnamed')]\n",
    "application_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_TYPE            17\n",
       "AFFILIATION                  6\n",
       "CLASSIFICATION              71\n",
       "USE_CASE                     5\n",
       "ORGANIZATION                 4\n",
       "STATUS                       2\n",
       "INCOME_AMT                   9\n",
       "SPECIAL_CONSIDERATIONS       2\n",
       "ASK_AMT                   8747\n",
       "IS_SUCCESSFUL                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "application_df.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3     27037\n",
       "T4      1542\n",
       "T6      1216\n",
       "T5      1173\n",
       "T19     1065\n",
       "T8       737\n",
       "T7       725\n",
       "T10      528\n",
       "T9       156\n",
       "T13       66\n",
       "T12       27\n",
       "T2        16\n",
       "T25        3\n",
       "T14        3\n",
       "T29        2\n",
       "T15        2\n",
       "T17        1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "application_type_counts = application_df['APPLICATION_TYPE'].value_counts()\n",
    "application_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3       27037\n",
       "Other     2266\n",
       "T4        1542\n",
       "T6        1216\n",
       "T5        1173\n",
       "T19       1065\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "application_types_to_replace = list(application_type_counts[application_type_counts < 750].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "application_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "         ...  \n",
       "C4120        1\n",
       "C8210        1\n",
       "C2561        1\n",
       "C4500        1\n",
       "C2150        1\n",
       "Name: CLASSIFICATION, Length: 71, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "classification_val_counts = application_df['CLASSIFICATION'].value_counts()\n",
    "classification_val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "C7000      777\n",
       "C1700      287\n",
       "C4000      194\n",
       "C5000      116\n",
       "C1270      114\n",
       "C2700      104\n",
       "C2800       95\n",
       "C7100       75\n",
       "C1300       58\n",
       "C1280       50\n",
       "C1230       36\n",
       "C1400       34\n",
       "C7200       32\n",
       "C2300       32\n",
       "C1240       30\n",
       "C8000       20\n",
       "C7120       18\n",
       "C1500       16\n",
       "C1800       15\n",
       "C6000       15\n",
       "C1250       14\n",
       "C8200       11\n",
       "C1238       10\n",
       "C1278       10\n",
       "C1235        9\n",
       "C1237        9\n",
       "C7210        7\n",
       "C2400        6\n",
       "C1720        6\n",
       "C4100        6\n",
       "C1257        5\n",
       "C1600        5\n",
       "C1260        3\n",
       "C2710        3\n",
       "C0           3\n",
       "C3200        2\n",
       "C1234        2\n",
       "C1246        2\n",
       "C1267        2\n",
       "C1256        2\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
    "greater_than_one = classification_val_counts[classification_val_counts>1]\n",
    "greater_than_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "Other     1484\n",
       "C7000      777\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "classifications_to_replace = list (classification_val_counts[classification_val_counts<750].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "      <th>APPLICATION_TYPE_Other</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T3</th>\n",
       "      <th>APPLICATION_TYPE_T4</th>\n",
       "      <th>APPLICATION_TYPE_T5</th>\n",
       "      <th>APPLICATION_TYPE_T6</th>\n",
       "      <th>AFFILIATION_CompanySponsored</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATUS  ASK_AMT  IS_SUCCESSFUL  APPLICATION_TYPE_Other  \\\n",
       "0       1     5000              1                     1.0   \n",
       "1       1   108590              1                     0.0   \n",
       "2       1     5000              0                     0.0   \n",
       "3       1     6692              1                     0.0   \n",
       "4       1   142590              1                     0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T3  APPLICATION_TYPE_T4  \\\n",
       "0                   0.0                  0.0                  0.0   \n",
       "1                   0.0                  1.0                  0.0   \n",
       "2                   0.0                  0.0                  0.0   \n",
       "3                   0.0                  1.0                  0.0   \n",
       "4                   0.0                  1.0                  0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T5  APPLICATION_TYPE_T6  AFFILIATION_CompanySponsored  \\\n",
       "0                  0.0                  0.0                           0.0   \n",
       "1                  0.0                  0.0                           0.0   \n",
       "2                  1.0                  0.0                           1.0   \n",
       "3                  0.0                  0.0                           1.0   \n",
       "4                  0.0                  0.0                           0.0   \n",
       "\n",
       "   ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n",
       "0  ...                0.0                     0.0                       0.0   \n",
       "1  ...                1.0                     0.0                       0.0   \n",
       "2  ...                0.0                     0.0                       0.0   \n",
       "3  ...                0.0                     1.0                       0.0   \n",
       "4  ...                0.0                     0.0                       1.0   \n",
       "\n",
       "   INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n",
       "0                 0.0               0.0                     0.0   \n",
       "1                 0.0               0.0                     0.0   \n",
       "2                 0.0               0.0                     0.0   \n",
       "3                 0.0               0.0                     0.0   \n",
       "4                 0.0               0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n",
       "0              0.0                0.0                       1.0   \n",
       "1              0.0                0.0                       1.0   \n",
       "2              0.0                0.0                       1.0   \n",
       "3              0.0                0.0                       1.0   \n",
       "4              0.0                0.0                       1.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "application_df = pd.get_dummies(application_df,dtype=float)\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gus Bustillos\\AppData\\Local\\Temp\\ipykernel_19612\\1048604523.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = application_df.drop(['IS_SUCCESSFUL'],1).values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.000000e+00, 2.631396e+06, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       ...,\n",
       "       [1.000000e+00, 1.443006e+06, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = application_df['IS_SUCCESSFUL'].values\n",
    "X = application_df.drop(['IS_SUCCESSFUL'],1).values\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 12)                504       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 24)                312       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 841 (3.29 KB)\n",
      "Trainable params: 841 (3.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "features = len( X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 12\n",
    "hidden_nodes_layer2 = 24\n",
    "hidden_nodes_layer3 = 36\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=features, activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, input_dim=features, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "684/684 [==============================] - 5s 5ms/step - loss: 0.6005 - accuracy: 0.6926 - val_loss: 0.5565 - val_accuracy: 0.7354\n",
      "Epoch 2/800\n",
      "684/684 [==============================] - 3s 5ms/step - loss: 0.5693 - accuracy: 0.7212 - val_loss: 0.5508 - val_accuracy: 0.7370\n",
      "Epoch 3/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5650 - accuracy: 0.7214 - val_loss: 0.5518 - val_accuracy: 0.7385\n",
      "Epoch 4/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5631 - accuracy: 0.7233 - val_loss: 0.5472 - val_accuracy: 0.7393\n",
      "Epoch 5/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7241 - val_loss: 0.5489 - val_accuracy: 0.7346\n",
      "Epoch 6/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5602 - accuracy: 0.7252 - val_loss: 0.5472 - val_accuracy: 0.7383\n",
      "Epoch 7/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5593 - accuracy: 0.7249 - val_loss: 0.5484 - val_accuracy: 0.7352\n",
      "Epoch 8/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5586 - accuracy: 0.7245 - val_loss: 0.5462 - val_accuracy: 0.7383\n",
      "Epoch 9/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5579 - accuracy: 0.7275 - val_loss: 0.5462 - val_accuracy: 0.7391\n",
      "Epoch 10/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5572 - accuracy: 0.7276 - val_loss: 0.5451 - val_accuracy: 0.7375\n",
      "Epoch 11/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5564 - accuracy: 0.7277 - val_loss: 0.5470 - val_accuracy: 0.7370\n",
      "Epoch 12/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5561 - accuracy: 0.7271 - val_loss: 0.5460 - val_accuracy: 0.7372\n",
      "Epoch 13/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5553 - accuracy: 0.7261 - val_loss: 0.5469 - val_accuracy: 0.7378\n",
      "Epoch 14/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5555 - accuracy: 0.7272 - val_loss: 0.5454 - val_accuracy: 0.7378\n",
      "Epoch 15/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5547 - accuracy: 0.7270 - val_loss: 0.5445 - val_accuracy: 0.7383\n",
      "Epoch 16/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5548 - accuracy: 0.7269 - val_loss: 0.5463 - val_accuracy: 0.7362\n",
      "Epoch 17/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5538 - accuracy: 0.7287 - val_loss: 0.5451 - val_accuracy: 0.7378\n",
      "Epoch 18/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5539 - accuracy: 0.7281 - val_loss: 0.5496 - val_accuracy: 0.7367\n",
      "Epoch 19/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5534 - accuracy: 0.7275 - val_loss: 0.5475 - val_accuracy: 0.7370\n",
      "Epoch 20/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5535 - accuracy: 0.7288 - val_loss: 0.5442 - val_accuracy: 0.7396\n",
      "Epoch 21/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5530 - accuracy: 0.7274 - val_loss: 0.5454 - val_accuracy: 0.7383\n",
      "Epoch 22/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5527 - accuracy: 0.7285 - val_loss: 0.5475 - val_accuracy: 0.7383\n",
      "Epoch 23/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5528 - accuracy: 0.7286 - val_loss: 0.5460 - val_accuracy: 0.7370\n",
      "Epoch 24/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5527 - accuracy: 0.7282 - val_loss: 0.5457 - val_accuracy: 0.7375\n",
      "Epoch 25/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5524 - accuracy: 0.7289 - val_loss: 0.5477 - val_accuracy: 0.7378\n",
      "Epoch 26/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5524 - accuracy: 0.7293 - val_loss: 0.5467 - val_accuracy: 0.7391\n",
      "Epoch 27/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5519 - accuracy: 0.7300 - val_loss: 0.5458 - val_accuracy: 0.7378\n",
      "Epoch 28/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5519 - accuracy: 0.7298 - val_loss: 0.5471 - val_accuracy: 0.7370\n",
      "Epoch 29/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5515 - accuracy: 0.7298 - val_loss: 0.5484 - val_accuracy: 0.7372\n",
      "Epoch 30/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5519 - accuracy: 0.7293 - val_loss: 0.5460 - val_accuracy: 0.7388\n",
      "Epoch 31/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5519 - accuracy: 0.7286 - val_loss: 0.5457 - val_accuracy: 0.7398\n",
      "Epoch 32/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5510 - accuracy: 0.7310 - val_loss: 0.5488 - val_accuracy: 0.7372\n",
      "Epoch 33/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5517 - accuracy: 0.7298 - val_loss: 0.5467 - val_accuracy: 0.7372\n",
      "Epoch 34/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5510 - accuracy: 0.7298 - val_loss: 0.5508 - val_accuracy: 0.7370\n",
      "Epoch 35/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5514 - accuracy: 0.7292 - val_loss: 0.5490 - val_accuracy: 0.7359\n",
      "Epoch 36/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5513 - accuracy: 0.7295 - val_loss: 0.5488 - val_accuracy: 0.7359\n",
      "Epoch 37/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5512 - accuracy: 0.7302 - val_loss: 0.5480 - val_accuracy: 0.7385\n",
      "Epoch 38/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5510 - accuracy: 0.7310 - val_loss: 0.5474 - val_accuracy: 0.7385\n",
      "Epoch 39/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5507 - accuracy: 0.7299 - val_loss: 0.5458 - val_accuracy: 0.7372\n",
      "Epoch 40/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5511 - accuracy: 0.7300 - val_loss: 0.5457 - val_accuracy: 0.7385\n",
      "Epoch 41/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5509 - accuracy: 0.7305 - val_loss: 0.5467 - val_accuracy: 0.7378\n",
      "Epoch 42/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5506 - accuracy: 0.7306 - val_loss: 0.5470 - val_accuracy: 0.7365\n",
      "Epoch 43/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5505 - accuracy: 0.7302 - val_loss: 0.5478 - val_accuracy: 0.7370\n",
      "Epoch 44/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5509 - accuracy: 0.7288 - val_loss: 0.5494 - val_accuracy: 0.7365\n",
      "Epoch 45/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5502 - accuracy: 0.7306 - val_loss: 0.5473 - val_accuracy: 0.7370\n",
      "Epoch 46/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5505 - accuracy: 0.7308 - val_loss: 0.5451 - val_accuracy: 0.7398\n",
      "Epoch 47/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5502 - accuracy: 0.7306 - val_loss: 0.5490 - val_accuracy: 0.7398\n",
      "Epoch 48/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7312 - val_loss: 0.5483 - val_accuracy: 0.7383\n",
      "Epoch 49/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5502 - accuracy: 0.7303 - val_loss: 0.5496 - val_accuracy: 0.7341\n",
      "Epoch 50/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5503 - accuracy: 0.7298 - val_loss: 0.5474 - val_accuracy: 0.7370\n",
      "Epoch 51/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5499 - accuracy: 0.7298 - val_loss: 0.5491 - val_accuracy: 0.7367\n",
      "Epoch 52/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5498 - accuracy: 0.7309 - val_loss: 0.5477 - val_accuracy: 0.7375\n",
      "Epoch 53/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5500 - accuracy: 0.7297 - val_loss: 0.5477 - val_accuracy: 0.7383\n",
      "Epoch 54/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5499 - accuracy: 0.7311 - val_loss: 0.5484 - val_accuracy: 0.7385\n",
      "Epoch 55/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5502 - accuracy: 0.7305 - val_loss: 0.5474 - val_accuracy: 0.7375\n",
      "Epoch 56/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5499 - accuracy: 0.7313 - val_loss: 0.5460 - val_accuracy: 0.7367\n",
      "Epoch 57/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5493 - accuracy: 0.7303 - val_loss: 0.5478 - val_accuracy: 0.7375\n",
      "Epoch 58/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5495 - accuracy: 0.7315 - val_loss: 0.5466 - val_accuracy: 0.7378\n",
      "Epoch 59/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5500 - accuracy: 0.7316 - val_loss: 0.5464 - val_accuracy: 0.7375\n",
      "Epoch 60/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5496 - accuracy: 0.7312 - val_loss: 0.5480 - val_accuracy: 0.7372\n",
      "Epoch 61/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5501 - accuracy: 0.7315 - val_loss: 0.5495 - val_accuracy: 0.7370\n",
      "Epoch 62/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5495 - accuracy: 0.7313 - val_loss: 0.5476 - val_accuracy: 0.7388\n",
      "Epoch 63/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5492 - accuracy: 0.7322 - val_loss: 0.5468 - val_accuracy: 0.7372\n",
      "Epoch 64/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5493 - accuracy: 0.7314 - val_loss: 0.5469 - val_accuracy: 0.7380\n",
      "Epoch 65/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5492 - accuracy: 0.7326 - val_loss: 0.5497 - val_accuracy: 0.7372\n",
      "Epoch 66/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5493 - accuracy: 0.7324 - val_loss: 0.5465 - val_accuracy: 0.7380\n",
      "Epoch 67/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5493 - accuracy: 0.7309 - val_loss: 0.5480 - val_accuracy: 0.7378\n",
      "Epoch 68/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5492 - accuracy: 0.7318 - val_loss: 0.5476 - val_accuracy: 0.7365\n",
      "Epoch 69/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5489 - accuracy: 0.7325 - val_loss: 0.5478 - val_accuracy: 0.7372\n",
      "Epoch 70/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5492 - accuracy: 0.7317 - val_loss: 0.5477 - val_accuracy: 0.7365\n",
      "Epoch 71/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5491 - accuracy: 0.7314 - val_loss: 0.5476 - val_accuracy: 0.7372\n",
      "Epoch 72/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5486 - accuracy: 0.7320 - val_loss: 0.5496 - val_accuracy: 0.7396\n",
      "Epoch 73/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5492 - accuracy: 0.7317 - val_loss: 0.5494 - val_accuracy: 0.7365\n",
      "Epoch 74/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5487 - accuracy: 0.7316 - val_loss: 0.5484 - val_accuracy: 0.7367\n",
      "Epoch 75/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5485 - accuracy: 0.7331 - val_loss: 0.5477 - val_accuracy: 0.7372\n",
      "Epoch 76/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5489 - accuracy: 0.7314 - val_loss: 0.5471 - val_accuracy: 0.7375\n",
      "Epoch 77/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5484 - accuracy: 0.7327 - val_loss: 0.5476 - val_accuracy: 0.7380\n",
      "Epoch 78/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5486 - accuracy: 0.7309 - val_loss: 0.5477 - val_accuracy: 0.7365\n",
      "Epoch 79/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5485 - accuracy: 0.7314 - val_loss: 0.5506 - val_accuracy: 0.7380\n",
      "Epoch 80/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5485 - accuracy: 0.7325 - val_loss: 0.5495 - val_accuracy: 0.7375\n",
      "Epoch 81/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5485 - accuracy: 0.7319 - val_loss: 0.5476 - val_accuracy: 0.7375\n",
      "Epoch 82/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5486 - accuracy: 0.7323 - val_loss: 0.5478 - val_accuracy: 0.7388\n",
      "Epoch 83/800\n",
      "684/684 [==============================] - 2s 4ms/step - loss: 0.5480 - accuracy: 0.7314 - val_loss: 0.5509 - val_accuracy: 0.7362\n",
      "Epoch 84/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5485 - accuracy: 0.7329 - val_loss: 0.5496 - val_accuracy: 0.7359\n",
      "Epoch 85/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5482 - accuracy: 0.7310 - val_loss: 0.5520 - val_accuracy: 0.7352\n",
      "Epoch 86/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5481 - accuracy: 0.7322 - val_loss: 0.5521 - val_accuracy: 0.7336\n",
      "Epoch 87/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5484 - accuracy: 0.7325 - val_loss: 0.5527 - val_accuracy: 0.7367\n",
      "Epoch 88/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5479 - accuracy: 0.7328 - val_loss: 0.5500 - val_accuracy: 0.7372\n",
      "Epoch 89/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5484 - accuracy: 0.7323 - val_loss: 0.5476 - val_accuracy: 0.7370\n",
      "Epoch 90/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5480 - accuracy: 0.7324 - val_loss: 0.5516 - val_accuracy: 0.7346\n",
      "Epoch 91/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5481 - accuracy: 0.7321 - val_loss: 0.5509 - val_accuracy: 0.7370\n",
      "Epoch 92/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5483 - accuracy: 0.7326 - val_loss: 0.5508 - val_accuracy: 0.7357\n",
      "Epoch 93/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5481 - accuracy: 0.7324 - val_loss: 0.5525 - val_accuracy: 0.7383\n",
      "Epoch 94/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5481 - accuracy: 0.7334 - val_loss: 0.5495 - val_accuracy: 0.7383\n",
      "Epoch 95/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5475 - accuracy: 0.7319 - val_loss: 0.5483 - val_accuracy: 0.7370\n",
      "Epoch 96/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5481 - accuracy: 0.7322 - val_loss: 0.5515 - val_accuracy: 0.7359\n",
      "Epoch 97/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5477 - accuracy: 0.7324 - val_loss: 0.5495 - val_accuracy: 0.7370\n",
      "Epoch 98/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5476 - accuracy: 0.7323 - val_loss: 0.5513 - val_accuracy: 0.7365\n",
      "Epoch 99/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5480 - accuracy: 0.7325 - val_loss: 0.5496 - val_accuracy: 0.7365\n",
      "Epoch 100/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5478 - accuracy: 0.7340 - val_loss: 0.5512 - val_accuracy: 0.7370\n",
      "Epoch 101/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5473 - accuracy: 0.7331 - val_loss: 0.5505 - val_accuracy: 0.7383\n",
      "Epoch 102/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5473 - accuracy: 0.7330 - val_loss: 0.5539 - val_accuracy: 0.7378\n",
      "Epoch 103/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5477 - accuracy: 0.7322 - val_loss: 0.5510 - val_accuracy: 0.7367\n",
      "Epoch 104/800\n",
      "684/684 [==============================] - 2s 4ms/step - loss: 0.5477 - accuracy: 0.7323 - val_loss: 0.5510 - val_accuracy: 0.7367\n",
      "Epoch 105/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5478 - accuracy: 0.7337 - val_loss: 0.5513 - val_accuracy: 0.7375\n",
      "Epoch 106/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5474 - accuracy: 0.7332 - val_loss: 0.5509 - val_accuracy: 0.7380\n",
      "Epoch 107/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5479 - accuracy: 0.7325 - val_loss: 0.5503 - val_accuracy: 0.7372\n",
      "Epoch 108/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5474 - accuracy: 0.7323 - val_loss: 0.5503 - val_accuracy: 0.7372\n",
      "Epoch 109/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5469 - accuracy: 0.7324 - val_loss: 0.5520 - val_accuracy: 0.7370\n",
      "Epoch 110/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5474 - accuracy: 0.7320 - val_loss: 0.5499 - val_accuracy: 0.7370\n",
      "Epoch 111/800\n",
      "684/684 [==============================] - 4s 6ms/step - loss: 0.5474 - accuracy: 0.7327 - val_loss: 0.5518 - val_accuracy: 0.7365\n",
      "Epoch 112/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5471 - accuracy: 0.7332 - val_loss: 0.5528 - val_accuracy: 0.7372\n",
      "Epoch 113/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5473 - accuracy: 0.7328 - val_loss: 0.5498 - val_accuracy: 0.7365\n",
      "Epoch 114/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5475 - accuracy: 0.7324 - val_loss: 0.5507 - val_accuracy: 0.7378\n",
      "Epoch 115/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5471 - accuracy: 0.7329 - val_loss: 0.5520 - val_accuracy: 0.7357\n",
      "Epoch 116/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5475 - accuracy: 0.7327 - val_loss: 0.5525 - val_accuracy: 0.7372\n",
      "Epoch 117/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5468 - accuracy: 0.7324 - val_loss: 0.5519 - val_accuracy: 0.7370\n",
      "Epoch 118/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5474 - accuracy: 0.7321 - val_loss: 0.5509 - val_accuracy: 0.7370\n",
      "Epoch 119/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5464 - accuracy: 0.7329 - val_loss: 0.5538 - val_accuracy: 0.7359\n",
      "Epoch 120/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5475 - accuracy: 0.7324 - val_loss: 0.5517 - val_accuracy: 0.7372\n",
      "Epoch 121/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5469 - accuracy: 0.7334 - val_loss: 0.5527 - val_accuracy: 0.7367\n",
      "Epoch 122/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5470 - accuracy: 0.7328 - val_loss: 0.5525 - val_accuracy: 0.7357\n",
      "Epoch 123/800\n",
      "684/684 [==============================] - 2s 4ms/step - loss: 0.5466 - accuracy: 0.7325 - val_loss: 0.5530 - val_accuracy: 0.7370\n",
      "Epoch 124/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5471 - accuracy: 0.7339 - val_loss: 0.5541 - val_accuracy: 0.7341\n",
      "Epoch 125/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5469 - accuracy: 0.7328 - val_loss: 0.5548 - val_accuracy: 0.7357\n",
      "Epoch 126/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5472 - accuracy: 0.7328 - val_loss: 0.5552 - val_accuracy: 0.7357\n",
      "Epoch 127/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5467 - accuracy: 0.7335 - val_loss: 0.5565 - val_accuracy: 0.7336\n",
      "Epoch 128/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5469 - accuracy: 0.7325 - val_loss: 0.5509 - val_accuracy: 0.7365\n",
      "Epoch 129/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5466 - accuracy: 0.7336 - val_loss: 0.5520 - val_accuracy: 0.7378\n",
      "Epoch 130/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5465 - accuracy: 0.7334 - val_loss: 0.5523 - val_accuracy: 0.7367\n",
      "Epoch 131/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5466 - accuracy: 0.7323 - val_loss: 0.5556 - val_accuracy: 0.7310\n",
      "Epoch 132/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5466 - accuracy: 0.7326 - val_loss: 0.5544 - val_accuracy: 0.7339\n",
      "Epoch 133/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5465 - accuracy: 0.7332 - val_loss: 0.5535 - val_accuracy: 0.7362\n",
      "Epoch 134/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5467 - accuracy: 0.7330 - val_loss: 0.5536 - val_accuracy: 0.7341\n",
      "Epoch 135/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5468 - accuracy: 0.7327 - val_loss: 0.5519 - val_accuracy: 0.7370\n",
      "Epoch 136/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5462 - accuracy: 0.7332 - val_loss: 0.5530 - val_accuracy: 0.7339\n",
      "Epoch 137/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5469 - accuracy: 0.7324 - val_loss: 0.5541 - val_accuracy: 0.7352\n",
      "Epoch 138/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5465 - accuracy: 0.7340 - val_loss: 0.5549 - val_accuracy: 0.7336\n",
      "Epoch 139/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5468 - accuracy: 0.7330 - val_loss: 0.5527 - val_accuracy: 0.7357\n",
      "Epoch 140/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5465 - accuracy: 0.7333 - val_loss: 0.5528 - val_accuracy: 0.7370\n",
      "Epoch 141/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5466 - accuracy: 0.7325 - val_loss: 0.5519 - val_accuracy: 0.7359\n",
      "Epoch 142/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5467 - accuracy: 0.7325 - val_loss: 0.5531 - val_accuracy: 0.7354\n",
      "Epoch 143/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5465 - accuracy: 0.7331 - val_loss: 0.5508 - val_accuracy: 0.7362\n",
      "Epoch 144/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5463 - accuracy: 0.7324 - val_loss: 0.5510 - val_accuracy: 0.7357\n",
      "Epoch 145/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5464 - accuracy: 0.7340 - val_loss: 0.5546 - val_accuracy: 0.7357\n",
      "Epoch 146/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5462 - accuracy: 0.7335 - val_loss: 0.5512 - val_accuracy: 0.7362\n",
      "Epoch 147/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5463 - accuracy: 0.7337 - val_loss: 0.5569 - val_accuracy: 0.7367\n",
      "Epoch 148/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5460 - accuracy: 0.7324 - val_loss: 0.5556 - val_accuracy: 0.7372\n",
      "Epoch 149/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5464 - accuracy: 0.7342 - val_loss: 0.5554 - val_accuracy: 0.7346\n",
      "Epoch 150/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5464 - accuracy: 0.7333 - val_loss: 0.5556 - val_accuracy: 0.7336\n",
      "Epoch 151/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5462 - accuracy: 0.7332 - val_loss: 0.5530 - val_accuracy: 0.7367\n",
      "Epoch 152/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5463 - accuracy: 0.7330 - val_loss: 0.5566 - val_accuracy: 0.7357\n",
      "Epoch 153/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5459 - accuracy: 0.7334 - val_loss: 0.5530 - val_accuracy: 0.7362\n",
      "Epoch 154/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5461 - accuracy: 0.7324 - val_loss: 0.5538 - val_accuracy: 0.7357\n",
      "Epoch 155/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5464 - accuracy: 0.7333 - val_loss: 0.5540 - val_accuracy: 0.7362\n",
      "Epoch 156/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5465 - accuracy: 0.7335 - val_loss: 0.5527 - val_accuracy: 0.7357\n",
      "Epoch 157/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5459 - accuracy: 0.7336 - val_loss: 0.5541 - val_accuracy: 0.7344\n",
      "Epoch 158/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5462 - accuracy: 0.7337 - val_loss: 0.5555 - val_accuracy: 0.7349\n",
      "Epoch 159/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5459 - accuracy: 0.7346 - val_loss: 0.5572 - val_accuracy: 0.7339\n",
      "Epoch 160/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5462 - accuracy: 0.7334 - val_loss: 0.5531 - val_accuracy: 0.7357\n",
      "Epoch 161/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5465 - accuracy: 0.7333 - val_loss: 0.5514 - val_accuracy: 0.7365\n",
      "Epoch 162/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5458 - accuracy: 0.7330 - val_loss: 0.5556 - val_accuracy: 0.7370\n",
      "Epoch 163/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5460 - accuracy: 0.7340 - val_loss: 0.5534 - val_accuracy: 0.7362\n",
      "Epoch 164/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5460 - accuracy: 0.7333 - val_loss: 0.5563 - val_accuracy: 0.7339\n",
      "Epoch 165/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5462 - accuracy: 0.7329 - val_loss: 0.5534 - val_accuracy: 0.7357\n",
      "Epoch 166/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5464 - accuracy: 0.7339 - val_loss: 0.5569 - val_accuracy: 0.7370\n",
      "Epoch 167/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5463 - accuracy: 0.7332 - val_loss: 0.5550 - val_accuracy: 0.7365\n",
      "Epoch 168/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7340 - val_loss: 0.5544 - val_accuracy: 0.7359\n",
      "Epoch 169/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5460 - accuracy: 0.7339 - val_loss: 0.5546 - val_accuracy: 0.7352\n",
      "Epoch 170/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7332 - val_loss: 0.5543 - val_accuracy: 0.7362\n",
      "Epoch 171/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5459 - accuracy: 0.7335 - val_loss: 0.5539 - val_accuracy: 0.7370\n",
      "Epoch 172/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5459 - accuracy: 0.7327 - val_loss: 0.5542 - val_accuracy: 0.7370\n",
      "Epoch 173/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5458 - accuracy: 0.7335 - val_loss: 0.5545 - val_accuracy: 0.7367\n",
      "Epoch 174/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5455 - accuracy: 0.7331 - val_loss: 0.5536 - val_accuracy: 0.7367\n",
      "Epoch 175/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5460 - accuracy: 0.7340 - val_loss: 0.5548 - val_accuracy: 0.7362\n",
      "Epoch 176/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5457 - accuracy: 0.7336 - val_loss: 0.5597 - val_accuracy: 0.7321\n",
      "Epoch 177/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5463 - accuracy: 0.7337 - val_loss: 0.5565 - val_accuracy: 0.7357\n",
      "Epoch 178/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5458 - accuracy: 0.7335 - val_loss: 0.5556 - val_accuracy: 0.7349\n",
      "Epoch 179/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5458 - accuracy: 0.7351 - val_loss: 0.5552 - val_accuracy: 0.7359\n",
      "Epoch 180/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7345 - val_loss: 0.5576 - val_accuracy: 0.7315\n",
      "Epoch 181/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7333 - val_loss: 0.5556 - val_accuracy: 0.7357\n",
      "Epoch 182/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5460 - accuracy: 0.7335 - val_loss: 0.5550 - val_accuracy: 0.7362\n",
      "Epoch 183/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5459 - accuracy: 0.7332 - val_loss: 0.5550 - val_accuracy: 0.7370\n",
      "Epoch 184/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7339 - val_loss: 0.5562 - val_accuracy: 0.7344\n",
      "Epoch 185/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7337 - val_loss: 0.5559 - val_accuracy: 0.7365\n",
      "Epoch 186/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5457 - accuracy: 0.7338 - val_loss: 0.5542 - val_accuracy: 0.7341\n",
      "Epoch 187/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5458 - accuracy: 0.7329 - val_loss: 0.5566 - val_accuracy: 0.7362\n",
      "Epoch 188/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7326 - val_loss: 0.5539 - val_accuracy: 0.7352\n",
      "Epoch 189/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5457 - accuracy: 0.7335 - val_loss: 0.5548 - val_accuracy: 0.7359\n",
      "Epoch 190/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5457 - accuracy: 0.7335 - val_loss: 0.5600 - val_accuracy: 0.7341\n",
      "Epoch 191/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7337 - val_loss: 0.5580 - val_accuracy: 0.7352\n",
      "Epoch 192/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7343 - val_loss: 0.5570 - val_accuracy: 0.7367\n",
      "Epoch 193/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5464 - accuracy: 0.7329 - val_loss: 0.5569 - val_accuracy: 0.7383\n",
      "Epoch 194/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7338 - val_loss: 0.5561 - val_accuracy: 0.7354\n",
      "Epoch 195/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.7340 - val_loss: 0.5572 - val_accuracy: 0.7349\n",
      "Epoch 196/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7334 - val_loss: 0.5534 - val_accuracy: 0.7354\n",
      "Epoch 197/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7333 - val_loss: 0.5548 - val_accuracy: 0.7354\n",
      "Epoch 198/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7335 - val_loss: 0.5547 - val_accuracy: 0.7357\n",
      "Epoch 199/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7340 - val_loss: 0.5566 - val_accuracy: 0.7359\n",
      "Epoch 200/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5454 - accuracy: 0.7329 - val_loss: 0.5555 - val_accuracy: 0.7362\n",
      "Epoch 201/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5454 - accuracy: 0.7329 - val_loss: 0.5572 - val_accuracy: 0.7359\n",
      "Epoch 202/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7352 - val_loss: 0.5557 - val_accuracy: 0.7367\n",
      "Epoch 203/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7340 - val_loss: 0.5566 - val_accuracy: 0.7357\n",
      "Epoch 204/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5456 - accuracy: 0.7330 - val_loss: 0.5563 - val_accuracy: 0.7359\n",
      "Epoch 205/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7340 - val_loss: 0.5575 - val_accuracy: 0.7336\n",
      "Epoch 206/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7330 - val_loss: 0.5570 - val_accuracy: 0.7357\n",
      "Epoch 207/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7340 - val_loss: 0.5541 - val_accuracy: 0.7354\n",
      "Epoch 208/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7340 - val_loss: 0.5550 - val_accuracy: 0.7334\n",
      "Epoch 209/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7340 - val_loss: 0.5574 - val_accuracy: 0.7352\n",
      "Epoch 210/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5452 - accuracy: 0.7333 - val_loss: 0.5561 - val_accuracy: 0.7341\n",
      "Epoch 211/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.7330 - val_loss: 0.5557 - val_accuracy: 0.7362\n",
      "Epoch 212/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5457 - accuracy: 0.7325 - val_loss: 0.5534 - val_accuracy: 0.7357\n",
      "Epoch 213/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5454 - accuracy: 0.7329 - val_loss: 0.5586 - val_accuracy: 0.7362\n",
      "Epoch 214/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5459 - accuracy: 0.7336 - val_loss: 0.5541 - val_accuracy: 0.7362\n",
      "Epoch 215/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7342 - val_loss: 0.5550 - val_accuracy: 0.7367\n",
      "Epoch 216/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5454 - accuracy: 0.7343 - val_loss: 0.5548 - val_accuracy: 0.7346\n",
      "Epoch 217/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7331 - val_loss: 0.5552 - val_accuracy: 0.7375\n",
      "Epoch 218/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7342 - val_loss: 0.5553 - val_accuracy: 0.7365\n",
      "Epoch 219/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5454 - accuracy: 0.7334 - val_loss: 0.5562 - val_accuracy: 0.7372\n",
      "Epoch 220/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7331 - val_loss: 0.5538 - val_accuracy: 0.7375\n",
      "Epoch 221/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.7335 - val_loss: 0.5541 - val_accuracy: 0.7362\n",
      "Epoch 222/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7328 - val_loss: 0.5606 - val_accuracy: 0.7341\n",
      "Epoch 223/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7343 - val_loss: 0.5587 - val_accuracy: 0.7365\n",
      "Epoch 224/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7331 - val_loss: 0.5558 - val_accuracy: 0.7354\n",
      "Epoch 225/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7334 - val_loss: 0.5567 - val_accuracy: 0.7357\n",
      "Epoch 226/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.7336 - val_loss: 0.5547 - val_accuracy: 0.7357\n",
      "Epoch 227/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7332 - val_loss: 0.5555 - val_accuracy: 0.7359\n",
      "Epoch 228/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7333 - val_loss: 0.5570 - val_accuracy: 0.7341\n",
      "Epoch 229/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7341 - val_loss: 0.5556 - val_accuracy: 0.7375\n",
      "Epoch 230/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7343 - val_loss: 0.5562 - val_accuracy: 0.7341\n",
      "Epoch 231/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7330 - val_loss: 0.5559 - val_accuracy: 0.7365\n",
      "Epoch 232/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7344 - val_loss: 0.5549 - val_accuracy: 0.7372\n",
      "Epoch 233/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.7326 - val_loss: 0.5562 - val_accuracy: 0.7365\n",
      "Epoch 234/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7341 - val_loss: 0.5549 - val_accuracy: 0.7362\n",
      "Epoch 235/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.7334 - val_loss: 0.5603 - val_accuracy: 0.7354\n",
      "Epoch 236/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7330 - val_loss: 0.5550 - val_accuracy: 0.7375\n",
      "Epoch 237/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7343 - val_loss: 0.5556 - val_accuracy: 0.7334\n",
      "Epoch 238/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5446 - accuracy: 0.7350 - val_loss: 0.5547 - val_accuracy: 0.7365\n",
      "Epoch 239/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.7343 - val_loss: 0.5537 - val_accuracy: 0.7367\n",
      "Epoch 240/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7337 - val_loss: 0.5552 - val_accuracy: 0.7357\n",
      "Epoch 241/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7340 - val_loss: 0.5570 - val_accuracy: 0.7370\n",
      "Epoch 242/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7351 - val_loss: 0.5543 - val_accuracy: 0.7378\n",
      "Epoch 243/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7327 - val_loss: 0.5560 - val_accuracy: 0.7357\n",
      "Epoch 244/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7335 - val_loss: 0.5541 - val_accuracy: 0.7365\n",
      "Epoch 245/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7335 - val_loss: 0.5563 - val_accuracy: 0.7367\n",
      "Epoch 246/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7346 - val_loss: 0.5532 - val_accuracy: 0.7362\n",
      "Epoch 247/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7344 - val_loss: 0.5586 - val_accuracy: 0.7336\n",
      "Epoch 248/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7343 - val_loss: 0.5562 - val_accuracy: 0.7362\n",
      "Epoch 249/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7347 - val_loss: 0.5555 - val_accuracy: 0.7344\n",
      "Epoch 250/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7341 - val_loss: 0.5557 - val_accuracy: 0.7352\n",
      "Epoch 251/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7331 - val_loss: 0.5583 - val_accuracy: 0.7370\n",
      "Epoch 252/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7352 - val_loss: 0.5538 - val_accuracy: 0.7362\n",
      "Epoch 253/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7346 - val_loss: 0.5581 - val_accuracy: 0.7339\n",
      "Epoch 254/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7333 - val_loss: 0.5570 - val_accuracy: 0.7321\n",
      "Epoch 255/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5446 - accuracy: 0.7341 - val_loss: 0.5559 - val_accuracy: 0.7352\n",
      "Epoch 256/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5449 - accuracy: 0.7337 - val_loss: 0.5550 - val_accuracy: 0.7365\n",
      "Epoch 257/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7344 - val_loss: 0.5555 - val_accuracy: 0.7341\n",
      "Epoch 258/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5446 - accuracy: 0.7335 - val_loss: 0.5596 - val_accuracy: 0.7341\n",
      "Epoch 259/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7345 - val_loss: 0.5562 - val_accuracy: 0.7352\n",
      "Epoch 260/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5446 - accuracy: 0.7346 - val_loss: 0.5570 - val_accuracy: 0.7362\n",
      "Epoch 261/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7339 - val_loss: 0.5551 - val_accuracy: 0.7359\n",
      "Epoch 262/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5447 - accuracy: 0.7349 - val_loss: 0.5545 - val_accuracy: 0.7352\n",
      "Epoch 263/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7339 - val_loss: 0.5534 - val_accuracy: 0.7354\n",
      "Epoch 264/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7340 - val_loss: 0.5557 - val_accuracy: 0.7367\n",
      "Epoch 265/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7334 - val_loss: 0.5565 - val_accuracy: 0.7359\n",
      "Epoch 266/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7349 - val_loss: 0.5535 - val_accuracy: 0.7367\n",
      "Epoch 267/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7338 - val_loss: 0.5549 - val_accuracy: 0.7359\n",
      "Epoch 268/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7335 - val_loss: 0.5559 - val_accuracy: 0.7336\n",
      "Epoch 269/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7332 - val_loss: 0.5541 - val_accuracy: 0.7370\n",
      "Epoch 270/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7337 - val_loss: 0.5564 - val_accuracy: 0.7352\n",
      "Epoch 271/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7343 - val_loss: 0.5538 - val_accuracy: 0.7362\n",
      "Epoch 272/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7345 - val_loss: 0.5557 - val_accuracy: 0.7349\n",
      "Epoch 273/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7341 - val_loss: 0.5544 - val_accuracy: 0.7375\n",
      "Epoch 274/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7337 - val_loss: 0.5540 - val_accuracy: 0.7362\n",
      "Epoch 275/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7346 - val_loss: 0.5563 - val_accuracy: 0.7362\n",
      "Epoch 276/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7335 - val_loss: 0.5557 - val_accuracy: 0.7344\n",
      "Epoch 277/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5445 - accuracy: 0.7335 - val_loss: 0.5560 - val_accuracy: 0.7359\n",
      "Epoch 278/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7345 - val_loss: 0.5584 - val_accuracy: 0.7315\n",
      "Epoch 279/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7339 - val_loss: 0.5552 - val_accuracy: 0.7359\n",
      "Epoch 280/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7348 - val_loss: 0.5586 - val_accuracy: 0.7362\n",
      "Epoch 281/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7333 - val_loss: 0.5559 - val_accuracy: 0.7354\n",
      "Epoch 282/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7337 - val_loss: 0.5561 - val_accuracy: 0.7365\n",
      "Epoch 283/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5445 - accuracy: 0.7338 - val_loss: 0.5550 - val_accuracy: 0.7365\n",
      "Epoch 284/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7340 - val_loss: 0.5564 - val_accuracy: 0.7341\n",
      "Epoch 285/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7336 - val_loss: 0.5538 - val_accuracy: 0.7367\n",
      "Epoch 286/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7344 - val_loss: 0.5557 - val_accuracy: 0.7362\n",
      "Epoch 287/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7346 - val_loss: 0.5585 - val_accuracy: 0.7359\n",
      "Epoch 288/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7338 - val_loss: 0.5562 - val_accuracy: 0.7365\n",
      "Epoch 289/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7341 - val_loss: 0.5572 - val_accuracy: 0.7318\n",
      "Epoch 290/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7337 - val_loss: 0.5573 - val_accuracy: 0.7357\n",
      "Epoch 291/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7337 - val_loss: 0.5561 - val_accuracy: 0.7352\n",
      "Epoch 292/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7344 - val_loss: 0.5585 - val_accuracy: 0.7352\n",
      "Epoch 293/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7343 - val_loss: 0.5555 - val_accuracy: 0.7349\n",
      "Epoch 294/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7339 - val_loss: 0.5556 - val_accuracy: 0.7349\n",
      "Epoch 295/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7347 - val_loss: 0.5546 - val_accuracy: 0.7352\n",
      "Epoch 296/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7340 - val_loss: 0.5584 - val_accuracy: 0.7354\n",
      "Epoch 297/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7336 - val_loss: 0.5582 - val_accuracy: 0.7354\n",
      "Epoch 298/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7352 - val_loss: 0.5553 - val_accuracy: 0.7354\n",
      "Epoch 299/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5445 - accuracy: 0.7338 - val_loss: 0.5563 - val_accuracy: 0.7346\n",
      "Epoch 300/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5440 - accuracy: 0.7332 - val_loss: 0.5554 - val_accuracy: 0.7357\n",
      "Epoch 301/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5446 - accuracy: 0.7345 - val_loss: 0.5559 - val_accuracy: 0.7359\n",
      "Epoch 302/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7347 - val_loss: 0.5549 - val_accuracy: 0.7354\n",
      "Epoch 303/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7334 - val_loss: 0.5569 - val_accuracy: 0.7354\n",
      "Epoch 304/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7348 - val_loss: 0.5562 - val_accuracy: 0.7370\n",
      "Epoch 305/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7338 - val_loss: 0.5576 - val_accuracy: 0.7344\n",
      "Epoch 306/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7335 - val_loss: 0.5568 - val_accuracy: 0.7367\n",
      "Epoch 307/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7355 - val_loss: 0.5572 - val_accuracy: 0.7354\n",
      "Epoch 308/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5441 - accuracy: 0.7345 - val_loss: 0.5565 - val_accuracy: 0.7352\n",
      "Epoch 309/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5445 - accuracy: 0.7347 - val_loss: 0.5554 - val_accuracy: 0.7367\n",
      "Epoch 310/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7345 - val_loss: 0.5569 - val_accuracy: 0.7339\n",
      "Epoch 311/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7323 - val_loss: 0.5560 - val_accuracy: 0.7349\n",
      "Epoch 312/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7339 - val_loss: 0.5559 - val_accuracy: 0.7352\n",
      "Epoch 313/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7343 - val_loss: 0.5553 - val_accuracy: 0.7359\n",
      "Epoch 314/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7349 - val_loss: 0.5586 - val_accuracy: 0.7331\n",
      "Epoch 315/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7342 - val_loss: 0.5556 - val_accuracy: 0.7349\n",
      "Epoch 316/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7349 - val_loss: 0.5617 - val_accuracy: 0.7349\n",
      "Epoch 317/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7340 - val_loss: 0.5589 - val_accuracy: 0.7367\n",
      "Epoch 318/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5444 - accuracy: 0.7336 - val_loss: 0.5555 - val_accuracy: 0.7365\n",
      "Epoch 319/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5443 - accuracy: 0.7342 - val_loss: 0.5565 - val_accuracy: 0.7344\n",
      "Epoch 320/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7341 - val_loss: 0.5579 - val_accuracy: 0.7352\n",
      "Epoch 321/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7346 - val_loss: 0.5562 - val_accuracy: 0.7349\n",
      "Epoch 322/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7347 - val_loss: 0.5566 - val_accuracy: 0.7344\n",
      "Epoch 323/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7336 - val_loss: 0.5548 - val_accuracy: 0.7357\n",
      "Epoch 324/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5440 - accuracy: 0.7330 - val_loss: 0.5560 - val_accuracy: 0.7370\n",
      "Epoch 325/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5442 - accuracy: 0.7348 - val_loss: 0.5562 - val_accuracy: 0.7352\n",
      "Epoch 326/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5441 - accuracy: 0.7335 - val_loss: 0.5582 - val_accuracy: 0.7362\n",
      "Epoch 327/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7348 - val_loss: 0.5545 - val_accuracy: 0.7349\n",
      "Epoch 328/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7349 - val_loss: 0.5572 - val_accuracy: 0.7362\n",
      "Epoch 329/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7340 - val_loss: 0.5568 - val_accuracy: 0.7365\n",
      "Epoch 330/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7345 - val_loss: 0.5577 - val_accuracy: 0.7321\n",
      "Epoch 331/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7342 - val_loss: 0.5544 - val_accuracy: 0.7352\n",
      "Epoch 332/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7338 - val_loss: 0.5558 - val_accuracy: 0.7365\n",
      "Epoch 333/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7344 - val_loss: 0.5557 - val_accuracy: 0.7354\n",
      "Epoch 334/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7331 - val_loss: 0.5541 - val_accuracy: 0.7367\n",
      "Epoch 335/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7341 - val_loss: 0.5564 - val_accuracy: 0.7357\n",
      "Epoch 336/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7351 - val_loss: 0.5549 - val_accuracy: 0.7367\n",
      "Epoch 337/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7342 - val_loss: 0.5549 - val_accuracy: 0.7354\n",
      "Epoch 338/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7347 - val_loss: 0.5562 - val_accuracy: 0.7359\n",
      "Epoch 339/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7346 - val_loss: 0.5570 - val_accuracy: 0.7362\n",
      "Epoch 340/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5437 - accuracy: 0.7331 - val_loss: 0.5587 - val_accuracy: 0.7359\n",
      "Epoch 341/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7335 - val_loss: 0.5561 - val_accuracy: 0.7354\n",
      "Epoch 342/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7341 - val_loss: 0.5554 - val_accuracy: 0.7362\n",
      "Epoch 343/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7337 - val_loss: 0.5563 - val_accuracy: 0.7341\n",
      "Epoch 344/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7337 - val_loss: 0.5591 - val_accuracy: 0.7365\n",
      "Epoch 345/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7350 - val_loss: 0.5598 - val_accuracy: 0.7339\n",
      "Epoch 346/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7332 - val_loss: 0.5567 - val_accuracy: 0.7372\n",
      "Epoch 347/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7343 - val_loss: 0.5583 - val_accuracy: 0.7352\n",
      "Epoch 348/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7348 - val_loss: 0.5562 - val_accuracy: 0.7370\n",
      "Epoch 349/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7347 - val_loss: 0.5557 - val_accuracy: 0.7367\n",
      "Epoch 350/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7350 - val_loss: 0.5575 - val_accuracy: 0.7349\n",
      "Epoch 351/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7338 - val_loss: 0.5575 - val_accuracy: 0.7359\n",
      "Epoch 352/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7341 - val_loss: 0.5578 - val_accuracy: 0.7339\n",
      "Epoch 353/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7345 - val_loss: 0.5574 - val_accuracy: 0.7349\n",
      "Epoch 354/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7349 - val_loss: 0.5590 - val_accuracy: 0.7352\n",
      "Epoch 355/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7346 - val_loss: 0.5587 - val_accuracy: 0.7328\n",
      "Epoch 356/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7346 - val_loss: 0.5594 - val_accuracy: 0.7344\n",
      "Epoch 357/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7344 - val_loss: 0.5580 - val_accuracy: 0.7354\n",
      "Epoch 358/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7350 - val_loss: 0.5571 - val_accuracy: 0.7354\n",
      "Epoch 359/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7346 - val_loss: 0.5572 - val_accuracy: 0.7357\n",
      "Epoch 360/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7343 - val_loss: 0.5583 - val_accuracy: 0.7346\n",
      "Epoch 361/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7350 - val_loss: 0.5576 - val_accuracy: 0.7359\n",
      "Epoch 362/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7349 - val_loss: 0.5579 - val_accuracy: 0.7334\n",
      "Epoch 363/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5437 - accuracy: 0.7340 - val_loss: 0.5585 - val_accuracy: 0.7352\n",
      "Epoch 364/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7353 - val_loss: 0.5563 - val_accuracy: 0.7365\n",
      "Epoch 365/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7343 - val_loss: 0.5556 - val_accuracy: 0.7367\n",
      "Epoch 366/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7331 - val_loss: 0.5577 - val_accuracy: 0.7365\n",
      "Epoch 367/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7341 - val_loss: 0.5569 - val_accuracy: 0.7362\n",
      "Epoch 368/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5437 - accuracy: 0.7347 - val_loss: 0.5567 - val_accuracy: 0.7365\n",
      "Epoch 369/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7341 - val_loss: 0.5558 - val_accuracy: 0.7362\n",
      "Epoch 370/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7332 - val_loss: 0.5610 - val_accuracy: 0.7346\n",
      "Epoch 371/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7340 - val_loss: 0.5591 - val_accuracy: 0.7295\n",
      "Epoch 372/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7337 - val_loss: 0.5563 - val_accuracy: 0.7362\n",
      "Epoch 373/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5437 - accuracy: 0.7350 - val_loss: 0.5559 - val_accuracy: 0.7352\n",
      "Epoch 374/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7349 - val_loss: 0.5546 - val_accuracy: 0.7359\n",
      "Epoch 375/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7343 - val_loss: 0.5564 - val_accuracy: 0.7352\n",
      "Epoch 376/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7350 - val_loss: 0.5606 - val_accuracy: 0.7349\n",
      "Epoch 377/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7348 - val_loss: 0.5566 - val_accuracy: 0.7346\n",
      "Epoch 378/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7336 - val_loss: 0.5584 - val_accuracy: 0.7349\n",
      "Epoch 379/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7345 - val_loss: 0.5590 - val_accuracy: 0.7349\n",
      "Epoch 380/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7350 - val_loss: 0.5574 - val_accuracy: 0.7352\n",
      "Epoch 381/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7344 - val_loss: 0.5557 - val_accuracy: 0.7357\n",
      "Epoch 382/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7346 - val_loss: 0.5564 - val_accuracy: 0.7359\n",
      "Epoch 383/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7353 - val_loss: 0.5560 - val_accuracy: 0.7354\n",
      "Epoch 384/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7345 - val_loss: 0.5554 - val_accuracy: 0.7367\n",
      "Epoch 385/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7345 - val_loss: 0.5568 - val_accuracy: 0.7352\n",
      "Epoch 386/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7343 - val_loss: 0.5615 - val_accuracy: 0.7308\n",
      "Epoch 387/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7345 - val_loss: 0.5566 - val_accuracy: 0.7354\n",
      "Epoch 388/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7352 - val_loss: 0.5592 - val_accuracy: 0.7349\n",
      "Epoch 389/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7350 - val_loss: 0.5574 - val_accuracy: 0.7362\n",
      "Epoch 390/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7340 - val_loss: 0.5593 - val_accuracy: 0.7308\n",
      "Epoch 391/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7352 - val_loss: 0.5576 - val_accuracy: 0.7344\n",
      "Epoch 392/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7343 - val_loss: 0.5593 - val_accuracy: 0.7367\n",
      "Epoch 393/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7351 - val_loss: 0.5572 - val_accuracy: 0.7362\n",
      "Epoch 394/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7341 - val_loss: 0.5608 - val_accuracy: 0.7297\n",
      "Epoch 395/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7358 - val_loss: 0.5579 - val_accuracy: 0.7352\n",
      "Epoch 396/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7354 - val_loss: 0.5584 - val_accuracy: 0.7344\n",
      "Epoch 397/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7343 - val_loss: 0.5563 - val_accuracy: 0.7336\n",
      "Epoch 398/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7336 - val_loss: 0.5572 - val_accuracy: 0.7352\n",
      "Epoch 399/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7356 - val_loss: 0.5579 - val_accuracy: 0.7357\n",
      "Epoch 400/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7352 - val_loss: 0.5577 - val_accuracy: 0.7357\n",
      "Epoch 401/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7335 - val_loss: 0.5571 - val_accuracy: 0.7372\n",
      "Epoch 402/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7335 - val_loss: 0.5572 - val_accuracy: 0.7370\n",
      "Epoch 403/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7354 - val_loss: 0.5591 - val_accuracy: 0.7362\n",
      "Epoch 404/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7339 - val_loss: 0.5580 - val_accuracy: 0.7357\n",
      "Epoch 405/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7340 - val_loss: 0.5583 - val_accuracy: 0.7346\n",
      "Epoch 406/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7352 - val_loss: 0.5649 - val_accuracy: 0.7300\n",
      "Epoch 407/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7354 - val_loss: 0.5596 - val_accuracy: 0.7331\n",
      "Epoch 408/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7333 - val_loss: 0.5596 - val_accuracy: 0.7344\n",
      "Epoch 409/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5437 - accuracy: 0.7357 - val_loss: 0.5575 - val_accuracy: 0.7352\n",
      "Epoch 410/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7351 - val_loss: 0.5579 - val_accuracy: 0.7365\n",
      "Epoch 411/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7338 - val_loss: 0.5559 - val_accuracy: 0.7354\n",
      "Epoch 412/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7344 - val_loss: 0.5579 - val_accuracy: 0.7354\n",
      "Epoch 413/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7350 - val_loss: 0.5576 - val_accuracy: 0.7365\n",
      "Epoch 414/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7354 - val_loss: 0.5558 - val_accuracy: 0.7367\n",
      "Epoch 415/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7348 - val_loss: 0.5568 - val_accuracy: 0.7367\n",
      "Epoch 416/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7342 - val_loss: 0.5592 - val_accuracy: 0.7334\n",
      "Epoch 417/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7335 - val_loss: 0.5602 - val_accuracy: 0.7352\n",
      "Epoch 418/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7349 - val_loss: 0.5560 - val_accuracy: 0.7346\n",
      "Epoch 419/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5437 - accuracy: 0.7344 - val_loss: 0.5612 - val_accuracy: 0.7341\n",
      "Epoch 420/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7352 - val_loss: 0.5574 - val_accuracy: 0.7367\n",
      "Epoch 421/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7355 - val_loss: 0.5576 - val_accuracy: 0.7334\n",
      "Epoch 422/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7344 - val_loss: 0.5602 - val_accuracy: 0.7336\n",
      "Epoch 423/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7342 - val_loss: 0.5583 - val_accuracy: 0.7365\n",
      "Epoch 424/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7340 - val_loss: 0.5577 - val_accuracy: 0.7362\n",
      "Epoch 425/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7342 - val_loss: 0.5578 - val_accuracy: 0.7354\n",
      "Epoch 426/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7349 - val_loss: 0.5585 - val_accuracy: 0.7359\n",
      "Epoch 427/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7342 - val_loss: 0.5582 - val_accuracy: 0.7346\n",
      "Epoch 428/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7338 - val_loss: 0.5565 - val_accuracy: 0.7362\n",
      "Epoch 429/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7336 - val_loss: 0.5589 - val_accuracy: 0.7359\n",
      "Epoch 430/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7341 - val_loss: 0.5576 - val_accuracy: 0.7349\n",
      "Epoch 431/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7341 - val_loss: 0.5581 - val_accuracy: 0.7344\n",
      "Epoch 432/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7344 - val_loss: 0.5578 - val_accuracy: 0.7354\n",
      "Epoch 433/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7354 - val_loss: 0.5569 - val_accuracy: 0.7362\n",
      "Epoch 434/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7345 - val_loss: 0.5566 - val_accuracy: 0.7357\n",
      "Epoch 435/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7351 - val_loss: 0.5614 - val_accuracy: 0.7331\n",
      "Epoch 436/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7346 - val_loss: 0.5583 - val_accuracy: 0.7349\n",
      "Epoch 437/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7339 - val_loss: 0.5616 - val_accuracy: 0.7365\n",
      "Epoch 438/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7343 - val_loss: 0.5611 - val_accuracy: 0.7354\n",
      "Epoch 439/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7350 - val_loss: 0.5561 - val_accuracy: 0.7365\n",
      "Epoch 440/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7339 - val_loss: 0.5553 - val_accuracy: 0.7357\n",
      "Epoch 441/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7355 - val_loss: 0.5559 - val_accuracy: 0.7365\n",
      "Epoch 442/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7346 - val_loss: 0.5577 - val_accuracy: 0.7352\n",
      "Epoch 443/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7350 - val_loss: 0.5596 - val_accuracy: 0.7334\n",
      "Epoch 444/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7344 - val_loss: 0.5584 - val_accuracy: 0.7357\n",
      "Epoch 445/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7338 - val_loss: 0.5590 - val_accuracy: 0.7323\n",
      "Epoch 446/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7352 - val_loss: 0.5585 - val_accuracy: 0.7359\n",
      "Epoch 447/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7349 - val_loss: 0.5597 - val_accuracy: 0.7341\n",
      "Epoch 448/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7343 - val_loss: 0.5586 - val_accuracy: 0.7357\n",
      "Epoch 449/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7344 - val_loss: 0.5573 - val_accuracy: 0.7349\n",
      "Epoch 450/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7351 - val_loss: 0.5565 - val_accuracy: 0.7341\n",
      "Epoch 451/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7348 - val_loss: 0.5571 - val_accuracy: 0.7367\n",
      "Epoch 452/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7344 - val_loss: 0.5574 - val_accuracy: 0.7359\n",
      "Epoch 453/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7352 - val_loss: 0.5582 - val_accuracy: 0.7344\n",
      "Epoch 454/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7345 - val_loss: 0.5578 - val_accuracy: 0.7354\n",
      "Epoch 455/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7357 - val_loss: 0.5589 - val_accuracy: 0.7357\n",
      "Epoch 456/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7341 - val_loss: 0.5614 - val_accuracy: 0.7341\n",
      "Epoch 457/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7347 - val_loss: 0.5572 - val_accuracy: 0.7357\n",
      "Epoch 458/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7341 - val_loss: 0.5607 - val_accuracy: 0.7352\n",
      "Epoch 459/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7353 - val_loss: 0.5579 - val_accuracy: 0.7354\n",
      "Epoch 460/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7351 - val_loss: 0.5586 - val_accuracy: 0.7359\n",
      "Epoch 461/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7354 - val_loss: 0.5591 - val_accuracy: 0.7341\n",
      "Epoch 462/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7338 - val_loss: 0.5584 - val_accuracy: 0.7362\n",
      "Epoch 463/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7347 - val_loss: 0.5565 - val_accuracy: 0.7357\n",
      "Epoch 464/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7357 - val_loss: 0.5580 - val_accuracy: 0.7372\n",
      "Epoch 465/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7359 - val_loss: 0.5606 - val_accuracy: 0.7362\n",
      "Epoch 466/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7352 - val_loss: 0.5584 - val_accuracy: 0.7359\n",
      "Epoch 467/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7352 - val_loss: 0.5567 - val_accuracy: 0.7365\n",
      "Epoch 468/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7344 - val_loss: 0.5578 - val_accuracy: 0.7349\n",
      "Epoch 469/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7352 - val_loss: 0.5599 - val_accuracy: 0.7359\n",
      "Epoch 470/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7344 - val_loss: 0.5597 - val_accuracy: 0.7349\n",
      "Epoch 471/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7339 - val_loss: 0.5579 - val_accuracy: 0.7359\n",
      "Epoch 472/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7351 - val_loss: 0.5583 - val_accuracy: 0.7359\n",
      "Epoch 473/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7353 - val_loss: 0.5563 - val_accuracy: 0.7354\n",
      "Epoch 474/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7343 - val_loss: 0.5583 - val_accuracy: 0.7354\n",
      "Epoch 475/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7340 - val_loss: 0.5575 - val_accuracy: 0.7359\n",
      "Epoch 476/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7346 - val_loss: 0.5588 - val_accuracy: 0.7352\n",
      "Epoch 477/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7353 - val_loss: 0.5588 - val_accuracy: 0.7344\n",
      "Epoch 478/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7350 - val_loss: 0.5594 - val_accuracy: 0.7341\n",
      "Epoch 479/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7358 - val_loss: 0.5574 - val_accuracy: 0.7367\n",
      "Epoch 480/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7346 - val_loss: 0.5592 - val_accuracy: 0.7328\n",
      "Epoch 481/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7351 - val_loss: 0.5566 - val_accuracy: 0.7349\n",
      "Epoch 482/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7353 - val_loss: 0.5603 - val_accuracy: 0.7344\n",
      "Epoch 483/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7342 - val_loss: 0.5614 - val_accuracy: 0.7352\n",
      "Epoch 484/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7356 - val_loss: 0.5613 - val_accuracy: 0.7346\n",
      "Epoch 485/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7346 - val_loss: 0.5595 - val_accuracy: 0.7354\n",
      "Epoch 486/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7346 - val_loss: 0.5573 - val_accuracy: 0.7352\n",
      "Epoch 487/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7350 - val_loss: 0.5600 - val_accuracy: 0.7365\n",
      "Epoch 488/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7343 - val_loss: 0.5573 - val_accuracy: 0.7359\n",
      "Epoch 489/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7348 - val_loss: 0.5586 - val_accuracy: 0.7362\n",
      "Epoch 490/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7353 - val_loss: 0.5598 - val_accuracy: 0.7365\n",
      "Epoch 491/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7346 - val_loss: 0.5587 - val_accuracy: 0.7354\n",
      "Epoch 492/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7344 - val_loss: 0.5577 - val_accuracy: 0.7367\n",
      "Epoch 493/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7353 - val_loss: 0.5580 - val_accuracy: 0.7365\n",
      "Epoch 494/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7348 - val_loss: 0.5573 - val_accuracy: 0.7354\n",
      "Epoch 495/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7351 - val_loss: 0.5580 - val_accuracy: 0.7362\n",
      "Epoch 496/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7362 - val_loss: 0.5582 - val_accuracy: 0.7349\n",
      "Epoch 497/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7351 - val_loss: 0.5587 - val_accuracy: 0.7357\n",
      "Epoch 498/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7351 - val_loss: 0.5579 - val_accuracy: 0.7354\n",
      "Epoch 499/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7362 - val_loss: 0.5570 - val_accuracy: 0.7367\n",
      "Epoch 500/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7356 - val_loss: 0.5588 - val_accuracy: 0.7349\n",
      "Epoch 501/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7350 - val_loss: 0.5588 - val_accuracy: 0.7352\n",
      "Epoch 502/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7352 - val_loss: 0.5596 - val_accuracy: 0.7344\n",
      "Epoch 503/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7346 - val_loss: 0.5564 - val_accuracy: 0.7357\n",
      "Epoch 504/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7362 - val_loss: 0.5590 - val_accuracy: 0.7357\n",
      "Epoch 505/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7346 - val_loss: 0.5584 - val_accuracy: 0.7354\n",
      "Epoch 506/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7357 - val_loss: 0.5594 - val_accuracy: 0.7354\n",
      "Epoch 507/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7352 - val_loss: 0.5584 - val_accuracy: 0.7367\n",
      "Epoch 508/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7339 - val_loss: 0.5576 - val_accuracy: 0.7362\n",
      "Epoch 509/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7341 - val_loss: 0.5579 - val_accuracy: 0.7365\n",
      "Epoch 510/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7357 - val_loss: 0.5588 - val_accuracy: 0.7362\n",
      "Epoch 511/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7356 - val_loss: 0.5581 - val_accuracy: 0.7362\n",
      "Epoch 512/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7343 - val_loss: 0.5594 - val_accuracy: 0.7354\n",
      "Epoch 513/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7357 - val_loss: 0.5571 - val_accuracy: 0.7344\n",
      "Epoch 514/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7348 - val_loss: 0.5597 - val_accuracy: 0.7344\n",
      "Epoch 515/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7349 - val_loss: 0.5583 - val_accuracy: 0.7352\n",
      "Epoch 516/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7354 - val_loss: 0.5593 - val_accuracy: 0.7359\n",
      "Epoch 517/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7355 - val_loss: 0.5597 - val_accuracy: 0.7349\n",
      "Epoch 518/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7353 - val_loss: 0.5582 - val_accuracy: 0.7357\n",
      "Epoch 519/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7337 - val_loss: 0.5593 - val_accuracy: 0.7310\n",
      "Epoch 520/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7342 - val_loss: 0.5595 - val_accuracy: 0.7341\n",
      "Epoch 521/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7348 - val_loss: 0.5586 - val_accuracy: 0.7370\n",
      "Epoch 522/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7350 - val_loss: 0.5571 - val_accuracy: 0.7380\n",
      "Epoch 523/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7350 - val_loss: 0.5596 - val_accuracy: 0.7346\n",
      "Epoch 524/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7358 - val_loss: 0.5659 - val_accuracy: 0.7276\n",
      "Epoch 525/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7344 - val_loss: 0.5617 - val_accuracy: 0.7339\n",
      "Epoch 526/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7355 - val_loss: 0.5609 - val_accuracy: 0.7318\n",
      "Epoch 527/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7348 - val_loss: 0.5579 - val_accuracy: 0.7352\n",
      "Epoch 528/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7350 - val_loss: 0.5580 - val_accuracy: 0.7378\n",
      "Epoch 529/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7354 - val_loss: 0.5590 - val_accuracy: 0.7352\n",
      "Epoch 530/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7352 - val_loss: 0.5596 - val_accuracy: 0.7346\n",
      "Epoch 531/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7358 - val_loss: 0.5601 - val_accuracy: 0.7349\n",
      "Epoch 532/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7361 - val_loss: 0.5579 - val_accuracy: 0.7367\n",
      "Epoch 533/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7356 - val_loss: 0.5588 - val_accuracy: 0.7359\n",
      "Epoch 534/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7356 - val_loss: 0.5599 - val_accuracy: 0.7359\n",
      "Epoch 535/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7356 - val_loss: 0.5578 - val_accuracy: 0.7362\n",
      "Epoch 536/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7357 - val_loss: 0.5592 - val_accuracy: 0.7349\n",
      "Epoch 537/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7346 - val_loss: 0.5588 - val_accuracy: 0.7341\n",
      "Epoch 538/800\n",
      "684/684 [==============================] - 3s 5ms/step - loss: 0.5429 - accuracy: 0.7345 - val_loss: 0.5590 - val_accuracy: 0.7362\n",
      "Epoch 539/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5423 - accuracy: 0.7353 - val_loss: 0.5580 - val_accuracy: 0.7365\n",
      "Epoch 540/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7356 - val_loss: 0.5608 - val_accuracy: 0.7318\n",
      "Epoch 541/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7347 - val_loss: 0.5575 - val_accuracy: 0.7365\n",
      "Epoch 542/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5421 - accuracy: 0.7361 - val_loss: 0.5570 - val_accuracy: 0.7365\n",
      "Epoch 543/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7357 - val_loss: 0.5576 - val_accuracy: 0.7354\n",
      "Epoch 544/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7345 - val_loss: 0.5581 - val_accuracy: 0.7354\n",
      "Epoch 545/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7348 - val_loss: 0.5590 - val_accuracy: 0.7336\n",
      "Epoch 546/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7349 - val_loss: 0.5594 - val_accuracy: 0.7346\n",
      "Epoch 547/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7357 - val_loss: 0.5586 - val_accuracy: 0.7365\n",
      "Epoch 548/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5425 - accuracy: 0.7353 - val_loss: 0.5584 - val_accuracy: 0.7349\n",
      "Epoch 549/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7357 - val_loss: 0.5593 - val_accuracy: 0.7367\n",
      "Epoch 550/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5425 - accuracy: 0.7349 - val_loss: 0.5590 - val_accuracy: 0.7349\n",
      "Epoch 551/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7353 - val_loss: 0.5610 - val_accuracy: 0.7341\n",
      "Epoch 552/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7341 - val_loss: 0.5590 - val_accuracy: 0.7359\n",
      "Epoch 553/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7356 - val_loss: 0.5571 - val_accuracy: 0.7365\n",
      "Epoch 554/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5425 - accuracy: 0.7361 - val_loss: 0.5603 - val_accuracy: 0.7305\n",
      "Epoch 555/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7346 - val_loss: 0.5587 - val_accuracy: 0.7362\n",
      "Epoch 556/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.7353 - val_loss: 0.5585 - val_accuracy: 0.7346\n",
      "Epoch 557/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.7349 - val_loss: 0.5612 - val_accuracy: 0.7344\n",
      "Epoch 558/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5428 - accuracy: 0.7356 - val_loss: 0.5592 - val_accuracy: 0.7357\n",
      "Epoch 559/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7356 - val_loss: 0.5606 - val_accuracy: 0.7352\n",
      "Epoch 560/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7357 - val_loss: 0.5601 - val_accuracy: 0.7357\n",
      "Epoch 561/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5424 - accuracy: 0.7357 - val_loss: 0.5607 - val_accuracy: 0.7367\n",
      "Epoch 562/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7355 - val_loss: 0.5593 - val_accuracy: 0.7352\n",
      "Epoch 563/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7357 - val_loss: 0.5602 - val_accuracy: 0.7357\n",
      "Epoch 564/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5418 - accuracy: 0.7358 - val_loss: 0.5590 - val_accuracy: 0.7367\n",
      "Epoch 565/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7346 - val_loss: 0.5616 - val_accuracy: 0.7339\n",
      "Epoch 566/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7360 - val_loss: 0.5591 - val_accuracy: 0.7359\n",
      "Epoch 567/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7355 - val_loss: 0.5577 - val_accuracy: 0.7372\n",
      "Epoch 568/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7343 - val_loss: 0.5578 - val_accuracy: 0.7365\n",
      "Epoch 569/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7350 - val_loss: 0.5579 - val_accuracy: 0.7370\n",
      "Epoch 570/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7351 - val_loss: 0.5592 - val_accuracy: 0.7359\n",
      "Epoch 571/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5425 - accuracy: 0.7364 - val_loss: 0.5604 - val_accuracy: 0.7346\n",
      "Epoch 572/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7357 - val_loss: 0.5593 - val_accuracy: 0.7346\n",
      "Epoch 573/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.7352 - val_loss: 0.5621 - val_accuracy: 0.7295\n",
      "Epoch 574/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.7343 - val_loss: 0.5568 - val_accuracy: 0.7349\n",
      "Epoch 575/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7352 - val_loss: 0.5603 - val_accuracy: 0.7344\n",
      "Epoch 576/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7362 - val_loss: 0.5604 - val_accuracy: 0.7354\n",
      "Epoch 577/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7346 - val_loss: 0.5611 - val_accuracy: 0.7344\n",
      "Epoch 578/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5424 - accuracy: 0.7352 - val_loss: 0.5580 - val_accuracy: 0.7354\n",
      "Epoch 579/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7354 - val_loss: 0.5578 - val_accuracy: 0.7365\n",
      "Epoch 580/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7352 - val_loss: 0.5586 - val_accuracy: 0.7359\n",
      "Epoch 581/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7330 - val_loss: 0.5607 - val_accuracy: 0.7336\n",
      "Epoch 582/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7354 - val_loss: 0.5602 - val_accuracy: 0.7357\n",
      "Epoch 583/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7349 - val_loss: 0.5641 - val_accuracy: 0.7334\n",
      "Epoch 584/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7355 - val_loss: 0.5585 - val_accuracy: 0.7357\n",
      "Epoch 585/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7358 - val_loss: 0.5608 - val_accuracy: 0.7341\n",
      "Epoch 586/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7348 - val_loss: 0.5599 - val_accuracy: 0.7352\n",
      "Epoch 587/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7354 - val_loss: 0.5580 - val_accuracy: 0.7357\n",
      "Epoch 588/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7355 - val_loss: 0.5588 - val_accuracy: 0.7357\n",
      "Epoch 589/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7360 - val_loss: 0.5602 - val_accuracy: 0.7354\n",
      "Epoch 590/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7354 - val_loss: 0.5575 - val_accuracy: 0.7349\n",
      "Epoch 591/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7353 - val_loss: 0.5596 - val_accuracy: 0.7344\n",
      "Epoch 592/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7344 - val_loss: 0.5593 - val_accuracy: 0.7349\n",
      "Epoch 593/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7351 - val_loss: 0.5590 - val_accuracy: 0.7354\n",
      "Epoch 594/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7355 - val_loss: 0.5579 - val_accuracy: 0.7365\n",
      "Epoch 595/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7357 - val_loss: 0.5633 - val_accuracy: 0.7302\n",
      "Epoch 596/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7362 - val_loss: 0.5593 - val_accuracy: 0.7362\n",
      "Epoch 597/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7357 - val_loss: 0.5572 - val_accuracy: 0.7357\n",
      "Epoch 598/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7352 - val_loss: 0.5586 - val_accuracy: 0.7352\n",
      "Epoch 599/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7344 - val_loss: 0.5629 - val_accuracy: 0.7310\n",
      "Epoch 600/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7359 - val_loss: 0.5586 - val_accuracy: 0.7357\n",
      "Epoch 601/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7357 - val_loss: 0.5594 - val_accuracy: 0.7357\n",
      "Epoch 602/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7346 - val_loss: 0.5583 - val_accuracy: 0.7354\n",
      "Epoch 603/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7357 - val_loss: 0.5593 - val_accuracy: 0.7357\n",
      "Epoch 604/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7350 - val_loss: 0.5615 - val_accuracy: 0.7336\n",
      "Epoch 605/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7356 - val_loss: 0.5594 - val_accuracy: 0.7346\n",
      "Epoch 606/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7353 - val_loss: 0.5600 - val_accuracy: 0.7359\n",
      "Epoch 607/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7356 - val_loss: 0.5615 - val_accuracy: 0.7339\n",
      "Epoch 608/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7355 - val_loss: 0.5633 - val_accuracy: 0.7318\n",
      "Epoch 609/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7353 - val_loss: 0.5578 - val_accuracy: 0.7354\n",
      "Epoch 610/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7353 - val_loss: 0.5561 - val_accuracy: 0.7370\n",
      "Epoch 611/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7348 - val_loss: 0.5603 - val_accuracy: 0.7300\n",
      "Epoch 612/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7361 - val_loss: 0.5582 - val_accuracy: 0.7357\n",
      "Epoch 613/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7353 - val_loss: 0.5613 - val_accuracy: 0.7339\n",
      "Epoch 614/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7352 - val_loss: 0.5620 - val_accuracy: 0.7346\n",
      "Epoch 615/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7361 - val_loss: 0.5594 - val_accuracy: 0.7349\n",
      "Epoch 616/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7362 - val_loss: 0.5579 - val_accuracy: 0.7365\n",
      "Epoch 617/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7355 - val_loss: 0.5610 - val_accuracy: 0.7370\n",
      "Epoch 618/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7358 - val_loss: 0.5594 - val_accuracy: 0.7318\n",
      "Epoch 619/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5419 - accuracy: 0.7356 - val_loss: 0.5622 - val_accuracy: 0.7336\n",
      "Epoch 620/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7363 - val_loss: 0.5609 - val_accuracy: 0.7359\n",
      "Epoch 621/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7360 - val_loss: 0.5600 - val_accuracy: 0.7359\n",
      "Epoch 622/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7364 - val_loss: 0.5617 - val_accuracy: 0.7318\n",
      "Epoch 623/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7350 - val_loss: 0.5568 - val_accuracy: 0.7365\n",
      "Epoch 624/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7351 - val_loss: 0.5592 - val_accuracy: 0.7336\n",
      "Epoch 625/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7360 - val_loss: 0.5652 - val_accuracy: 0.7274\n",
      "Epoch 626/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7357 - val_loss: 0.5604 - val_accuracy: 0.7349\n",
      "Epoch 627/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5425 - accuracy: 0.7355 - val_loss: 0.5591 - val_accuracy: 0.7349\n",
      "Epoch 628/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7354 - val_loss: 0.5617 - val_accuracy: 0.7346\n",
      "Epoch 629/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7345 - val_loss: 0.5599 - val_accuracy: 0.7334\n",
      "Epoch 630/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7351 - val_loss: 0.5589 - val_accuracy: 0.7365\n",
      "Epoch 631/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7352 - val_loss: 0.5643 - val_accuracy: 0.7300\n",
      "Epoch 632/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.7352 - val_loss: 0.5595 - val_accuracy: 0.7367\n",
      "Epoch 633/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7361 - val_loss: 0.5672 - val_accuracy: 0.7256\n",
      "Epoch 634/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7353 - val_loss: 0.5613 - val_accuracy: 0.7341\n",
      "Epoch 635/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7365 - val_loss: 0.5610 - val_accuracy: 0.7357\n",
      "Epoch 636/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7356 - val_loss: 0.5617 - val_accuracy: 0.7336\n",
      "Epoch 637/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7353 - val_loss: 0.5622 - val_accuracy: 0.7357\n",
      "Epoch 638/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7359 - val_loss: 0.5626 - val_accuracy: 0.7367\n",
      "Epoch 639/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7353 - val_loss: 0.5632 - val_accuracy: 0.7336\n",
      "Epoch 640/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7348 - val_loss: 0.5651 - val_accuracy: 0.7318\n",
      "Epoch 641/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7358 - val_loss: 0.5615 - val_accuracy: 0.7354\n",
      "Epoch 642/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7357 - val_loss: 0.5612 - val_accuracy: 0.7321\n",
      "Epoch 643/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7357 - val_loss: 0.5614 - val_accuracy: 0.7344\n",
      "Epoch 644/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7356 - val_loss: 0.5610 - val_accuracy: 0.7346\n",
      "Epoch 645/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7356 - val_loss: 0.5598 - val_accuracy: 0.7352\n",
      "Epoch 646/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7348 - val_loss: 0.5583 - val_accuracy: 0.7352\n",
      "Epoch 647/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7346 - val_loss: 0.5624 - val_accuracy: 0.7297\n",
      "Epoch 648/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7354 - val_loss: 0.5610 - val_accuracy: 0.7352\n",
      "Epoch 649/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7336 - val_loss: 0.5591 - val_accuracy: 0.7362\n",
      "Epoch 650/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7360 - val_loss: 0.5599 - val_accuracy: 0.7359\n",
      "Epoch 651/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7352 - val_loss: 0.5630 - val_accuracy: 0.7318\n",
      "Epoch 652/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7350 - val_loss: 0.5602 - val_accuracy: 0.7341\n",
      "Epoch 653/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7355 - val_loss: 0.5605 - val_accuracy: 0.7308\n",
      "Epoch 654/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7335 - val_loss: 0.5598 - val_accuracy: 0.7334\n",
      "Epoch 655/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7360 - val_loss: 0.5600 - val_accuracy: 0.7352\n",
      "Epoch 656/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7352 - val_loss: 0.5621 - val_accuracy: 0.7344\n",
      "Epoch 657/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7360 - val_loss: 0.5622 - val_accuracy: 0.7334\n",
      "Epoch 658/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7354 - val_loss: 0.5643 - val_accuracy: 0.7297\n",
      "Epoch 659/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7360 - val_loss: 0.5596 - val_accuracy: 0.7344\n",
      "Epoch 660/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7357 - val_loss: 0.5614 - val_accuracy: 0.7341\n",
      "Epoch 661/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7360 - val_loss: 0.5614 - val_accuracy: 0.7349\n",
      "Epoch 662/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7362 - val_loss: 0.5599 - val_accuracy: 0.7357\n",
      "Epoch 663/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7362 - val_loss: 0.5620 - val_accuracy: 0.7344\n",
      "Epoch 664/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7351 - val_loss: 0.5603 - val_accuracy: 0.7352\n",
      "Epoch 665/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7357 - val_loss: 0.5600 - val_accuracy: 0.7354\n",
      "Epoch 666/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7356 - val_loss: 0.5616 - val_accuracy: 0.7344\n",
      "Epoch 667/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7353 - val_loss: 0.5621 - val_accuracy: 0.7346\n",
      "Epoch 668/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7356 - val_loss: 0.5611 - val_accuracy: 0.7357\n",
      "Epoch 669/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7356 - val_loss: 0.5614 - val_accuracy: 0.7375\n",
      "Epoch 670/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7341 - val_loss: 0.5602 - val_accuracy: 0.7354\n",
      "Epoch 671/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5420 - accuracy: 0.7364 - val_loss: 0.5599 - val_accuracy: 0.7367\n",
      "Epoch 672/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7352 - val_loss: 0.5605 - val_accuracy: 0.7328\n",
      "Epoch 673/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7355 - val_loss: 0.5619 - val_accuracy: 0.7341\n",
      "Epoch 674/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7351 - val_loss: 0.5604 - val_accuracy: 0.7346\n",
      "Epoch 675/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7351 - val_loss: 0.5627 - val_accuracy: 0.7357\n",
      "Epoch 676/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7352 - val_loss: 0.5599 - val_accuracy: 0.7349\n",
      "Epoch 677/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7351 - val_loss: 0.5600 - val_accuracy: 0.7346\n",
      "Epoch 678/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7349 - val_loss: 0.5635 - val_accuracy: 0.7305\n",
      "Epoch 679/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7361 - val_loss: 0.5596 - val_accuracy: 0.7357\n",
      "Epoch 680/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7354 - val_loss: 0.5607 - val_accuracy: 0.7354\n",
      "Epoch 681/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7356 - val_loss: 0.5605 - val_accuracy: 0.7341\n",
      "Epoch 682/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7357 - val_loss: 0.5588 - val_accuracy: 0.7354\n",
      "Epoch 683/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7348 - val_loss: 0.5608 - val_accuracy: 0.7344\n",
      "Epoch 684/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7357 - val_loss: 0.5587 - val_accuracy: 0.7365\n",
      "Epoch 685/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7365 - val_loss: 0.5592 - val_accuracy: 0.7365\n",
      "Epoch 686/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7356 - val_loss: 0.5595 - val_accuracy: 0.7346\n",
      "Epoch 687/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5418 - accuracy: 0.7359 - val_loss: 0.5613 - val_accuracy: 0.7326\n",
      "Epoch 688/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7352 - val_loss: 0.5634 - val_accuracy: 0.7326\n",
      "Epoch 689/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5415 - accuracy: 0.7357 - val_loss: 0.5632 - val_accuracy: 0.7339\n",
      "Epoch 690/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7359 - val_loss: 0.5606 - val_accuracy: 0.7336\n",
      "Epoch 691/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7350 - val_loss: 0.5617 - val_accuracy: 0.7334\n",
      "Epoch 692/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7361 - val_loss: 0.5599 - val_accuracy: 0.7349\n",
      "Epoch 693/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7352 - val_loss: 0.5611 - val_accuracy: 0.7352\n",
      "Epoch 694/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7347 - val_loss: 0.5645 - val_accuracy: 0.7334\n",
      "Epoch 695/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7363 - val_loss: 0.5606 - val_accuracy: 0.7352\n",
      "Epoch 696/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7349 - val_loss: 0.5616 - val_accuracy: 0.7349\n",
      "Epoch 697/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7357 - val_loss: 0.5618 - val_accuracy: 0.7344\n",
      "Epoch 698/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7356 - val_loss: 0.5627 - val_accuracy: 0.7341\n",
      "Epoch 699/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7365 - val_loss: 0.5594 - val_accuracy: 0.7346\n",
      "Epoch 700/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7346 - val_loss: 0.5639 - val_accuracy: 0.7334\n",
      "Epoch 701/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7361 - val_loss: 0.5622 - val_accuracy: 0.7352\n",
      "Epoch 702/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7356 - val_loss: 0.5627 - val_accuracy: 0.7357\n",
      "Epoch 703/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7360 - val_loss: 0.5619 - val_accuracy: 0.7334\n",
      "Epoch 704/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7346 - val_loss: 0.5606 - val_accuracy: 0.7359\n",
      "Epoch 705/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7355 - val_loss: 0.5613 - val_accuracy: 0.7365\n",
      "Epoch 706/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7354 - val_loss: 0.5610 - val_accuracy: 0.7357\n",
      "Epoch 707/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5417 - accuracy: 0.7363 - val_loss: 0.5627 - val_accuracy: 0.7354\n",
      "Epoch 708/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5418 - accuracy: 0.7359 - val_loss: 0.5585 - val_accuracy: 0.7365\n",
      "Epoch 709/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5421 - accuracy: 0.7360 - val_loss: 0.5610 - val_accuracy: 0.7339\n",
      "Epoch 710/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5421 - accuracy: 0.7353 - val_loss: 0.5600 - val_accuracy: 0.7352\n",
      "Epoch 711/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5419 - accuracy: 0.7346 - val_loss: 0.5609 - val_accuracy: 0.7372\n",
      "Epoch 712/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5416 - accuracy: 0.7357 - val_loss: 0.5593 - val_accuracy: 0.7362\n",
      "Epoch 713/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7348 - val_loss: 0.5599 - val_accuracy: 0.7346\n",
      "Epoch 714/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7358 - val_loss: 0.5605 - val_accuracy: 0.7370\n",
      "Epoch 715/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7355 - val_loss: 0.5611 - val_accuracy: 0.7326\n",
      "Epoch 716/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7357 - val_loss: 0.5606 - val_accuracy: 0.7344\n",
      "Epoch 717/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5413 - accuracy: 0.7365 - val_loss: 0.5621 - val_accuracy: 0.7346\n",
      "Epoch 718/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7357 - val_loss: 0.5599 - val_accuracy: 0.7362\n",
      "Epoch 719/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5418 - accuracy: 0.7353 - val_loss: 0.5610 - val_accuracy: 0.7349\n",
      "Epoch 720/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7349 - val_loss: 0.5604 - val_accuracy: 0.7349\n",
      "Epoch 721/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7350 - val_loss: 0.5590 - val_accuracy: 0.7354\n",
      "Epoch 722/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7362 - val_loss: 0.5622 - val_accuracy: 0.7295\n",
      "Epoch 723/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7351 - val_loss: 0.5609 - val_accuracy: 0.7346\n",
      "Epoch 724/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7353 - val_loss: 0.5629 - val_accuracy: 0.7339\n",
      "Epoch 725/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7358 - val_loss: 0.5629 - val_accuracy: 0.7323\n",
      "Epoch 726/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7361 - val_loss: 0.5636 - val_accuracy: 0.7346\n",
      "Epoch 727/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7360 - val_loss: 0.5616 - val_accuracy: 0.7357\n",
      "Epoch 728/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7365 - val_loss: 0.5634 - val_accuracy: 0.7336\n",
      "Epoch 729/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7367 - val_loss: 0.5634 - val_accuracy: 0.7339\n",
      "Epoch 730/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7358 - val_loss: 0.5619 - val_accuracy: 0.7346\n",
      "Epoch 731/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7359 - val_loss: 0.5611 - val_accuracy: 0.7349\n",
      "Epoch 732/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7368 - val_loss: 0.5609 - val_accuracy: 0.7362\n",
      "Epoch 733/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7362 - val_loss: 0.5619 - val_accuracy: 0.7341\n",
      "Epoch 734/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7355 - val_loss: 0.5634 - val_accuracy: 0.7341\n",
      "Epoch 735/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7362 - val_loss: 0.5607 - val_accuracy: 0.7349\n",
      "Epoch 736/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7362 - val_loss: 0.5601 - val_accuracy: 0.7359\n",
      "Epoch 737/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7349 - val_loss: 0.5633 - val_accuracy: 0.7331\n",
      "Epoch 738/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7354 - val_loss: 0.5608 - val_accuracy: 0.7334\n",
      "Epoch 739/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7353 - val_loss: 0.5628 - val_accuracy: 0.7315\n",
      "Epoch 740/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7358 - val_loss: 0.5601 - val_accuracy: 0.7308\n",
      "Epoch 741/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7351 - val_loss: 0.5616 - val_accuracy: 0.7344\n",
      "Epoch 742/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7351 - val_loss: 0.5642 - val_accuracy: 0.7359\n",
      "Epoch 743/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7368 - val_loss: 0.5646 - val_accuracy: 0.7292\n",
      "Epoch 744/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7364 - val_loss: 0.5594 - val_accuracy: 0.7344\n",
      "Epoch 745/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7356 - val_loss: 0.5603 - val_accuracy: 0.7352\n",
      "Epoch 746/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7352 - val_loss: 0.5656 - val_accuracy: 0.7315\n",
      "Epoch 747/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7359 - val_loss: 0.5626 - val_accuracy: 0.7331\n",
      "Epoch 748/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7356 - val_loss: 0.5614 - val_accuracy: 0.7352\n",
      "Epoch 749/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7357 - val_loss: 0.5630 - val_accuracy: 0.7323\n",
      "Epoch 750/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7354 - val_loss: 0.5633 - val_accuracy: 0.7331\n",
      "Epoch 751/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7348 - val_loss: 0.5606 - val_accuracy: 0.7359\n",
      "Epoch 752/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7367 - val_loss: 0.5648 - val_accuracy: 0.7310\n",
      "Epoch 753/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7346 - val_loss: 0.5606 - val_accuracy: 0.7354\n",
      "Epoch 754/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7363 - val_loss: 0.5628 - val_accuracy: 0.7357\n",
      "Epoch 755/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7368 - val_loss: 0.5622 - val_accuracy: 0.7346\n",
      "Epoch 756/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7358 - val_loss: 0.5618 - val_accuracy: 0.7344\n",
      "Epoch 757/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7356 - val_loss: 0.5630 - val_accuracy: 0.7341\n",
      "Epoch 758/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7373 - val_loss: 0.5615 - val_accuracy: 0.7346\n",
      "Epoch 759/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7360 - val_loss: 0.5614 - val_accuracy: 0.7341\n",
      "Epoch 760/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7362 - val_loss: 0.5643 - val_accuracy: 0.7334\n",
      "Epoch 761/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7366 - val_loss: 0.5629 - val_accuracy: 0.7323\n",
      "Epoch 762/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7362 - val_loss: 0.5618 - val_accuracy: 0.7349\n",
      "Epoch 763/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7361 - val_loss: 0.5621 - val_accuracy: 0.7336\n",
      "Epoch 764/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7368 - val_loss: 0.5625 - val_accuracy: 0.7349\n",
      "Epoch 765/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7354 - val_loss: 0.5636 - val_accuracy: 0.7339\n",
      "Epoch 766/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7361 - val_loss: 0.5621 - val_accuracy: 0.7334\n",
      "Epoch 767/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7358 - val_loss: 0.5636 - val_accuracy: 0.7331\n",
      "Epoch 768/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7363 - val_loss: 0.5625 - val_accuracy: 0.7331\n",
      "Epoch 769/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7360 - val_loss: 0.5648 - val_accuracy: 0.7339\n",
      "Epoch 770/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7362 - val_loss: 0.5625 - val_accuracy: 0.7341\n",
      "Epoch 771/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7344 - val_loss: 0.5621 - val_accuracy: 0.7354\n",
      "Epoch 772/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7361 - val_loss: 0.5611 - val_accuracy: 0.7339\n",
      "Epoch 773/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7365 - val_loss: 0.5686 - val_accuracy: 0.7328\n",
      "Epoch 774/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7355 - val_loss: 0.5622 - val_accuracy: 0.7336\n",
      "Epoch 775/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7352 - val_loss: 0.5644 - val_accuracy: 0.7346\n",
      "Epoch 776/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7345 - val_loss: 0.5607 - val_accuracy: 0.7346\n",
      "Epoch 777/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7366 - val_loss: 0.5624 - val_accuracy: 0.7349\n",
      "Epoch 778/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7364 - val_loss: 0.5625 - val_accuracy: 0.7359\n",
      "Epoch 779/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7357 - val_loss: 0.5624 - val_accuracy: 0.7354\n",
      "Epoch 780/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7357 - val_loss: 0.5608 - val_accuracy: 0.7336\n",
      "Epoch 781/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7352 - val_loss: 0.5626 - val_accuracy: 0.7349\n",
      "Epoch 782/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7364 - val_loss: 0.5624 - val_accuracy: 0.7359\n",
      "Epoch 783/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7350 - val_loss: 0.5633 - val_accuracy: 0.7341\n",
      "Epoch 784/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7368 - val_loss: 0.5621 - val_accuracy: 0.7346\n",
      "Epoch 785/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7363 - val_loss: 0.5630 - val_accuracy: 0.7346\n",
      "Epoch 786/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7360 - val_loss: 0.5617 - val_accuracy: 0.7346\n",
      "Epoch 787/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7371 - val_loss: 0.5628 - val_accuracy: 0.7357\n",
      "Epoch 788/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7367 - val_loss: 0.5641 - val_accuracy: 0.7352\n",
      "Epoch 789/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7367 - val_loss: 0.5610 - val_accuracy: 0.7359\n",
      "Epoch 790/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7360 - val_loss: 0.5625 - val_accuracy: 0.7336\n",
      "Epoch 791/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7360 - val_loss: 0.5615 - val_accuracy: 0.7315\n",
      "Epoch 792/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7364 - val_loss: 0.5648 - val_accuracy: 0.7336\n",
      "Epoch 793/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7369 - val_loss: 0.5633 - val_accuracy: 0.7346\n",
      "Epoch 794/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7357 - val_loss: 0.5645 - val_accuracy: 0.7354\n",
      "Epoch 795/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7353 - val_loss: 0.5635 - val_accuracy: 0.7344\n",
      "Epoch 796/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7366 - val_loss: 0.5635 - val_accuracy: 0.7341\n",
      "Epoch 797/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7367 - val_loss: 0.5619 - val_accuracy: 0.7331\n",
      "Epoch 798/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7361 - val_loss: 0.5643 - val_accuracy: 0.7328\n",
      "Epoch 799/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7364 - val_loss: 0.5644 - val_accuracy: 0.7346\n",
      "Epoch 800/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7359 - val_loss: 0.5636 - val_accuracy: 0.7341\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,validation_split=0.15, epochs=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 0s - loss: 0.5561 - accuracy: 0.7362 - 404ms/epoch - 2ms/step\n",
      "Loss: 0.5560988783836365, Accuracy: 0.7362099289894104\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gus Bustillos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export our model to HDF5 file\n",
    "# .h5 files are considered legacy and the library suggested to save it as .keras, saved both formats. \n",
    "nn.save(\"./modelHDF5.h5\")\n",
    "nn.save(\"./modelKERAS.keras\")\n",
    "plot_df = pd.DataFrame(fit_model.history, index = range(1, len(fit_model.history['loss'])+1))\n",
    "plot_df.plot(y = 'accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
