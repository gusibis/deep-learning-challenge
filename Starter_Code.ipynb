{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN                          NAME APPLICATION_TYPE  AFFILIATION  \\\n",
       "0  10520599  BLUE KNIGHTS MOTORCYCLE CLUB              T10  Independent   \n",
       "\n",
       "  CLASSIFICATION    USE_CASE ORGANIZATION  STATUS INCOME_AMT  \\\n",
       "0          C1000  ProductDev  Association       1          0   \n",
       "\n",
       "  SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0                      N     5000              1  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "\n",
    "if not os.path.isfile('./charity_data.csv'):\n",
    "    application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
    "    application_df.to_csv('./charity_data.csv', encoding='utf-8')\n",
    "else:\n",
    "    application_df = pd.read_csv('./charity_data.csv')\n",
    "    application_df = application_df.loc[:, ~application_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "application_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  APPLICATION_TYPE  AFFILIATION CLASSIFICATION    USE_CASE ORGANIZATION  \\\n",
       "0              T10  Independent          C1000  ProductDev  Association   \n",
       "\n",
       "   STATUS INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0       1          0                      N     5000              1  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df.drop(['EIN', 'NAME'],  axis=1, inplace=True)\n",
    "# application_df = application_df.loc[:, ~application_df.columns.str.contains('^Unnamed')]\n",
    "application_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_TYPE            17\n",
       "AFFILIATION                  6\n",
       "CLASSIFICATION              71\n",
       "USE_CASE                     5\n",
       "ORGANIZATION                 4\n",
       "STATUS                       2\n",
       "INCOME_AMT                   9\n",
       "SPECIAL_CONSIDERATIONS       2\n",
       "ASK_AMT                   8747\n",
       "IS_SUCCESSFUL                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "application_df.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3     27037\n",
       "T4      1542\n",
       "T6      1216\n",
       "T5      1173\n",
       "T19     1065\n",
       "T8       737\n",
       "T7       725\n",
       "T10      528\n",
       "T9       156\n",
       "T13       66\n",
       "T12       27\n",
       "T2        16\n",
       "T25        3\n",
       "T14        3\n",
       "T29        2\n",
       "T15        2\n",
       "T17        1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "application_type_counts = application_df['APPLICATION_TYPE'].value_counts()\n",
    "application_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3       27037\n",
       "Other     2266\n",
       "T4        1542\n",
       "T6        1216\n",
       "T5        1173\n",
       "T19       1065\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "application_types_to_replace = list(application_type_counts[application_type_counts < 750].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "application_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "         ...  \n",
       "C4120        1\n",
       "C8210        1\n",
       "C2561        1\n",
       "C4500        1\n",
       "C2150        1\n",
       "Name: CLASSIFICATION, Length: 71, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "classification_val_counts = application_df['CLASSIFICATION'].value_counts()\n",
    "classification_val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "C7000      777\n",
       "C1700      287\n",
       "C4000      194\n",
       "C5000      116\n",
       "C1270      114\n",
       "C2700      104\n",
       "C2800       95\n",
       "C7100       75\n",
       "C1300       58\n",
       "C1280       50\n",
       "C1230       36\n",
       "C1400       34\n",
       "C7200       32\n",
       "C2300       32\n",
       "C1240       30\n",
       "C8000       20\n",
       "C7120       18\n",
       "C1500       16\n",
       "C1800       15\n",
       "C6000       15\n",
       "C1250       14\n",
       "C8200       11\n",
       "C1238       10\n",
       "C1278       10\n",
       "C1235        9\n",
       "C1237        9\n",
       "C7210        7\n",
       "C2400        6\n",
       "C1720        6\n",
       "C4100        6\n",
       "C1257        5\n",
       "C1600        5\n",
       "C1260        3\n",
       "C2710        3\n",
       "C0           3\n",
       "C3200        2\n",
       "C1234        2\n",
       "C1246        2\n",
       "C1267        2\n",
       "C1256        2\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
    "greater_than_one = classification_val_counts[classification_val_counts>1]\n",
    "greater_than_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "Other     1484\n",
       "C7000      777\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "classifications_to_replace = list (classification_val_counts[classification_val_counts<750].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "      <th>APPLICATION_TYPE_Other</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T3</th>\n",
       "      <th>APPLICATION_TYPE_T4</th>\n",
       "      <th>APPLICATION_TYPE_T5</th>\n",
       "      <th>APPLICATION_TYPE_T6</th>\n",
       "      <th>AFFILIATION_CompanySponsored</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATUS  ASK_AMT  IS_SUCCESSFUL  APPLICATION_TYPE_Other  \\\n",
       "0       1     5000              1                     1.0   \n",
       "1       1   108590              1                     0.0   \n",
       "2       1     5000              0                     0.0   \n",
       "3       1     6692              1                     0.0   \n",
       "4       1   142590              1                     0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T3  APPLICATION_TYPE_T4  \\\n",
       "0                   0.0                  0.0                  0.0   \n",
       "1                   0.0                  1.0                  0.0   \n",
       "2                   0.0                  0.0                  0.0   \n",
       "3                   0.0                  1.0                  0.0   \n",
       "4                   0.0                  1.0                  0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T5  APPLICATION_TYPE_T6  AFFILIATION_CompanySponsored  \\\n",
       "0                  0.0                  0.0                           0.0   \n",
       "1                  0.0                  0.0                           0.0   \n",
       "2                  1.0                  0.0                           1.0   \n",
       "3                  0.0                  0.0                           1.0   \n",
       "4                  0.0                  0.0                           0.0   \n",
       "\n",
       "   ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n",
       "0  ...                0.0                     0.0                       0.0   \n",
       "1  ...                1.0                     0.0                       0.0   \n",
       "2  ...                0.0                     0.0                       0.0   \n",
       "3  ...                0.0                     1.0                       0.0   \n",
       "4  ...                0.0                     0.0                       1.0   \n",
       "\n",
       "   INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n",
       "0                 0.0               0.0                     0.0   \n",
       "1                 0.0               0.0                     0.0   \n",
       "2                 0.0               0.0                     0.0   \n",
       "3                 0.0               0.0                     0.0   \n",
       "4                 0.0               0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n",
       "0              0.0                0.0                       1.0   \n",
       "1              0.0                0.0                       1.0   \n",
       "2              0.0                0.0                       1.0   \n",
       "3              0.0                0.0                       1.0   \n",
       "4              0.0                0.0                       1.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "application_df = pd.get_dummies(application_df,dtype=float)\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gus Bustillos\\AppData\\Local\\Temp\\ipykernel_19612\\1048604523.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = application_df.drop(['IS_SUCCESSFUL'],1).values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.000000e+00, 2.631396e+06, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       ...,\n",
       "       [1.000000e+00, 1.443006e+06, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 5.000000e+03, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = application_df['IS_SUCCESSFUL'].values\n",
    "X = application_df.drop(['IS_SUCCESSFUL'],1).values\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 12)                504       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 24)                312       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 36)                900       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 37        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1753 (6.85 KB)\n",
      "Trainable params: 1753 (6.85 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "features = len( X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 12\n",
    "hidden_nodes_layer2 = 24\n",
    "# hidden_nodes_layer3 = 36 #Loss: 0.5824776291847229, Accuracy: 0.7370262145996094\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=features, activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, input_dim=features, activation='relu'))\n",
    "\n",
    "# third hidden \n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, input_dim=features, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "684/684 [==============================] - 4s 3ms/step - loss: 0.5940 - accuracy: 0.6997 - val_loss: 0.5604 - val_accuracy: 0.7313\n",
      "Epoch 2/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5688 - accuracy: 0.7226 - val_loss: 0.5492 - val_accuracy: 0.7409\n",
      "Epoch 3/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5643 - accuracy: 0.7242 - val_loss: 0.5473 - val_accuracy: 0.7388\n",
      "Epoch 4/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5624 - accuracy: 0.7249 - val_loss: 0.5494 - val_accuracy: 0.7385\n",
      "Epoch 5/800\n",
      "684/684 [==============================] - 2s 4ms/step - loss: 0.5603 - accuracy: 0.7266 - val_loss: 0.5459 - val_accuracy: 0.7416\n",
      "Epoch 6/800\n",
      "684/684 [==============================] - 2s 4ms/step - loss: 0.5590 - accuracy: 0.7246 - val_loss: 0.5428 - val_accuracy: 0.7383\n",
      "Epoch 7/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5582 - accuracy: 0.7248 - val_loss: 0.5459 - val_accuracy: 0.7383\n",
      "Epoch 8/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5580 - accuracy: 0.7269 - val_loss: 0.5463 - val_accuracy: 0.7396\n",
      "Epoch 9/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5568 - accuracy: 0.7260 - val_loss: 0.5437 - val_accuracy: 0.7409\n",
      "Epoch 10/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5565 - accuracy: 0.7275 - val_loss: 0.5496 - val_accuracy: 0.7385\n",
      "Epoch 11/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5556 - accuracy: 0.7270 - val_loss: 0.5484 - val_accuracy: 0.7396\n",
      "Epoch 12/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5550 - accuracy: 0.7271 - val_loss: 0.5436 - val_accuracy: 0.7401\n",
      "Epoch 13/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5548 - accuracy: 0.7273 - val_loss: 0.5435 - val_accuracy: 0.7401\n",
      "Epoch 14/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5548 - accuracy: 0.7277 - val_loss: 0.5440 - val_accuracy: 0.7406\n",
      "Epoch 15/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5540 - accuracy: 0.7298 - val_loss: 0.5457 - val_accuracy: 0.7409\n",
      "Epoch 16/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5536 - accuracy: 0.7288 - val_loss: 0.5445 - val_accuracy: 0.7398\n",
      "Epoch 17/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5533 - accuracy: 0.7285 - val_loss: 0.5435 - val_accuracy: 0.7396\n",
      "Epoch 18/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5530 - accuracy: 0.7288 - val_loss: 0.5449 - val_accuracy: 0.7406\n",
      "Epoch 19/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5527 - accuracy: 0.7297 - val_loss: 0.5441 - val_accuracy: 0.7406\n",
      "Epoch 20/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5521 - accuracy: 0.7286 - val_loss: 0.5477 - val_accuracy: 0.7372\n",
      "Epoch 21/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5522 - accuracy: 0.7303 - val_loss: 0.5459 - val_accuracy: 0.7344\n",
      "Epoch 22/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5519 - accuracy: 0.7285 - val_loss: 0.5427 - val_accuracy: 0.7393\n",
      "Epoch 23/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5515 - accuracy: 0.7290 - val_loss: 0.5477 - val_accuracy: 0.7396\n",
      "Epoch 24/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5513 - accuracy: 0.7298 - val_loss: 0.5460 - val_accuracy: 0.7398\n",
      "Epoch 25/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5513 - accuracy: 0.7293 - val_loss: 0.5467 - val_accuracy: 0.7391\n",
      "Epoch 26/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5511 - accuracy: 0.7313 - val_loss: 0.5444 - val_accuracy: 0.7403\n",
      "Epoch 27/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5506 - accuracy: 0.7303 - val_loss: 0.5446 - val_accuracy: 0.7396\n",
      "Epoch 28/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5508 - accuracy: 0.7306 - val_loss: 0.5480 - val_accuracy: 0.7391\n",
      "Epoch 29/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5501 - accuracy: 0.7300 - val_loss: 0.5452 - val_accuracy: 0.7388\n",
      "Epoch 30/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5504 - accuracy: 0.7314 - val_loss: 0.5474 - val_accuracy: 0.7375\n",
      "Epoch 31/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5502 - accuracy: 0.7308 - val_loss: 0.5479 - val_accuracy: 0.7398\n",
      "Epoch 32/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5495 - accuracy: 0.7309 - val_loss: 0.5473 - val_accuracy: 0.7401\n",
      "Epoch 33/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5499 - accuracy: 0.7314 - val_loss: 0.5453 - val_accuracy: 0.7403\n",
      "Epoch 34/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5497 - accuracy: 0.7311 - val_loss: 0.5460 - val_accuracy: 0.7398\n",
      "Epoch 35/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5490 - accuracy: 0.7310 - val_loss: 0.5505 - val_accuracy: 0.7391\n",
      "Epoch 36/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5492 - accuracy: 0.7313 - val_loss: 0.5472 - val_accuracy: 0.7391\n",
      "Epoch 37/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5489 - accuracy: 0.7309 - val_loss: 0.5485 - val_accuracy: 0.7352\n",
      "Epoch 38/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5492 - accuracy: 0.7312 - val_loss: 0.5474 - val_accuracy: 0.7357\n",
      "Epoch 39/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5487 - accuracy: 0.7316 - val_loss: 0.5492 - val_accuracy: 0.7398\n",
      "Epoch 40/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5493 - accuracy: 0.7321 - val_loss: 0.5480 - val_accuracy: 0.7398\n",
      "Epoch 41/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5486 - accuracy: 0.7318 - val_loss: 0.5477 - val_accuracy: 0.7393\n",
      "Epoch 42/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5490 - accuracy: 0.7315 - val_loss: 0.5500 - val_accuracy: 0.7385\n",
      "Epoch 43/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5480 - accuracy: 0.7315 - val_loss: 0.5463 - val_accuracy: 0.7391\n",
      "Epoch 44/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5490 - accuracy: 0.7304 - val_loss: 0.5484 - val_accuracy: 0.7380\n",
      "Epoch 45/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5482 - accuracy: 0.7315 - val_loss: 0.5486 - val_accuracy: 0.7393\n",
      "Epoch 46/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5485 - accuracy: 0.7313 - val_loss: 0.5453 - val_accuracy: 0.7398\n",
      "Epoch 47/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5480 - accuracy: 0.7306 - val_loss: 0.5509 - val_accuracy: 0.7383\n",
      "Epoch 48/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5476 - accuracy: 0.7322 - val_loss: 0.5476 - val_accuracy: 0.7401\n",
      "Epoch 49/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5481 - accuracy: 0.7316 - val_loss: 0.5467 - val_accuracy: 0.7393\n",
      "Epoch 50/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5482 - accuracy: 0.7322 - val_loss: 0.5479 - val_accuracy: 0.7383\n",
      "Epoch 51/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5476 - accuracy: 0.7322 - val_loss: 0.5497 - val_accuracy: 0.7385\n",
      "Epoch 52/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5473 - accuracy: 0.7328 - val_loss: 0.5473 - val_accuracy: 0.7393\n",
      "Epoch 53/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5477 - accuracy: 0.7315 - val_loss: 0.5480 - val_accuracy: 0.7352\n",
      "Epoch 54/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5475 - accuracy: 0.7317 - val_loss: 0.5467 - val_accuracy: 0.7391\n",
      "Epoch 55/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5474 - accuracy: 0.7324 - val_loss: 0.5469 - val_accuracy: 0.7388\n",
      "Epoch 56/800\n",
      "684/684 [==============================] - 2s 4ms/step - loss: 0.5476 - accuracy: 0.7320 - val_loss: 0.5481 - val_accuracy: 0.7396\n",
      "Epoch 57/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5470 - accuracy: 0.7324 - val_loss: 0.5456 - val_accuracy: 0.7406\n",
      "Epoch 58/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5472 - accuracy: 0.7328 - val_loss: 0.5490 - val_accuracy: 0.7357\n",
      "Epoch 59/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5473 - accuracy: 0.7330 - val_loss: 0.5491 - val_accuracy: 0.7375\n",
      "Epoch 60/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5473 - accuracy: 0.7322 - val_loss: 0.5483 - val_accuracy: 0.7388\n",
      "Epoch 61/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5472 - accuracy: 0.7320 - val_loss: 0.5471 - val_accuracy: 0.7370\n",
      "Epoch 62/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5466 - accuracy: 0.7333 - val_loss: 0.5487 - val_accuracy: 0.7396\n",
      "Epoch 63/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5469 - accuracy: 0.7325 - val_loss: 0.5468 - val_accuracy: 0.7375\n",
      "Epoch 64/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5463 - accuracy: 0.7329 - val_loss: 0.5515 - val_accuracy: 0.7391\n",
      "Epoch 65/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5473 - accuracy: 0.7326 - val_loss: 0.5497 - val_accuracy: 0.7396\n",
      "Epoch 66/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5466 - accuracy: 0.7327 - val_loss: 0.5485 - val_accuracy: 0.7393\n",
      "Epoch 67/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5467 - accuracy: 0.7326 - val_loss: 0.5483 - val_accuracy: 0.7380\n",
      "Epoch 68/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5468 - accuracy: 0.7331 - val_loss: 0.5488 - val_accuracy: 0.7398\n",
      "Epoch 69/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5465 - accuracy: 0.7327 - val_loss: 0.5514 - val_accuracy: 0.7372\n",
      "Epoch 70/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5464 - accuracy: 0.7324 - val_loss: 0.5505 - val_accuracy: 0.7349\n",
      "Epoch 71/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5463 - accuracy: 0.7327 - val_loss: 0.5513 - val_accuracy: 0.7365\n",
      "Epoch 72/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5465 - accuracy: 0.7329 - val_loss: 0.5481 - val_accuracy: 0.7372\n",
      "Epoch 73/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5461 - accuracy: 0.7331 - val_loss: 0.5521 - val_accuracy: 0.7372\n",
      "Epoch 74/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5460 - accuracy: 0.7325 - val_loss: 0.5498 - val_accuracy: 0.7383\n",
      "Epoch 75/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5460 - accuracy: 0.7337 - val_loss: 0.5490 - val_accuracy: 0.7391\n",
      "Epoch 76/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5460 - accuracy: 0.7331 - val_loss: 0.5509 - val_accuracy: 0.7375\n",
      "Epoch 77/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5459 - accuracy: 0.7328 - val_loss: 0.5525 - val_accuracy: 0.7383\n",
      "Epoch 78/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5464 - accuracy: 0.7327 - val_loss: 0.5486 - val_accuracy: 0.7380\n",
      "Epoch 79/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5457 - accuracy: 0.7329 - val_loss: 0.5484 - val_accuracy: 0.7385\n",
      "Epoch 80/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5456 - accuracy: 0.7335 - val_loss: 0.5486 - val_accuracy: 0.7385\n",
      "Epoch 81/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5455 - accuracy: 0.7334 - val_loss: 0.5487 - val_accuracy: 0.7378\n",
      "Epoch 82/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5461 - accuracy: 0.7339 - val_loss: 0.5494 - val_accuracy: 0.7370\n",
      "Epoch 83/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5455 - accuracy: 0.7335 - val_loss: 0.5501 - val_accuracy: 0.7378\n",
      "Epoch 84/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5457 - accuracy: 0.7340 - val_loss: 0.5480 - val_accuracy: 0.7378\n",
      "Epoch 85/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7332 - val_loss: 0.5496 - val_accuracy: 0.7388\n",
      "Epoch 86/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.7330 - val_loss: 0.5503 - val_accuracy: 0.7388\n",
      "Epoch 87/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5453 - accuracy: 0.7337 - val_loss: 0.5541 - val_accuracy: 0.7380\n",
      "Epoch 88/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5456 - accuracy: 0.7336 - val_loss: 0.5489 - val_accuracy: 0.7380\n",
      "Epoch 89/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7335 - val_loss: 0.5492 - val_accuracy: 0.7388\n",
      "Epoch 90/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5454 - accuracy: 0.7331 - val_loss: 0.5498 - val_accuracy: 0.7383\n",
      "Epoch 91/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7336 - val_loss: 0.5500 - val_accuracy: 0.7367\n",
      "Epoch 92/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5452 - accuracy: 0.7340 - val_loss: 0.5533 - val_accuracy: 0.7378\n",
      "Epoch 93/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5451 - accuracy: 0.7340 - val_loss: 0.5485 - val_accuracy: 0.7378\n",
      "Epoch 94/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5449 - accuracy: 0.7330 - val_loss: 0.5498 - val_accuracy: 0.7378\n",
      "Epoch 95/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5441 - accuracy: 0.7337 - val_loss: 0.5501 - val_accuracy: 0.7339\n",
      "Epoch 96/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5446 - accuracy: 0.7349 - val_loss: 0.5488 - val_accuracy: 0.7391\n",
      "Epoch 97/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5450 - accuracy: 0.7328 - val_loss: 0.5497 - val_accuracy: 0.7370\n",
      "Epoch 98/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5448 - accuracy: 0.7338 - val_loss: 0.5506 - val_accuracy: 0.7357\n",
      "Epoch 99/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5450 - accuracy: 0.7343 - val_loss: 0.5515 - val_accuracy: 0.7383\n",
      "Epoch 100/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5450 - accuracy: 0.7341 - val_loss: 0.5511 - val_accuracy: 0.7385\n",
      "Epoch 101/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5448 - accuracy: 0.7330 - val_loss: 0.5502 - val_accuracy: 0.7367\n",
      "Epoch 102/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5445 - accuracy: 0.7335 - val_loss: 0.5503 - val_accuracy: 0.7388\n",
      "Epoch 103/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5446 - accuracy: 0.7338 - val_loss: 0.5506 - val_accuracy: 0.7388\n",
      "Epoch 104/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5445 - accuracy: 0.7340 - val_loss: 0.5520 - val_accuracy: 0.7391\n",
      "Epoch 105/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5447 - accuracy: 0.7344 - val_loss: 0.5515 - val_accuracy: 0.7372\n",
      "Epoch 106/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5447 - accuracy: 0.7339 - val_loss: 0.5539 - val_accuracy: 0.7359\n",
      "Epoch 107/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5446 - accuracy: 0.7347 - val_loss: 0.5516 - val_accuracy: 0.7367\n",
      "Epoch 108/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5443 - accuracy: 0.7348 - val_loss: 0.5509 - val_accuracy: 0.7380\n",
      "Epoch 109/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5448 - accuracy: 0.7339 - val_loss: 0.5536 - val_accuracy: 0.7352\n",
      "Epoch 110/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5444 - accuracy: 0.7341 - val_loss: 0.5544 - val_accuracy: 0.7375\n",
      "Epoch 111/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5442 - accuracy: 0.7343 - val_loss: 0.5516 - val_accuracy: 0.7370\n",
      "Epoch 112/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5442 - accuracy: 0.7347 - val_loss: 0.5513 - val_accuracy: 0.7359\n",
      "Epoch 113/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5444 - accuracy: 0.7348 - val_loss: 0.5543 - val_accuracy: 0.7372\n",
      "Epoch 114/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5439 - accuracy: 0.7352 - val_loss: 0.5544 - val_accuracy: 0.7375\n",
      "Epoch 115/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5441 - accuracy: 0.7335 - val_loss: 0.5556 - val_accuracy: 0.7352\n",
      "Epoch 116/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5447 - accuracy: 0.7343 - val_loss: 0.5533 - val_accuracy: 0.7372\n",
      "Epoch 117/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5437 - accuracy: 0.7345 - val_loss: 0.5560 - val_accuracy: 0.7378\n",
      "Epoch 118/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5445 - accuracy: 0.7341 - val_loss: 0.5527 - val_accuracy: 0.7370\n",
      "Epoch 119/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5441 - accuracy: 0.7333 - val_loss: 0.5520 - val_accuracy: 0.7362\n",
      "Epoch 120/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5442 - accuracy: 0.7335 - val_loss: 0.5520 - val_accuracy: 0.7365\n",
      "Epoch 121/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5446 - accuracy: 0.7339 - val_loss: 0.5554 - val_accuracy: 0.7383\n",
      "Epoch 122/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5440 - accuracy: 0.7346 - val_loss: 0.5536 - val_accuracy: 0.7383\n",
      "Epoch 123/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5440 - accuracy: 0.7343 - val_loss: 0.5540 - val_accuracy: 0.7372\n",
      "Epoch 124/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5437 - accuracy: 0.7341 - val_loss: 0.5537 - val_accuracy: 0.7370\n",
      "Epoch 125/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5438 - accuracy: 0.7336 - val_loss: 0.5534 - val_accuracy: 0.7349\n",
      "Epoch 126/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5437 - accuracy: 0.7351 - val_loss: 0.5531 - val_accuracy: 0.7372\n",
      "Epoch 127/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5438 - accuracy: 0.7339 - val_loss: 0.5568 - val_accuracy: 0.7370\n",
      "Epoch 128/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5442 - accuracy: 0.7344 - val_loss: 0.5555 - val_accuracy: 0.7370\n",
      "Epoch 129/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5439 - accuracy: 0.7345 - val_loss: 0.5536 - val_accuracy: 0.7367\n",
      "Epoch 130/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5441 - accuracy: 0.7339 - val_loss: 0.5570 - val_accuracy: 0.7354\n",
      "Epoch 131/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7348 - val_loss: 0.5555 - val_accuracy: 0.7326\n",
      "Epoch 132/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5438 - accuracy: 0.7345 - val_loss: 0.5555 - val_accuracy: 0.7357\n",
      "Epoch 133/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5437 - accuracy: 0.7344 - val_loss: 0.5549 - val_accuracy: 0.7367\n",
      "Epoch 134/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5437 - accuracy: 0.7350 - val_loss: 0.5567 - val_accuracy: 0.7339\n",
      "Epoch 135/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5436 - accuracy: 0.7352 - val_loss: 0.5553 - val_accuracy: 0.7357\n",
      "Epoch 136/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5437 - accuracy: 0.7344 - val_loss: 0.5545 - val_accuracy: 0.7359\n",
      "Epoch 137/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5435 - accuracy: 0.7348 - val_loss: 0.5581 - val_accuracy: 0.7334\n",
      "Epoch 138/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5435 - accuracy: 0.7345 - val_loss: 0.5565 - val_accuracy: 0.7365\n",
      "Epoch 139/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5435 - accuracy: 0.7351 - val_loss: 0.5566 - val_accuracy: 0.7354\n",
      "Epoch 140/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7345 - val_loss: 0.5547 - val_accuracy: 0.7354\n",
      "Epoch 141/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5439 - accuracy: 0.7348 - val_loss: 0.5565 - val_accuracy: 0.7372\n",
      "Epoch 142/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5437 - accuracy: 0.7346 - val_loss: 0.5549 - val_accuracy: 0.7359\n",
      "Epoch 143/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7348 - val_loss: 0.5572 - val_accuracy: 0.7354\n",
      "Epoch 144/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5437 - accuracy: 0.7350 - val_loss: 0.5593 - val_accuracy: 0.7354\n",
      "Epoch 145/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7349 - val_loss: 0.5573 - val_accuracy: 0.7354\n",
      "Epoch 146/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7341 - val_loss: 0.5569 - val_accuracy: 0.7359\n",
      "Epoch 147/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7350 - val_loss: 0.5559 - val_accuracy: 0.7357\n",
      "Epoch 148/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7348 - val_loss: 0.5573 - val_accuracy: 0.7372\n",
      "Epoch 149/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7347 - val_loss: 0.5563 - val_accuracy: 0.7370\n",
      "Epoch 150/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7346 - val_loss: 0.5569 - val_accuracy: 0.7334\n",
      "Epoch 151/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7348 - val_loss: 0.5573 - val_accuracy: 0.7367\n",
      "Epoch 152/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5429 - accuracy: 0.7351 - val_loss: 0.5579 - val_accuracy: 0.7383\n",
      "Epoch 153/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7345 - val_loss: 0.5580 - val_accuracy: 0.7372\n",
      "Epoch 154/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7345 - val_loss: 0.5543 - val_accuracy: 0.7380\n",
      "Epoch 155/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7353 - val_loss: 0.5569 - val_accuracy: 0.7354\n",
      "Epoch 156/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7346 - val_loss: 0.5600 - val_accuracy: 0.7357\n",
      "Epoch 157/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5428 - accuracy: 0.7344 - val_loss: 0.5555 - val_accuracy: 0.7365\n",
      "Epoch 158/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5432 - accuracy: 0.7347 - val_loss: 0.5596 - val_accuracy: 0.7365\n",
      "Epoch 159/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5430 - accuracy: 0.7352 - val_loss: 0.5580 - val_accuracy: 0.7367\n",
      "Epoch 160/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7348 - val_loss: 0.5582 - val_accuracy: 0.7367\n",
      "Epoch 161/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7355 - val_loss: 0.5581 - val_accuracy: 0.7367\n",
      "Epoch 162/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7347 - val_loss: 0.5564 - val_accuracy: 0.7365\n",
      "Epoch 163/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7345 - val_loss: 0.5577 - val_accuracy: 0.7365\n",
      "Epoch 164/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7348 - val_loss: 0.5593 - val_accuracy: 0.7359\n",
      "Epoch 165/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7350 - val_loss: 0.5584 - val_accuracy: 0.7352\n",
      "Epoch 166/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7346 - val_loss: 0.5597 - val_accuracy: 0.7365\n",
      "Epoch 167/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7349 - val_loss: 0.5579 - val_accuracy: 0.7367\n",
      "Epoch 168/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7339 - val_loss: 0.5580 - val_accuracy: 0.7362\n",
      "Epoch 169/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7348 - val_loss: 0.5551 - val_accuracy: 0.7372\n",
      "Epoch 170/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5428 - accuracy: 0.7347 - val_loss: 0.5589 - val_accuracy: 0.7359\n",
      "Epoch 171/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7346 - val_loss: 0.5594 - val_accuracy: 0.7367\n",
      "Epoch 172/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5425 - accuracy: 0.7355 - val_loss: 0.5556 - val_accuracy: 0.7336\n",
      "Epoch 173/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7352 - val_loss: 0.5555 - val_accuracy: 0.7367\n",
      "Epoch 174/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5429 - accuracy: 0.7351 - val_loss: 0.5601 - val_accuracy: 0.7372\n",
      "Epoch 175/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7355 - val_loss: 0.5582 - val_accuracy: 0.7362\n",
      "Epoch 176/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5427 - accuracy: 0.7351 - val_loss: 0.5567 - val_accuracy: 0.7362\n",
      "Epoch 177/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7345 - val_loss: 0.5572 - val_accuracy: 0.7359\n",
      "Epoch 178/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7346 - val_loss: 0.5585 - val_accuracy: 0.7357\n",
      "Epoch 179/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7357 - val_loss: 0.5603 - val_accuracy: 0.7372\n",
      "Epoch 180/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7357 - val_loss: 0.5579 - val_accuracy: 0.7378\n",
      "Epoch 181/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7351 - val_loss: 0.5606 - val_accuracy: 0.7365\n",
      "Epoch 182/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7347 - val_loss: 0.5606 - val_accuracy: 0.7352\n",
      "Epoch 183/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7350 - val_loss: 0.5586 - val_accuracy: 0.7367\n",
      "Epoch 184/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7350 - val_loss: 0.5606 - val_accuracy: 0.7354\n",
      "Epoch 185/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7358 - val_loss: 0.5580 - val_accuracy: 0.7372\n",
      "Epoch 186/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7348 - val_loss: 0.5594 - val_accuracy: 0.7359\n",
      "Epoch 187/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7355 - val_loss: 0.5583 - val_accuracy: 0.7367\n",
      "Epoch 188/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7358 - val_loss: 0.5593 - val_accuracy: 0.7367\n",
      "Epoch 189/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7347 - val_loss: 0.5610 - val_accuracy: 0.7352\n",
      "Epoch 190/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7357 - val_loss: 0.5594 - val_accuracy: 0.7367\n",
      "Epoch 191/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.7354 - val_loss: 0.5586 - val_accuracy: 0.7365\n",
      "Epoch 192/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7357 - val_loss: 0.5660 - val_accuracy: 0.7365\n",
      "Epoch 193/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7348 - val_loss: 0.5612 - val_accuracy: 0.7370\n",
      "Epoch 194/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7348 - val_loss: 0.5603 - val_accuracy: 0.7357\n",
      "Epoch 195/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7357 - val_loss: 0.5629 - val_accuracy: 0.7367\n",
      "Epoch 196/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7355 - val_loss: 0.5623 - val_accuracy: 0.7362\n",
      "Epoch 197/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7354 - val_loss: 0.5604 - val_accuracy: 0.7359\n",
      "Epoch 198/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7352 - val_loss: 0.5599 - val_accuracy: 0.7370\n",
      "Epoch 199/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7354 - val_loss: 0.5623 - val_accuracy: 0.7357\n",
      "Epoch 200/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7352 - val_loss: 0.5592 - val_accuracy: 0.7372\n",
      "Epoch 201/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.7354 - val_loss: 0.5612 - val_accuracy: 0.7365\n",
      "Epoch 202/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7343 - val_loss: 0.5624 - val_accuracy: 0.7378\n",
      "Epoch 203/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7352 - val_loss: 0.5610 - val_accuracy: 0.7362\n",
      "Epoch 204/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7350 - val_loss: 0.5617 - val_accuracy: 0.7357\n",
      "Epoch 205/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7352 - val_loss: 0.5621 - val_accuracy: 0.7372\n",
      "Epoch 206/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7358 - val_loss: 0.5611 - val_accuracy: 0.7357\n",
      "Epoch 207/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7349 - val_loss: 0.5587 - val_accuracy: 0.7362\n",
      "Epoch 208/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7355 - val_loss: 0.5641 - val_accuracy: 0.7370\n",
      "Epoch 209/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7352 - val_loss: 0.5627 - val_accuracy: 0.7359\n",
      "Epoch 210/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7353 - val_loss: 0.5636 - val_accuracy: 0.7375\n",
      "Epoch 211/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7348 - val_loss: 0.5603 - val_accuracy: 0.7375\n",
      "Epoch 212/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7357 - val_loss: 0.5620 - val_accuracy: 0.7359\n",
      "Epoch 213/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5424 - accuracy: 0.7360 - val_loss: 0.5611 - val_accuracy: 0.7352\n",
      "Epoch 214/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7353 - val_loss: 0.5591 - val_accuracy: 0.7349\n",
      "Epoch 215/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7355 - val_loss: 0.5602 - val_accuracy: 0.7372\n",
      "Epoch 216/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7350 - val_loss: 0.5596 - val_accuracy: 0.7365\n",
      "Epoch 217/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7355 - val_loss: 0.5597 - val_accuracy: 0.7370\n",
      "Epoch 218/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.7352 - val_loss: 0.5583 - val_accuracy: 0.7365\n",
      "Epoch 219/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5415 - accuracy: 0.7357 - val_loss: 0.5634 - val_accuracy: 0.7365\n",
      "Epoch 220/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7348 - val_loss: 0.5632 - val_accuracy: 0.7378\n",
      "Epoch 221/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7354 - val_loss: 0.5598 - val_accuracy: 0.7375\n",
      "Epoch 222/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7355 - val_loss: 0.5558 - val_accuracy: 0.7370\n",
      "Epoch 223/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7357 - val_loss: 0.5595 - val_accuracy: 0.7367\n",
      "Epoch 224/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5421 - accuracy: 0.7350 - val_loss: 0.5594 - val_accuracy: 0.7372\n",
      "Epoch 225/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7345 - val_loss: 0.5585 - val_accuracy: 0.7367\n",
      "Epoch 226/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7349 - val_loss: 0.5607 - val_accuracy: 0.7367\n",
      "Epoch 227/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7357 - val_loss: 0.5604 - val_accuracy: 0.7331\n",
      "Epoch 228/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5419 - accuracy: 0.7357 - val_loss: 0.5601 - val_accuracy: 0.7339\n",
      "Epoch 229/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5419 - accuracy: 0.7353 - val_loss: 0.5652 - val_accuracy: 0.7365\n",
      "Epoch 230/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5420 - accuracy: 0.7361 - val_loss: 0.5605 - val_accuracy: 0.7372\n",
      "Epoch 231/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7355 - val_loss: 0.5626 - val_accuracy: 0.7367\n",
      "Epoch 232/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7357 - val_loss: 0.5619 - val_accuracy: 0.7370\n",
      "Epoch 233/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7351 - val_loss: 0.5618 - val_accuracy: 0.7354\n",
      "Epoch 234/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7357 - val_loss: 0.5609 - val_accuracy: 0.7367\n",
      "Epoch 235/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7359 - val_loss: 0.5617 - val_accuracy: 0.7367\n",
      "Epoch 236/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7348 - val_loss: 0.5632 - val_accuracy: 0.7365\n",
      "Epoch 237/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7349 - val_loss: 0.5632 - val_accuracy: 0.7359\n",
      "Epoch 238/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7352 - val_loss: 0.5613 - val_accuracy: 0.7375\n",
      "Epoch 239/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7352 - val_loss: 0.5646 - val_accuracy: 0.7365\n",
      "Epoch 240/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7355 - val_loss: 0.5639 - val_accuracy: 0.7365\n",
      "Epoch 241/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7349 - val_loss: 0.5610 - val_accuracy: 0.7365\n",
      "Epoch 242/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7354 - val_loss: 0.5645 - val_accuracy: 0.7365\n",
      "Epoch 243/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7360 - val_loss: 0.5630 - val_accuracy: 0.7362\n",
      "Epoch 244/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7350 - val_loss: 0.5645 - val_accuracy: 0.7365\n",
      "Epoch 245/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7344 - val_loss: 0.5600 - val_accuracy: 0.7367\n",
      "Epoch 246/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7353 - val_loss: 0.5628 - val_accuracy: 0.7380\n",
      "Epoch 247/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7360 - val_loss: 0.5613 - val_accuracy: 0.7362\n",
      "Epoch 248/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7351 - val_loss: 0.5631 - val_accuracy: 0.7357\n",
      "Epoch 249/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7355 - val_loss: 0.5631 - val_accuracy: 0.7359\n",
      "Epoch 250/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7353 - val_loss: 0.5621 - val_accuracy: 0.7383\n",
      "Epoch 251/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7347 - val_loss: 0.5639 - val_accuracy: 0.7380\n",
      "Epoch 252/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7360 - val_loss: 0.5609 - val_accuracy: 0.7378\n",
      "Epoch 253/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7355 - val_loss: 0.5629 - val_accuracy: 0.7365\n",
      "Epoch 254/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5417 - accuracy: 0.7354 - val_loss: 0.5642 - val_accuracy: 0.7354\n",
      "Epoch 255/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5415 - accuracy: 0.7357 - val_loss: 0.5595 - val_accuracy: 0.7359\n",
      "Epoch 256/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7365 - val_loss: 0.5616 - val_accuracy: 0.7367\n",
      "Epoch 257/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7359 - val_loss: 0.5658 - val_accuracy: 0.7370\n",
      "Epoch 258/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5414 - accuracy: 0.7356 - val_loss: 0.5622 - val_accuracy: 0.7367\n",
      "Epoch 259/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5416 - accuracy: 0.7347 - val_loss: 0.5655 - val_accuracy: 0.7365\n",
      "Epoch 260/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7353 - val_loss: 0.5657 - val_accuracy: 0.7370\n",
      "Epoch 261/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7354 - val_loss: 0.5597 - val_accuracy: 0.7357\n",
      "Epoch 262/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7363 - val_loss: 0.5615 - val_accuracy: 0.7372\n",
      "Epoch 263/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7365 - val_loss: 0.5618 - val_accuracy: 0.7365\n",
      "Epoch 264/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7353 - val_loss: 0.5606 - val_accuracy: 0.7380\n",
      "Epoch 265/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7363 - val_loss: 0.5646 - val_accuracy: 0.7370\n",
      "Epoch 266/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7353 - val_loss: 0.5648 - val_accuracy: 0.7367\n",
      "Epoch 267/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7360 - val_loss: 0.5648 - val_accuracy: 0.7359\n",
      "Epoch 268/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7357 - val_loss: 0.5623 - val_accuracy: 0.7367\n",
      "Epoch 269/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7357 - val_loss: 0.5668 - val_accuracy: 0.7370\n",
      "Epoch 270/800\n",
      "684/684 [==============================] - 2s 4ms/step - loss: 0.5415 - accuracy: 0.7355 - val_loss: 0.5620 - val_accuracy: 0.7362\n",
      "Epoch 271/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5415 - accuracy: 0.7354 - val_loss: 0.5653 - val_accuracy: 0.7372\n",
      "Epoch 272/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5412 - accuracy: 0.7358 - val_loss: 0.5656 - val_accuracy: 0.7357\n",
      "Epoch 273/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7354 - val_loss: 0.5605 - val_accuracy: 0.7354\n",
      "Epoch 274/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7353 - val_loss: 0.5658 - val_accuracy: 0.7362\n",
      "Epoch 275/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7359 - val_loss: 0.5641 - val_accuracy: 0.7367\n",
      "Epoch 276/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7352 - val_loss: 0.5627 - val_accuracy: 0.7375\n",
      "Epoch 277/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7360 - val_loss: 0.5582 - val_accuracy: 0.7362\n",
      "Epoch 278/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7348 - val_loss: 0.5616 - val_accuracy: 0.7375\n",
      "Epoch 279/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5414 - accuracy: 0.7362 - val_loss: 0.5656 - val_accuracy: 0.7362\n",
      "Epoch 280/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7361 - val_loss: 0.5620 - val_accuracy: 0.7362\n",
      "Epoch 281/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7353 - val_loss: 0.5617 - val_accuracy: 0.7367\n",
      "Epoch 282/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7351 - val_loss: 0.5672 - val_accuracy: 0.7370\n",
      "Epoch 283/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7354 - val_loss: 0.5638 - val_accuracy: 0.7367\n",
      "Epoch 284/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5414 - accuracy: 0.7357 - val_loss: 0.5655 - val_accuracy: 0.7372\n",
      "Epoch 285/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7358 - val_loss: 0.5652 - val_accuracy: 0.7375\n",
      "Epoch 286/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7357 - val_loss: 0.5617 - val_accuracy: 0.7375\n",
      "Epoch 287/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5412 - accuracy: 0.7348 - val_loss: 0.5642 - val_accuracy: 0.7378\n",
      "Epoch 288/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7358 - val_loss: 0.5625 - val_accuracy: 0.7370\n",
      "Epoch 289/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7353 - val_loss: 0.5630 - val_accuracy: 0.7365\n",
      "Epoch 290/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7360 - val_loss: 0.5608 - val_accuracy: 0.7354\n",
      "Epoch 291/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7360 - val_loss: 0.5657 - val_accuracy: 0.7372\n",
      "Epoch 292/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7361 - val_loss: 0.5684 - val_accuracy: 0.7370\n",
      "Epoch 293/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7359 - val_loss: 0.5650 - val_accuracy: 0.7380\n",
      "Epoch 294/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7358 - val_loss: 0.5652 - val_accuracy: 0.7365\n",
      "Epoch 295/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7359 - val_loss: 0.5625 - val_accuracy: 0.7372\n",
      "Epoch 296/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7361 - val_loss: 0.5655 - val_accuracy: 0.7359\n",
      "Epoch 297/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5408 - accuracy: 0.7362 - val_loss: 0.5609 - val_accuracy: 0.7370\n",
      "Epoch 298/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7357 - val_loss: 0.5651 - val_accuracy: 0.7357\n",
      "Epoch 299/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7357 - val_loss: 0.5671 - val_accuracy: 0.7370\n",
      "Epoch 300/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7356 - val_loss: 0.5674 - val_accuracy: 0.7370\n",
      "Epoch 301/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7357 - val_loss: 0.5619 - val_accuracy: 0.7365\n",
      "Epoch 302/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7360 - val_loss: 0.5673 - val_accuracy: 0.7362\n",
      "Epoch 303/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7354 - val_loss: 0.5651 - val_accuracy: 0.7362\n",
      "Epoch 304/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7363 - val_loss: 0.5652 - val_accuracy: 0.7370\n",
      "Epoch 305/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7359 - val_loss: 0.5652 - val_accuracy: 0.7367\n",
      "Epoch 306/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7357 - val_loss: 0.5627 - val_accuracy: 0.7372\n",
      "Epoch 307/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7355 - val_loss: 0.5670 - val_accuracy: 0.7380\n",
      "Epoch 308/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7356 - val_loss: 0.5660 - val_accuracy: 0.7365\n",
      "Epoch 309/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7359 - val_loss: 0.5654 - val_accuracy: 0.7370\n",
      "Epoch 310/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7361 - val_loss: 0.5670 - val_accuracy: 0.7362\n",
      "Epoch 311/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7357 - val_loss: 0.5650 - val_accuracy: 0.7359\n",
      "Epoch 312/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5408 - accuracy: 0.7360 - val_loss: 0.5667 - val_accuracy: 0.7375\n",
      "Epoch 313/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7361 - val_loss: 0.5668 - val_accuracy: 0.7362\n",
      "Epoch 314/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7359 - val_loss: 0.5683 - val_accuracy: 0.7359\n",
      "Epoch 315/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7356 - val_loss: 0.5639 - val_accuracy: 0.7380\n",
      "Epoch 316/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7357 - val_loss: 0.5653 - val_accuracy: 0.7370\n",
      "Epoch 317/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5411 - accuracy: 0.7362 - val_loss: 0.5666 - val_accuracy: 0.7372\n",
      "Epoch 318/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5410 - accuracy: 0.7359 - val_loss: 0.5670 - val_accuracy: 0.7362\n",
      "Epoch 319/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5408 - accuracy: 0.7353 - val_loss: 0.5688 - val_accuracy: 0.7357\n",
      "Epoch 320/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5409 - accuracy: 0.7361 - val_loss: 0.5667 - val_accuracy: 0.7365\n",
      "Epoch 321/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5408 - accuracy: 0.7357 - val_loss: 0.5628 - val_accuracy: 0.7370\n",
      "Epoch 322/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5405 - accuracy: 0.7362 - val_loss: 0.5648 - val_accuracy: 0.7367\n",
      "Epoch 323/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7365 - val_loss: 0.5669 - val_accuracy: 0.7352\n",
      "Epoch 324/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5409 - accuracy: 0.7356 - val_loss: 0.5677 - val_accuracy: 0.7367\n",
      "Epoch 325/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5408 - accuracy: 0.7356 - val_loss: 0.5676 - val_accuracy: 0.7375\n",
      "Epoch 326/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7362 - val_loss: 0.5645 - val_accuracy: 0.7362\n",
      "Epoch 327/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5407 - accuracy: 0.7348 - val_loss: 0.5650 - val_accuracy: 0.7367\n",
      "Epoch 328/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5410 - accuracy: 0.7363 - val_loss: 0.5686 - val_accuracy: 0.7380\n",
      "Epoch 329/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7362 - val_loss: 0.5663 - val_accuracy: 0.7370\n",
      "Epoch 330/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7361 - val_loss: 0.5625 - val_accuracy: 0.7385\n",
      "Epoch 331/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5405 - accuracy: 0.7363 - val_loss: 0.5609 - val_accuracy: 0.7375\n",
      "Epoch 332/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5411 - accuracy: 0.7353 - val_loss: 0.5616 - val_accuracy: 0.7365\n",
      "Epoch 333/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7357 - val_loss: 0.5639 - val_accuracy: 0.7383\n",
      "Epoch 334/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5408 - accuracy: 0.7361 - val_loss: 0.5680 - val_accuracy: 0.7372\n",
      "Epoch 335/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5405 - accuracy: 0.7368 - val_loss: 0.5636 - val_accuracy: 0.7370\n",
      "Epoch 336/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7361 - val_loss: 0.5652 - val_accuracy: 0.7372\n",
      "Epoch 337/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5408 - accuracy: 0.7358 - val_loss: 0.5696 - val_accuracy: 0.7370\n",
      "Epoch 338/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7363 - val_loss: 0.5609 - val_accuracy: 0.7349\n",
      "Epoch 339/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5403 - accuracy: 0.7361 - val_loss: 0.5672 - val_accuracy: 0.7370\n",
      "Epoch 340/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7359 - val_loss: 0.5666 - val_accuracy: 0.7357\n",
      "Epoch 341/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7365 - val_loss: 0.5659 - val_accuracy: 0.7370\n",
      "Epoch 342/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7366 - val_loss: 0.5676 - val_accuracy: 0.7375\n",
      "Epoch 343/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7362 - val_loss: 0.5667 - val_accuracy: 0.7365\n",
      "Epoch 344/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7361 - val_loss: 0.5678 - val_accuracy: 0.7375\n",
      "Epoch 345/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7358 - val_loss: 0.5709 - val_accuracy: 0.7375\n",
      "Epoch 346/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7358 - val_loss: 0.5650 - val_accuracy: 0.7367\n",
      "Epoch 347/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7359 - val_loss: 0.5653 - val_accuracy: 0.7372\n",
      "Epoch 348/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7349 - val_loss: 0.5632 - val_accuracy: 0.7367\n",
      "Epoch 349/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7353 - val_loss: 0.5667 - val_accuracy: 0.7362\n",
      "Epoch 350/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7362 - val_loss: 0.5696 - val_accuracy: 0.7372\n",
      "Epoch 351/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7352 - val_loss: 0.5688 - val_accuracy: 0.7367\n",
      "Epoch 352/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7354 - val_loss: 0.5695 - val_accuracy: 0.7375\n",
      "Epoch 353/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7358 - val_loss: 0.5648 - val_accuracy: 0.7385\n",
      "Epoch 354/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5403 - accuracy: 0.7359 - val_loss: 0.5690 - val_accuracy: 0.7393\n",
      "Epoch 355/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7357 - val_loss: 0.5691 - val_accuracy: 0.7375\n",
      "Epoch 356/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7359 - val_loss: 0.5693 - val_accuracy: 0.7383\n",
      "Epoch 357/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7351 - val_loss: 0.5716 - val_accuracy: 0.7359\n",
      "Epoch 358/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7356 - val_loss: 0.5715 - val_accuracy: 0.7370\n",
      "Epoch 359/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7360 - val_loss: 0.5690 - val_accuracy: 0.7359\n",
      "Epoch 360/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7357 - val_loss: 0.5705 - val_accuracy: 0.7375\n",
      "Epoch 361/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5407 - accuracy: 0.7362 - val_loss: 0.5695 - val_accuracy: 0.7365\n",
      "Epoch 362/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5400 - accuracy: 0.7357 - val_loss: 0.5668 - val_accuracy: 0.7372\n",
      "Epoch 363/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5405 - accuracy: 0.7366 - val_loss: 0.5707 - val_accuracy: 0.7380\n",
      "Epoch 364/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5404 - accuracy: 0.7362 - val_loss: 0.5703 - val_accuracy: 0.7372\n",
      "Epoch 365/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7355 - val_loss: 0.5726 - val_accuracy: 0.7375\n",
      "Epoch 366/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5407 - accuracy: 0.7365 - val_loss: 0.5779 - val_accuracy: 0.7380\n",
      "Epoch 367/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5407 - accuracy: 0.7354 - val_loss: 0.5705 - val_accuracy: 0.7367\n",
      "Epoch 368/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5408 - accuracy: 0.7364 - val_loss: 0.5660 - val_accuracy: 0.7378\n",
      "Epoch 369/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5405 - accuracy: 0.7358 - val_loss: 0.5703 - val_accuracy: 0.7367\n",
      "Epoch 370/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5401 - accuracy: 0.7358 - val_loss: 0.5720 - val_accuracy: 0.7383\n",
      "Epoch 371/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7366 - val_loss: 0.5718 - val_accuracy: 0.7383\n",
      "Epoch 372/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7362 - val_loss: 0.5724 - val_accuracy: 0.7380\n",
      "Epoch 373/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5407 - accuracy: 0.7365 - val_loss: 0.5709 - val_accuracy: 0.7383\n",
      "Epoch 374/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5404 - accuracy: 0.7359 - val_loss: 0.5673 - val_accuracy: 0.7378\n",
      "Epoch 375/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5402 - accuracy: 0.7365 - val_loss: 0.5729 - val_accuracy: 0.7339\n",
      "Epoch 376/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5404 - accuracy: 0.7358 - val_loss: 0.5760 - val_accuracy: 0.7372\n",
      "Epoch 377/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5416 - accuracy: 0.7364 - val_loss: 0.5747 - val_accuracy: 0.7378\n",
      "Epoch 378/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5415 - accuracy: 0.7362 - val_loss: 0.5692 - val_accuracy: 0.7372\n",
      "Epoch 379/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7359 - val_loss: 0.5679 - val_accuracy: 0.7378\n",
      "Epoch 380/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5406 - accuracy: 0.7357 - val_loss: 0.5704 - val_accuracy: 0.7375\n",
      "Epoch 381/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5401 - accuracy: 0.7360 - val_loss: 0.5701 - val_accuracy: 0.7372\n",
      "Epoch 382/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5405 - accuracy: 0.7362 - val_loss: 0.5745 - val_accuracy: 0.7383\n",
      "Epoch 383/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5403 - accuracy: 0.7361 - val_loss: 0.5719 - val_accuracy: 0.7365\n",
      "Epoch 384/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5409 - accuracy: 0.7362 - val_loss: 0.5673 - val_accuracy: 0.7378\n",
      "Epoch 385/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5399 - accuracy: 0.7363 - val_loss: 0.5687 - val_accuracy: 0.7372\n",
      "Epoch 386/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5404 - accuracy: 0.7362 - val_loss: 0.5679 - val_accuracy: 0.7362\n",
      "Epoch 387/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7364 - val_loss: 0.5679 - val_accuracy: 0.7367\n",
      "Epoch 388/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5408 - accuracy: 0.7358 - val_loss: 0.5706 - val_accuracy: 0.7367\n",
      "Epoch 389/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5404 - accuracy: 0.7367 - val_loss: 0.5692 - val_accuracy: 0.7370\n",
      "Epoch 390/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5409 - accuracy: 0.7357 - val_loss: 0.5713 - val_accuracy: 0.7370\n",
      "Epoch 391/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5404 - accuracy: 0.7362 - val_loss: 0.5678 - val_accuracy: 0.7388\n",
      "Epoch 392/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5409 - accuracy: 0.7356 - val_loss: 0.5697 - val_accuracy: 0.7365\n",
      "Epoch 393/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5400 - accuracy: 0.7367 - val_loss: 0.5713 - val_accuracy: 0.7367\n",
      "Epoch 394/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5407 - accuracy: 0.7360 - val_loss: 0.5701 - val_accuracy: 0.7375\n",
      "Epoch 395/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5406 - accuracy: 0.7361 - val_loss: 0.5751 - val_accuracy: 0.7378\n",
      "Epoch 396/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7362 - val_loss: 0.5731 - val_accuracy: 0.7370\n",
      "Epoch 397/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5400 - accuracy: 0.7364 - val_loss: 0.5746 - val_accuracy: 0.7362\n",
      "Epoch 398/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7366 - val_loss: 0.5742 - val_accuracy: 0.7365\n",
      "Epoch 399/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5407 - accuracy: 0.7362 - val_loss: 0.5637 - val_accuracy: 0.7359\n",
      "Epoch 400/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5410 - accuracy: 0.7359 - val_loss: 0.5691 - val_accuracy: 0.7359\n",
      "Epoch 401/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7356 - val_loss: 0.5675 - val_accuracy: 0.7370\n",
      "Epoch 402/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5401 - accuracy: 0.7357 - val_loss: 0.5692 - val_accuracy: 0.7367\n",
      "Epoch 403/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5404 - accuracy: 0.7361 - val_loss: 0.5675 - val_accuracy: 0.7370\n",
      "Epoch 404/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5415 - accuracy: 0.7363 - val_loss: 0.5642 - val_accuracy: 0.7375\n",
      "Epoch 405/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5398 - accuracy: 0.7364 - val_loss: 0.5669 - val_accuracy: 0.7370\n",
      "Epoch 406/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5400 - accuracy: 0.7363 - val_loss: 0.5644 - val_accuracy: 0.7365\n",
      "Epoch 407/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5399 - accuracy: 0.7360 - val_loss: 0.5699 - val_accuracy: 0.7385\n",
      "Epoch 408/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5406 - accuracy: 0.7365 - val_loss: 0.5661 - val_accuracy: 0.7354\n",
      "Epoch 409/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5403 - accuracy: 0.7362 - val_loss: 0.5659 - val_accuracy: 0.7385\n",
      "Epoch 410/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5400 - accuracy: 0.7364 - val_loss: 0.5642 - val_accuracy: 0.7336\n",
      "Epoch 411/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7358 - val_loss: 0.5674 - val_accuracy: 0.7362\n",
      "Epoch 412/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7360 - val_loss: 0.5633 - val_accuracy: 0.7385\n",
      "Epoch 413/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5405 - accuracy: 0.7358 - val_loss: 0.5632 - val_accuracy: 0.7365\n",
      "Epoch 414/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7357 - val_loss: 0.5650 - val_accuracy: 0.7375\n",
      "Epoch 415/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7360 - val_loss: 0.5707 - val_accuracy: 0.7365\n",
      "Epoch 416/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5403 - accuracy: 0.7362 - val_loss: 0.5677 - val_accuracy: 0.7375\n",
      "Epoch 417/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7362 - val_loss: 0.5676 - val_accuracy: 0.7370\n",
      "Epoch 418/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5406 - accuracy: 0.7364 - val_loss: 0.5657 - val_accuracy: 0.7365\n",
      "Epoch 419/800\n",
      "684/684 [==============================] - 4s 6ms/step - loss: 0.5401 - accuracy: 0.7353 - val_loss: 0.5720 - val_accuracy: 0.7372\n",
      "Epoch 420/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5403 - accuracy: 0.7362 - val_loss: 0.5671 - val_accuracy: 0.7367\n",
      "Epoch 421/800\n",
      "684/684 [==============================] - 2s 4ms/step - loss: 0.5397 - accuracy: 0.7363 - val_loss: 0.5721 - val_accuracy: 0.7365\n",
      "Epoch 422/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7361 - val_loss: 0.5741 - val_accuracy: 0.7370\n",
      "Epoch 423/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7362 - val_loss: 0.5747 - val_accuracy: 0.7378\n",
      "Epoch 424/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7352 - val_loss: 0.5695 - val_accuracy: 0.7372\n",
      "Epoch 425/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7364 - val_loss: 0.5693 - val_accuracy: 0.7380\n",
      "Epoch 426/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7360 - val_loss: 0.5692 - val_accuracy: 0.7375\n",
      "Epoch 427/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5395 - accuracy: 0.7363 - val_loss: 0.5728 - val_accuracy: 0.7365\n",
      "Epoch 428/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7362 - val_loss: 0.5676 - val_accuracy: 0.7362\n",
      "Epoch 429/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7357 - val_loss: 0.5686 - val_accuracy: 0.7378\n",
      "Epoch 430/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7362 - val_loss: 0.5707 - val_accuracy: 0.7388\n",
      "Epoch 431/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7366 - val_loss: 0.5715 - val_accuracy: 0.7385\n",
      "Epoch 432/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7362 - val_loss: 0.5684 - val_accuracy: 0.7378\n",
      "Epoch 433/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5399 - accuracy: 0.7363 - val_loss: 0.5762 - val_accuracy: 0.7375\n",
      "Epoch 434/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5409 - accuracy: 0.7361 - val_loss: 0.5738 - val_accuracy: 0.7367\n",
      "Epoch 435/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5404 - accuracy: 0.7365 - val_loss: 0.5715 - val_accuracy: 0.7370\n",
      "Epoch 436/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7359 - val_loss: 0.5662 - val_accuracy: 0.7372\n",
      "Epoch 437/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7357 - val_loss: 0.5706 - val_accuracy: 0.7375\n",
      "Epoch 438/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7361 - val_loss: 0.5734 - val_accuracy: 0.7365\n",
      "Epoch 439/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7356 - val_loss: 0.5713 - val_accuracy: 0.7354\n",
      "Epoch 440/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7356 - val_loss: 0.5771 - val_accuracy: 0.7370\n",
      "Epoch 441/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.7357 - val_loss: 0.5749 - val_accuracy: 0.7370\n",
      "Epoch 442/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5396 - accuracy: 0.7366 - val_loss: 0.5766 - val_accuracy: 0.7372\n",
      "Epoch 443/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7363 - val_loss: 0.5711 - val_accuracy: 0.7372\n",
      "Epoch 444/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7359 - val_loss: 0.5723 - val_accuracy: 0.7354\n",
      "Epoch 445/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7362 - val_loss: 0.5790 - val_accuracy: 0.7365\n",
      "Epoch 446/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5403 - accuracy: 0.7363 - val_loss: 0.5732 - val_accuracy: 0.7354\n",
      "Epoch 447/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5400 - accuracy: 0.7362 - val_loss: 0.5746 - val_accuracy: 0.7370\n",
      "Epoch 448/800\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.5396 - accuracy: 0.7368 - val_loss: 0.5786 - val_accuracy: 0.7370\n",
      "Epoch 449/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5403 - accuracy: 0.7364 - val_loss: 0.5730 - val_accuracy: 0.7372\n",
      "Epoch 450/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7364 - val_loss: 0.5764 - val_accuracy: 0.7367\n",
      "Epoch 451/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5394 - accuracy: 0.7365 - val_loss: 0.5754 - val_accuracy: 0.7372\n",
      "Epoch 452/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7359 - val_loss: 0.5669 - val_accuracy: 0.7359\n",
      "Epoch 453/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5404 - accuracy: 0.7355 - val_loss: 0.5802 - val_accuracy: 0.7372\n",
      "Epoch 454/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5401 - accuracy: 0.7355 - val_loss: 0.5729 - val_accuracy: 0.7388\n",
      "Epoch 455/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7362 - val_loss: 0.5707 - val_accuracy: 0.7375\n",
      "Epoch 456/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7367 - val_loss: 0.5715 - val_accuracy: 0.7359\n",
      "Epoch 457/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7362 - val_loss: 0.5671 - val_accuracy: 0.7367\n",
      "Epoch 458/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7357 - val_loss: 0.5712 - val_accuracy: 0.7362\n",
      "Epoch 459/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7364 - val_loss: 0.5686 - val_accuracy: 0.7370\n",
      "Epoch 460/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7365 - val_loss: 0.5683 - val_accuracy: 0.7370\n",
      "Epoch 461/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7364 - val_loss: 0.5687 - val_accuracy: 0.7375\n",
      "Epoch 462/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5399 - accuracy: 0.7368 - val_loss: 0.5739 - val_accuracy: 0.7378\n",
      "Epoch 463/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5395 - accuracy: 0.7369 - val_loss: 0.5693 - val_accuracy: 0.7378\n",
      "Epoch 464/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7363 - val_loss: 0.5723 - val_accuracy: 0.7385\n",
      "Epoch 465/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5395 - accuracy: 0.7365 - val_loss: 0.5737 - val_accuracy: 0.7383\n",
      "Epoch 466/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7357 - val_loss: 0.5765 - val_accuracy: 0.7372\n",
      "Epoch 467/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5400 - accuracy: 0.7366 - val_loss: 0.5714 - val_accuracy: 0.7378\n",
      "Epoch 468/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7362 - val_loss: 0.5722 - val_accuracy: 0.7370\n",
      "Epoch 469/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7360 - val_loss: 0.5746 - val_accuracy: 0.7367\n",
      "Epoch 470/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7364 - val_loss: 0.5669 - val_accuracy: 0.7380\n",
      "Epoch 471/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5395 - accuracy: 0.7362 - val_loss: 0.5789 - val_accuracy: 0.7372\n",
      "Epoch 472/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7361 - val_loss: 0.5659 - val_accuracy: 0.7378\n",
      "Epoch 473/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7364 - val_loss: 0.5697 - val_accuracy: 0.7380\n",
      "Epoch 474/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7369 - val_loss: 0.5735 - val_accuracy: 0.7365\n",
      "Epoch 475/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7357 - val_loss: 0.5719 - val_accuracy: 0.7385\n",
      "Epoch 476/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7365 - val_loss: 0.5732 - val_accuracy: 0.7388\n",
      "Epoch 477/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.7366 - val_loss: 0.5740 - val_accuracy: 0.7372\n",
      "Epoch 478/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.7366 - val_loss: 0.5698 - val_accuracy: 0.7383\n",
      "Epoch 479/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5403 - accuracy: 0.7364 - val_loss: 0.5694 - val_accuracy: 0.7372\n",
      "Epoch 480/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7367 - val_loss: 0.5714 - val_accuracy: 0.7378\n",
      "Epoch 481/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7359 - val_loss: 0.5645 - val_accuracy: 0.7380\n",
      "Epoch 482/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7362 - val_loss: 0.5675 - val_accuracy: 0.7367\n",
      "Epoch 483/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7368 - val_loss: 0.5706 - val_accuracy: 0.7362\n",
      "Epoch 484/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5400 - accuracy: 0.7360 - val_loss: 0.5750 - val_accuracy: 0.7367\n",
      "Epoch 485/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5410 - accuracy: 0.7359 - val_loss: 0.5689 - val_accuracy: 0.7378\n",
      "Epoch 486/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5403 - accuracy: 0.7362 - val_loss: 0.5669 - val_accuracy: 0.7383\n",
      "Epoch 487/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5402 - accuracy: 0.7362 - val_loss: 0.5623 - val_accuracy: 0.7388\n",
      "Epoch 488/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7362 - val_loss: 0.5747 - val_accuracy: 0.7375\n",
      "Epoch 489/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7368 - val_loss: 0.5730 - val_accuracy: 0.7391\n",
      "Epoch 490/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7362 - val_loss: 0.5763 - val_accuracy: 0.7367\n",
      "Epoch 491/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7363 - val_loss: 0.5716 - val_accuracy: 0.7385\n",
      "Epoch 492/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7370 - val_loss: 0.5741 - val_accuracy: 0.7378\n",
      "Epoch 493/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7367 - val_loss: 0.5735 - val_accuracy: 0.7388\n",
      "Epoch 494/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7367 - val_loss: 0.5699 - val_accuracy: 0.7383\n",
      "Epoch 495/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7363 - val_loss: 0.5674 - val_accuracy: 0.7385\n",
      "Epoch 496/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7372 - val_loss: 0.5709 - val_accuracy: 0.7385\n",
      "Epoch 497/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5395 - accuracy: 0.7365 - val_loss: 0.5702 - val_accuracy: 0.7375\n",
      "Epoch 498/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7366 - val_loss: 0.5803 - val_accuracy: 0.7378\n",
      "Epoch 499/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.7362 - val_loss: 0.5761 - val_accuracy: 0.7378\n",
      "Epoch 500/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5396 - accuracy: 0.7367 - val_loss: 0.5709 - val_accuracy: 0.7365\n",
      "Epoch 501/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7360 - val_loss: 0.5762 - val_accuracy: 0.7370\n",
      "Epoch 502/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7366 - val_loss: 0.5780 - val_accuracy: 0.7365\n",
      "Epoch 503/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7370 - val_loss: 0.5751 - val_accuracy: 0.7378\n",
      "Epoch 504/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5401 - accuracy: 0.7364 - val_loss: 0.5784 - val_accuracy: 0.7378\n",
      "Epoch 505/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7362 - val_loss: 0.5764 - val_accuracy: 0.7370\n",
      "Epoch 506/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5394 - accuracy: 0.7360 - val_loss: 0.5765 - val_accuracy: 0.7362\n",
      "Epoch 507/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5405 - accuracy: 0.7363 - val_loss: 0.5823 - val_accuracy: 0.7375\n",
      "Epoch 508/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7372 - val_loss: 0.5812 - val_accuracy: 0.7385\n",
      "Epoch 509/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5400 - accuracy: 0.7364 - val_loss: 0.5814 - val_accuracy: 0.7365\n",
      "Epoch 510/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7367 - val_loss: 0.5795 - val_accuracy: 0.7380\n",
      "Epoch 511/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7363 - val_loss: 0.5732 - val_accuracy: 0.7372\n",
      "Epoch 512/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5391 - accuracy: 0.7369 - val_loss: 0.5910 - val_accuracy: 0.7354\n",
      "Epoch 513/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5397 - accuracy: 0.7364 - val_loss: 0.5794 - val_accuracy: 0.7370\n",
      "Epoch 514/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7368 - val_loss: 0.5791 - val_accuracy: 0.7372\n",
      "Epoch 515/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7362 - val_loss: 0.5720 - val_accuracy: 0.7378\n",
      "Epoch 516/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7362 - val_loss: 0.5879 - val_accuracy: 0.7380\n",
      "Epoch 517/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7364 - val_loss: 0.5760 - val_accuracy: 0.7383\n",
      "Epoch 518/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7363 - val_loss: 0.5751 - val_accuracy: 0.7383\n",
      "Epoch 519/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7366 - val_loss: 0.5761 - val_accuracy: 0.7367\n",
      "Epoch 520/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7367 - val_loss: 0.5658 - val_accuracy: 0.7383\n",
      "Epoch 521/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5394 - accuracy: 0.7366 - val_loss: 0.5716 - val_accuracy: 0.7380\n",
      "Epoch 522/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7371 - val_loss: 0.5809 - val_accuracy: 0.7391\n",
      "Epoch 523/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5393 - accuracy: 0.7366 - val_loss: 0.5824 - val_accuracy: 0.7375\n",
      "Epoch 524/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5401 - accuracy: 0.7367 - val_loss: 0.5812 - val_accuracy: 0.7370\n",
      "Epoch 525/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7364 - val_loss: 0.5756 - val_accuracy: 0.7388\n",
      "Epoch 526/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7367 - val_loss: 0.5846 - val_accuracy: 0.7383\n",
      "Epoch 527/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5399 - accuracy: 0.7362 - val_loss: 0.5800 - val_accuracy: 0.7370\n",
      "Epoch 528/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5399 - accuracy: 0.7362 - val_loss: 0.5857 - val_accuracy: 0.7383\n",
      "Epoch 529/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7366 - val_loss: 0.5774 - val_accuracy: 0.7388\n",
      "Epoch 530/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7369 - val_loss: 0.5765 - val_accuracy: 0.7372\n",
      "Epoch 531/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5393 - accuracy: 0.7367 - val_loss: 0.5833 - val_accuracy: 0.7375\n",
      "Epoch 532/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5393 - accuracy: 0.7359 - val_loss: 0.5810 - val_accuracy: 0.7380\n",
      "Epoch 533/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7358 - val_loss: 0.5821 - val_accuracy: 0.7372\n",
      "Epoch 534/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7365 - val_loss: 0.5790 - val_accuracy: 0.7375\n",
      "Epoch 535/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7359 - val_loss: 0.5839 - val_accuracy: 0.7380\n",
      "Epoch 536/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5395 - accuracy: 0.7358 - val_loss: 0.5869 - val_accuracy: 0.7396\n",
      "Epoch 537/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5394 - accuracy: 0.7367 - val_loss: 0.5806 - val_accuracy: 0.7380\n",
      "Epoch 538/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5395 - accuracy: 0.7362 - val_loss: 0.5781 - val_accuracy: 0.7380\n",
      "Epoch 539/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7360 - val_loss: 0.5790 - val_accuracy: 0.7372\n",
      "Epoch 540/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5386 - accuracy: 0.7371 - val_loss: 0.5783 - val_accuracy: 0.7370\n",
      "Epoch 541/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7365 - val_loss: 0.5825 - val_accuracy: 0.7380\n",
      "Epoch 542/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7356 - val_loss: 0.5803 - val_accuracy: 0.7380\n",
      "Epoch 543/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7367 - val_loss: 0.5842 - val_accuracy: 0.7354\n",
      "Epoch 544/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7369 - val_loss: 0.5801 - val_accuracy: 0.7372\n",
      "Epoch 545/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7368 - val_loss: 0.5866 - val_accuracy: 0.7359\n",
      "Epoch 546/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7368 - val_loss: 0.5820 - val_accuracy: 0.7378\n",
      "Epoch 547/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5405 - accuracy: 0.7365 - val_loss: 0.5794 - val_accuracy: 0.7375\n",
      "Epoch 548/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7368 - val_loss: 0.5742 - val_accuracy: 0.7380\n",
      "Epoch 549/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7363 - val_loss: 0.5776 - val_accuracy: 0.7375\n",
      "Epoch 550/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7353 - val_loss: 0.5641 - val_accuracy: 0.7378\n",
      "Epoch 551/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7367 - val_loss: 0.5644 - val_accuracy: 0.7383\n",
      "Epoch 552/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7365 - val_loss: 0.5669 - val_accuracy: 0.7378\n",
      "Epoch 553/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7371 - val_loss: 0.5795 - val_accuracy: 0.7383\n",
      "Epoch 554/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7364 - val_loss: 0.5857 - val_accuracy: 0.7388\n",
      "Epoch 555/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.7362 - val_loss: 0.5794 - val_accuracy: 0.7378\n",
      "Epoch 556/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5394 - accuracy: 0.7365 - val_loss: 0.5806 - val_accuracy: 0.7383\n",
      "Epoch 557/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7365 - val_loss: 0.5846 - val_accuracy: 0.7383\n",
      "Epoch 558/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7365 - val_loss: 0.5750 - val_accuracy: 0.7378\n",
      "Epoch 559/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5393 - accuracy: 0.7360 - val_loss: 0.5831 - val_accuracy: 0.7372\n",
      "Epoch 560/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7357 - val_loss: 0.5819 - val_accuracy: 0.7385\n",
      "Epoch 561/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5397 - accuracy: 0.7367 - val_loss: 0.5773 - val_accuracy: 0.7378\n",
      "Epoch 562/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5391 - accuracy: 0.7369 - val_loss: 0.5792 - val_accuracy: 0.7380\n",
      "Epoch 563/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7368 - val_loss: 0.5738 - val_accuracy: 0.7380\n",
      "Epoch 564/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7367 - val_loss: 0.5828 - val_accuracy: 0.7372\n",
      "Epoch 565/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5393 - accuracy: 0.7365 - val_loss: 0.5835 - val_accuracy: 0.7378\n",
      "Epoch 566/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7360 - val_loss: 0.5775 - val_accuracy: 0.7378\n",
      "Epoch 567/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.7373 - val_loss: 0.5932 - val_accuracy: 0.7388\n",
      "Epoch 568/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7359 - val_loss: 0.5863 - val_accuracy: 0.7378\n",
      "Epoch 569/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5391 - accuracy: 0.7367 - val_loss: 0.5884 - val_accuracy: 0.7370\n",
      "Epoch 570/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7363 - val_loss: 0.5775 - val_accuracy: 0.7365\n",
      "Epoch 571/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5400 - accuracy: 0.7365 - val_loss: 0.5839 - val_accuracy: 0.7370\n",
      "Epoch 572/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5398 - accuracy: 0.7359 - val_loss: 0.5788 - val_accuracy: 0.7380\n",
      "Epoch 573/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5390 - accuracy: 0.7368 - val_loss: 0.5828 - val_accuracy: 0.7385\n",
      "Epoch 574/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.7371 - val_loss: 0.5823 - val_accuracy: 0.7383\n",
      "Epoch 575/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7356 - val_loss: 0.5785 - val_accuracy: 0.7396\n",
      "Epoch 576/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7364 - val_loss: 0.5774 - val_accuracy: 0.7352\n",
      "Epoch 577/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7361 - val_loss: 0.5854 - val_accuracy: 0.7339\n",
      "Epoch 578/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5395 - accuracy: 0.7365 - val_loss: 0.5784 - val_accuracy: 0.7383\n",
      "Epoch 579/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7365 - val_loss: 0.5744 - val_accuracy: 0.7383\n",
      "Epoch 580/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7362 - val_loss: 0.5759 - val_accuracy: 0.7375\n",
      "Epoch 581/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7363 - val_loss: 0.5812 - val_accuracy: 0.7378\n",
      "Epoch 582/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5390 - accuracy: 0.7369 - val_loss: 0.5863 - val_accuracy: 0.7378\n",
      "Epoch 583/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7365 - val_loss: 0.5879 - val_accuracy: 0.7378\n",
      "Epoch 584/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7366 - val_loss: 0.5864 - val_accuracy: 0.7375\n",
      "Epoch 585/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7360 - val_loss: 0.5820 - val_accuracy: 0.7372\n",
      "Epoch 586/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7361 - val_loss: 0.5891 - val_accuracy: 0.7380\n",
      "Epoch 587/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7370 - val_loss: 0.5968 - val_accuracy: 0.7383\n",
      "Epoch 588/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7358 - val_loss: 0.5730 - val_accuracy: 0.7378\n",
      "Epoch 589/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7365 - val_loss: 0.5894 - val_accuracy: 0.7349\n",
      "Epoch 590/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7366 - val_loss: 0.5905 - val_accuracy: 0.7385\n",
      "Epoch 591/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7365 - val_loss: 0.5978 - val_accuracy: 0.7365\n",
      "Epoch 592/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7365 - val_loss: 0.5767 - val_accuracy: 0.7385\n",
      "Epoch 593/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7360 - val_loss: 0.5952 - val_accuracy: 0.7383\n",
      "Epoch 594/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5391 - accuracy: 0.7367 - val_loss: 0.5869 - val_accuracy: 0.7365\n",
      "Epoch 595/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7357 - val_loss: 0.5911 - val_accuracy: 0.7380\n",
      "Epoch 596/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7369 - val_loss: 0.5883 - val_accuracy: 0.7393\n",
      "Epoch 597/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7365 - val_loss: 0.5911 - val_accuracy: 0.7391\n",
      "Epoch 598/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7361 - val_loss: 0.5951 - val_accuracy: 0.7385\n",
      "Epoch 599/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7366 - val_loss: 0.6146 - val_accuracy: 0.7378\n",
      "Epoch 600/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5397 - accuracy: 0.7375 - val_loss: 0.5777 - val_accuracy: 0.7362\n",
      "Epoch 601/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7362 - val_loss: 0.5937 - val_accuracy: 0.7385\n",
      "Epoch 602/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7366 - val_loss: 0.5846 - val_accuracy: 0.7385\n",
      "Epoch 603/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7364 - val_loss: 0.5882 - val_accuracy: 0.7375\n",
      "Epoch 604/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7370 - val_loss: 0.5923 - val_accuracy: 0.7380\n",
      "Epoch 605/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7363 - val_loss: 0.5910 - val_accuracy: 0.7380\n",
      "Epoch 606/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.7370 - val_loss: 0.5935 - val_accuracy: 0.7385\n",
      "Epoch 607/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5394 - accuracy: 0.7364 - val_loss: 0.5927 - val_accuracy: 0.7380\n",
      "Epoch 608/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7367 - val_loss: 0.5868 - val_accuracy: 0.7365\n",
      "Epoch 609/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5389 - accuracy: 0.7370 - val_loss: 0.6021 - val_accuracy: 0.7372\n",
      "Epoch 610/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5387 - accuracy: 0.7362 - val_loss: 0.6003 - val_accuracy: 0.7375\n",
      "Epoch 611/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5394 - accuracy: 0.7367 - val_loss: 0.5939 - val_accuracy: 0.7370\n",
      "Epoch 612/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5385 - accuracy: 0.7373 - val_loss: 0.5914 - val_accuracy: 0.7378\n",
      "Epoch 613/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7368 - val_loss: 0.5935 - val_accuracy: 0.7372\n",
      "Epoch 614/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7368 - val_loss: 0.5955 - val_accuracy: 0.7375\n",
      "Epoch 615/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7364 - val_loss: 0.6021 - val_accuracy: 0.7372\n",
      "Epoch 616/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7368 - val_loss: 0.5789 - val_accuracy: 0.7357\n",
      "Epoch 617/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5405 - accuracy: 0.7365 - val_loss: 0.5707 - val_accuracy: 0.7375\n",
      "Epoch 618/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7366 - val_loss: 0.5819 - val_accuracy: 0.7370\n",
      "Epoch 619/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.7373 - val_loss: 0.5830 - val_accuracy: 0.7372\n",
      "Epoch 620/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7358 - val_loss: 0.5886 - val_accuracy: 0.7378\n",
      "Epoch 621/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7367 - val_loss: 0.5867 - val_accuracy: 0.7372\n",
      "Epoch 622/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7365 - val_loss: 0.5904 - val_accuracy: 0.7372\n",
      "Epoch 623/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7362 - val_loss: 0.5838 - val_accuracy: 0.7367\n",
      "Epoch 624/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7367 - val_loss: 0.5849 - val_accuracy: 0.7365\n",
      "Epoch 625/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7364 - val_loss: 0.5855 - val_accuracy: 0.7372\n",
      "Epoch 626/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7367 - val_loss: 0.5863 - val_accuracy: 0.7375\n",
      "Epoch 627/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7363 - val_loss: 0.5866 - val_accuracy: 0.7372\n",
      "Epoch 628/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.7367 - val_loss: 0.5983 - val_accuracy: 0.7380\n",
      "Epoch 629/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7365 - val_loss: 0.5875 - val_accuracy: 0.7372\n",
      "Epoch 630/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5387 - accuracy: 0.7363 - val_loss: 0.5766 - val_accuracy: 0.7370\n",
      "Epoch 631/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7369 - val_loss: 0.5985 - val_accuracy: 0.7367\n",
      "Epoch 632/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7362 - val_loss: 0.5845 - val_accuracy: 0.7367\n",
      "Epoch 633/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7369 - val_loss: 0.6092 - val_accuracy: 0.7383\n",
      "Epoch 634/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5397 - accuracy: 0.7367 - val_loss: 0.5760 - val_accuracy: 0.7367\n",
      "Epoch 635/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7373 - val_loss: 0.5980 - val_accuracy: 0.7375\n",
      "Epoch 636/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7367 - val_loss: 0.5945 - val_accuracy: 0.7344\n",
      "Epoch 637/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7367 - val_loss: 0.6063 - val_accuracy: 0.7365\n",
      "Epoch 638/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7362 - val_loss: 0.6034 - val_accuracy: 0.7378\n",
      "Epoch 639/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7364 - val_loss: 0.5957 - val_accuracy: 0.7367\n",
      "Epoch 640/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7363 - val_loss: 0.5941 - val_accuracy: 0.7391\n",
      "Epoch 641/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7362 - val_loss: 0.6041 - val_accuracy: 0.7383\n",
      "Epoch 642/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5386 - accuracy: 0.7361 - val_loss: 0.5976 - val_accuracy: 0.7378\n",
      "Epoch 643/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7366 - val_loss: 0.5922 - val_accuracy: 0.7375\n",
      "Epoch 644/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7360 - val_loss: 0.5869 - val_accuracy: 0.7385\n",
      "Epoch 645/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5387 - accuracy: 0.7362 - val_loss: 0.6084 - val_accuracy: 0.7370\n",
      "Epoch 646/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7365 - val_loss: 0.6003 - val_accuracy: 0.7383\n",
      "Epoch 647/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7370 - val_loss: 0.6037 - val_accuracy: 0.7383\n",
      "Epoch 648/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7369 - val_loss: 0.6006 - val_accuracy: 0.7370\n",
      "Epoch 649/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7366 - val_loss: 0.6112 - val_accuracy: 0.7375\n",
      "Epoch 650/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5394 - accuracy: 0.7366 - val_loss: 0.5878 - val_accuracy: 0.7378\n",
      "Epoch 651/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7366 - val_loss: 0.5931 - val_accuracy: 0.7383\n",
      "Epoch 652/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7360 - val_loss: 0.5945 - val_accuracy: 0.7375\n",
      "Epoch 653/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5393 - accuracy: 0.7366 - val_loss: 0.6036 - val_accuracy: 0.7372\n",
      "Epoch 654/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5391 - accuracy: 0.7360 - val_loss: 0.5874 - val_accuracy: 0.7375\n",
      "Epoch 655/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7357 - val_loss: 0.5993 - val_accuracy: 0.7383\n",
      "Epoch 656/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7362 - val_loss: 0.5995 - val_accuracy: 0.7375\n",
      "Epoch 657/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7366 - val_loss: 0.5986 - val_accuracy: 0.7375\n",
      "Epoch 658/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5390 - accuracy: 0.7364 - val_loss: 0.5919 - val_accuracy: 0.7380\n",
      "Epoch 659/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7360 - val_loss: 0.6069 - val_accuracy: 0.7370\n",
      "Epoch 660/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5393 - accuracy: 0.7363 - val_loss: 0.6014 - val_accuracy: 0.7380\n",
      "Epoch 661/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7362 - val_loss: 0.6064 - val_accuracy: 0.7367\n",
      "Epoch 662/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5397 - accuracy: 0.7363 - val_loss: 0.6059 - val_accuracy: 0.7367\n",
      "Epoch 663/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5387 - accuracy: 0.7373 - val_loss: 0.6072 - val_accuracy: 0.7375\n",
      "Epoch 664/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7365 - val_loss: 0.5961 - val_accuracy: 0.7370\n",
      "Epoch 665/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5384 - accuracy: 0.7366 - val_loss: 0.6068 - val_accuracy: 0.7378\n",
      "Epoch 666/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.7369 - val_loss: 0.5828 - val_accuracy: 0.7359\n",
      "Epoch 667/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5390 - accuracy: 0.7361 - val_loss: 0.5891 - val_accuracy: 0.7372\n",
      "Epoch 668/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5402 - accuracy: 0.7361 - val_loss: 0.5951 - val_accuracy: 0.7357\n",
      "Epoch 669/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5386 - accuracy: 0.7372 - val_loss: 0.5940 - val_accuracy: 0.7362\n",
      "Epoch 670/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7367 - val_loss: 0.5945 - val_accuracy: 0.7365\n",
      "Epoch 671/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5381 - accuracy: 0.7371 - val_loss: 0.5939 - val_accuracy: 0.7372\n",
      "Epoch 672/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5384 - accuracy: 0.7363 - val_loss: 0.5962 - val_accuracy: 0.7378\n",
      "Epoch 673/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7364 - val_loss: 0.5888 - val_accuracy: 0.7370\n",
      "Epoch 674/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5391 - accuracy: 0.7367 - val_loss: 0.5852 - val_accuracy: 0.7383\n",
      "Epoch 675/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5387 - accuracy: 0.7377 - val_loss: 0.5996 - val_accuracy: 0.7359\n",
      "Epoch 676/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5388 - accuracy: 0.7365 - val_loss: 0.5958 - val_accuracy: 0.7362\n",
      "Epoch 677/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7366 - val_loss: 0.5858 - val_accuracy: 0.7378\n",
      "Epoch 678/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7366 - val_loss: 0.5939 - val_accuracy: 0.7362\n",
      "Epoch 679/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7370 - val_loss: 0.5942 - val_accuracy: 0.7375\n",
      "Epoch 680/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7367 - val_loss: 0.6005 - val_accuracy: 0.7367\n",
      "Epoch 681/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7367 - val_loss: 0.6120 - val_accuracy: 0.7383\n",
      "Epoch 682/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7364 - val_loss: 0.5683 - val_accuracy: 0.7359\n",
      "Epoch 683/800\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5400 - accuracy: 0.7364 - val_loss: 0.5781 - val_accuracy: 0.7362\n",
      "Epoch 684/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7369 - val_loss: 0.5825 - val_accuracy: 0.7354\n",
      "Epoch 685/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7367 - val_loss: 0.5811 - val_accuracy: 0.7370\n",
      "Epoch 686/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7369 - val_loss: 0.5862 - val_accuracy: 0.7365\n",
      "Epoch 687/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7363 - val_loss: 0.5858 - val_accuracy: 0.7385\n",
      "Epoch 688/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7374 - val_loss: 0.6003 - val_accuracy: 0.7378\n",
      "Epoch 689/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5389 - accuracy: 0.7367 - val_loss: 0.5954 - val_accuracy: 0.7372\n",
      "Epoch 690/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7367 - val_loss: 0.5919 - val_accuracy: 0.7359\n",
      "Epoch 691/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7370 - val_loss: 0.5848 - val_accuracy: 0.7388\n",
      "Epoch 692/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5393 - accuracy: 0.7362 - val_loss: 0.5960 - val_accuracy: 0.7372\n",
      "Epoch 693/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7367 - val_loss: 0.5875 - val_accuracy: 0.7359\n",
      "Epoch 694/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7369 - val_loss: 0.5809 - val_accuracy: 0.7383\n",
      "Epoch 695/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7361 - val_loss: 0.5892 - val_accuracy: 0.7362\n",
      "Epoch 696/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7368 - val_loss: 0.5799 - val_accuracy: 0.7367\n",
      "Epoch 697/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7367 - val_loss: 0.5943 - val_accuracy: 0.7370\n",
      "Epoch 698/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7363 - val_loss: 0.5921 - val_accuracy: 0.7367\n",
      "Epoch 699/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7364 - val_loss: 0.5982 - val_accuracy: 0.7365\n",
      "Epoch 700/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7366 - val_loss: 0.5854 - val_accuracy: 0.7378\n",
      "Epoch 701/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7367 - val_loss: 0.5938 - val_accuracy: 0.7372\n",
      "Epoch 702/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7365 - val_loss: 0.5953 - val_accuracy: 0.7359\n",
      "Epoch 703/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7364 - val_loss: 0.5893 - val_accuracy: 0.7370\n",
      "Epoch 704/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7370 - val_loss: 0.5926 - val_accuracy: 0.7367\n",
      "Epoch 705/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7373 - val_loss: 0.5829 - val_accuracy: 0.7357\n",
      "Epoch 706/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7362 - val_loss: 0.5972 - val_accuracy: 0.7367\n",
      "Epoch 707/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7373 - val_loss: 0.5864 - val_accuracy: 0.7365\n",
      "Epoch 708/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7367 - val_loss: 0.5897 - val_accuracy: 0.7359\n",
      "Epoch 709/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7370 - val_loss: 0.5922 - val_accuracy: 0.7362\n",
      "Epoch 710/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7360 - val_loss: 0.6128 - val_accuracy: 0.7354\n",
      "Epoch 711/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7363 - val_loss: 0.5915 - val_accuracy: 0.7370\n",
      "Epoch 712/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7367 - val_loss: 0.5883 - val_accuracy: 0.7362\n",
      "Epoch 713/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7368 - val_loss: 0.5880 - val_accuracy: 0.7354\n",
      "Epoch 714/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5389 - accuracy: 0.7370 - val_loss: 0.5894 - val_accuracy: 0.7370\n",
      "Epoch 715/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7367 - val_loss: 0.5910 - val_accuracy: 0.7367\n",
      "Epoch 716/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7371 - val_loss: 0.5947 - val_accuracy: 0.7357\n",
      "Epoch 717/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7369 - val_loss: 0.6010 - val_accuracy: 0.7365\n",
      "Epoch 718/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7370 - val_loss: 0.5894 - val_accuracy: 0.7370\n",
      "Epoch 719/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7371 - val_loss: 0.5906 - val_accuracy: 0.7370\n",
      "Epoch 720/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7367 - val_loss: 0.5864 - val_accuracy: 0.7367\n",
      "Epoch 721/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7369 - val_loss: 0.5943 - val_accuracy: 0.7372\n",
      "Epoch 722/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7362 - val_loss: 0.5963 - val_accuracy: 0.7375\n",
      "Epoch 723/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7369 - val_loss: 0.5932 - val_accuracy: 0.7359\n",
      "Epoch 724/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7371 - val_loss: 0.6015 - val_accuracy: 0.7359\n",
      "Epoch 725/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7368 - val_loss: 0.6014 - val_accuracy: 0.7359\n",
      "Epoch 726/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7365 - val_loss: 0.5868 - val_accuracy: 0.7378\n",
      "Epoch 727/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7370 - val_loss: 0.6263 - val_accuracy: 0.7362\n",
      "Epoch 728/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7373 - val_loss: 0.6164 - val_accuracy: 0.7362\n",
      "Epoch 729/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7373 - val_loss: 0.6218 - val_accuracy: 0.7365\n",
      "Epoch 730/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7374 - val_loss: 0.5986 - val_accuracy: 0.7362\n",
      "Epoch 731/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7366 - val_loss: 0.6050 - val_accuracy: 0.7370\n",
      "Epoch 732/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7373 - val_loss: 0.6014 - val_accuracy: 0.7370\n",
      "Epoch 733/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7373 - val_loss: 0.6001 - val_accuracy: 0.7359\n",
      "Epoch 734/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.7373 - val_loss: 0.5991 - val_accuracy: 0.7357\n",
      "Epoch 735/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7371 - val_loss: 0.5937 - val_accuracy: 0.7367\n",
      "Epoch 736/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7370 - val_loss: 0.5942 - val_accuracy: 0.7362\n",
      "Epoch 737/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7369 - val_loss: 0.5827 - val_accuracy: 0.7357\n",
      "Epoch 738/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7367 - val_loss: 0.6057 - val_accuracy: 0.7375\n",
      "Epoch 739/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7372 - val_loss: 0.6038 - val_accuracy: 0.7372\n",
      "Epoch 740/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5384 - accuracy: 0.7365 - val_loss: 0.6033 - val_accuracy: 0.7370\n",
      "Epoch 741/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5381 - accuracy: 0.7369 - val_loss: 0.5955 - val_accuracy: 0.7341\n",
      "Epoch 742/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5386 - accuracy: 0.7362 - val_loss: 0.5931 - val_accuracy: 0.7365\n",
      "Epoch 743/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7370 - val_loss: 0.5894 - val_accuracy: 0.7380\n",
      "Epoch 744/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5385 - accuracy: 0.7373 - val_loss: 0.5990 - val_accuracy: 0.7365\n",
      "Epoch 745/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5379 - accuracy: 0.7362 - val_loss: 0.5978 - val_accuracy: 0.7357\n",
      "Epoch 746/800\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5387 - accuracy: 0.7362 - val_loss: 0.5960 - val_accuracy: 0.7365\n",
      "Epoch 747/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5386 - accuracy: 0.7361 - val_loss: 0.5828 - val_accuracy: 0.7357\n",
      "Epoch 748/800\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5387 - accuracy: 0.7376 - val_loss: 0.5948 - val_accuracy: 0.7357\n",
      "Epoch 749/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7367 - val_loss: 0.5876 - val_accuracy: 0.7359\n",
      "Epoch 750/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7361 - val_loss: 0.5865 - val_accuracy: 0.7331\n",
      "Epoch 751/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7366 - val_loss: 0.5974 - val_accuracy: 0.7354\n",
      "Epoch 752/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7368 - val_loss: 0.5866 - val_accuracy: 0.7354\n",
      "Epoch 753/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7370 - val_loss: 0.5933 - val_accuracy: 0.7336\n",
      "Epoch 754/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5394 - accuracy: 0.7368 - val_loss: 0.5933 - val_accuracy: 0.7383\n",
      "Epoch 755/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7358 - val_loss: 0.5817 - val_accuracy: 0.7365\n",
      "Epoch 756/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7364 - val_loss: 0.5823 - val_accuracy: 0.7365\n",
      "Epoch 757/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7368 - val_loss: 0.5891 - val_accuracy: 0.7354\n",
      "Epoch 758/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7358 - val_loss: 0.6016 - val_accuracy: 0.7315\n",
      "Epoch 759/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7367 - val_loss: 0.5863 - val_accuracy: 0.7375\n",
      "Epoch 760/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7371 - val_loss: 0.5950 - val_accuracy: 0.7372\n",
      "Epoch 761/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7370 - val_loss: 0.5852 - val_accuracy: 0.7372\n",
      "Epoch 762/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7369 - val_loss: 0.5864 - val_accuracy: 0.7359\n",
      "Epoch 763/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5378 - accuracy: 0.7363 - val_loss: 0.5820 - val_accuracy: 0.7362\n",
      "Epoch 764/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7371 - val_loss: 0.5859 - val_accuracy: 0.7372\n",
      "Epoch 765/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7366 - val_loss: 0.5899 - val_accuracy: 0.7362\n",
      "Epoch 766/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7370 - val_loss: 0.5884 - val_accuracy: 0.7354\n",
      "Epoch 767/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7361 - val_loss: 0.5794 - val_accuracy: 0.7349\n",
      "Epoch 768/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7374 - val_loss: 0.5782 - val_accuracy: 0.7367\n",
      "Epoch 769/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7372 - val_loss: 0.5796 - val_accuracy: 0.7346\n",
      "Epoch 770/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7371 - val_loss: 0.5808 - val_accuracy: 0.7349\n",
      "Epoch 771/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7369 - val_loss: 0.5845 - val_accuracy: 0.7352\n",
      "Epoch 772/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7368 - val_loss: 0.5750 - val_accuracy: 0.7336\n",
      "Epoch 773/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7373 - val_loss: 0.5849 - val_accuracy: 0.7352\n",
      "Epoch 774/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7369 - val_loss: 0.5984 - val_accuracy: 0.7375\n",
      "Epoch 775/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7360 - val_loss: 0.5890 - val_accuracy: 0.7370\n",
      "Epoch 776/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7367 - val_loss: 0.5958 - val_accuracy: 0.7367\n",
      "Epoch 777/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7367 - val_loss: 0.5877 - val_accuracy: 0.7375\n",
      "Epoch 778/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7373 - val_loss: 0.6165 - val_accuracy: 0.7359\n",
      "Epoch 779/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7373 - val_loss: 0.6112 - val_accuracy: 0.7370\n",
      "Epoch 780/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7375 - val_loss: 0.6008 - val_accuracy: 0.7370\n",
      "Epoch 781/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7373 - val_loss: 0.5968 - val_accuracy: 0.7383\n",
      "Epoch 782/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7371 - val_loss: 0.5875 - val_accuracy: 0.7354\n",
      "Epoch 783/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7362 - val_loss: 0.5970 - val_accuracy: 0.7362\n",
      "Epoch 784/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7374 - val_loss: 0.6014 - val_accuracy: 0.7362\n",
      "Epoch 785/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7370 - val_loss: 0.5883 - val_accuracy: 0.7375\n",
      "Epoch 786/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7367 - val_loss: 0.5963 - val_accuracy: 0.7370\n",
      "Epoch 787/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7369 - val_loss: 0.5922 - val_accuracy: 0.7365\n",
      "Epoch 788/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7366 - val_loss: 0.6003 - val_accuracy: 0.7354\n",
      "Epoch 789/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7369 - val_loss: 0.5850 - val_accuracy: 0.7362\n",
      "Epoch 790/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5379 - accuracy: 0.7366 - val_loss: 0.5896 - val_accuracy: 0.7362\n",
      "Epoch 791/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7373 - val_loss: 0.5886 - val_accuracy: 0.7346\n",
      "Epoch 792/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7367 - val_loss: 0.5834 - val_accuracy: 0.7352\n",
      "Epoch 793/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7368 - val_loss: 0.5882 - val_accuracy: 0.7354\n",
      "Epoch 794/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5380 - accuracy: 0.7368 - val_loss: 0.5922 - val_accuracy: 0.7362\n",
      "Epoch 795/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7359 - val_loss: 0.5866 - val_accuracy: 0.7341\n",
      "Epoch 796/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7365 - val_loss: 0.5980 - val_accuracy: 0.7344\n",
      "Epoch 797/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7369 - val_loss: 0.5865 - val_accuracy: 0.7357\n",
      "Epoch 798/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7372 - val_loss: 0.5849 - val_accuracy: 0.7365\n",
      "Epoch 799/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7364 - val_loss: 0.5893 - val_accuracy: 0.7357\n",
      "Epoch 800/800\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7371 - val_loss: 0.5874 - val_accuracy: 0.7354\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,validation_split=0.15, epochs=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 0s - loss: 0.5825 - accuracy: 0.7370 - 483ms/epoch - 2ms/step\n",
      "Loss: 0.5824776291847229, Accuracy: 0.7370262145996094\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gus Bustillos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export our model to HDF5 file\n",
    "# .h5 files are considered legacy and the library suggested to save it as .keras, saved both formats. \n",
    "nn.save(\"./modelHDF5.h5\")\n",
    "nn.save(\"./modelKERAS.keras\")\n",
    "plot_df = pd.DataFrame(fit_model.history, index = range(1, len(fit_model.history['loss'])+1))\n",
    "plot_df.plot(y = 'accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
